{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9165d9bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# As part of this module we will explore the functions available under pyspark.sql.functions to derive new values from existing column values with in a Data Frame.\n",
    "\n",
    "# Pre-defined Functions\n",
    "# Create Dummy Data Frame\n",
    "# Categories of Functions\n",
    "# Special Functions - col and lit\n",
    "# String Manipulation Functions - 1\n",
    "# String Manipulation Functions - 2\n",
    "# Date and Time Overview\n",
    "# Date and Time Arithmetic\n",
    "# Date and Time - trunc and date_trunc\n",
    "# Date and Time - Extracting Information\n",
    "# Dealing with Unix Timestamp\n",
    "# Example - Word Count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "921f35ac",
   "metadata": {},
   "source": [
    "## Pre-defined Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "49f786fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We typically process data in the columns using functions in pyspark.sql.functions. Let us understand details about these functions in detail as part of this module.\n",
    "\n",
    "# Let us recap about Functions or APIs to process Data Frames.\n",
    "\n",
    "# Projection - select or withColumn or drop or selectExpr\n",
    "# Filtering - filter or where\n",
    "# Grouping data by key and perform aggregations - groupBy\n",
    "# Sorting data - sort or orderBy\n",
    "\n",
    "# We can pass column names or literals or expressions to all the Data Frame APIs.\n",
    "# Expressions include arithmetic operations, transformations using functions from pyspark.sql.functions.\n",
    "# There are approximately 300 functions under pyspark.sql.functions.\n",
    "\n",
    "# We will talk about some of the important functions used for String Manipulation, Date Manipulation etc.\n",
    "# Here are some of the examples of using functions to take care of required transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "782fdb22",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/Users/manideep/miniconda3/lib/python3.9/site-packages/pyspark/jars/spark-unsafe_2.12-3.2.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/06/30 11:30:55 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "23/06/30 11:30:56 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "23/06/30 11:30:56 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n"
     ]
    }
   ],
   "source": [
    "# Reading data\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
    "\n",
    "spark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\",False)\n",
    "\n",
    "\n",
    "orders = spark.read.csv(\n",
    "    'orders.csv',\n",
    "    schema='order_id INT, order_date STRING, order_customer_id INT, order_status STRING'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "af836b15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------------------+-----------------+---------------+\n",
      "|order_id|order_date           |order_customer_id|order_status   |\n",
      "+--------+---------------------+-----------------+---------------+\n",
      "|1       |2013-07-25 00:00:00.0|11599            |CLOSED         |\n",
      "|2       |2013-07-25 00:00:00.0|256              |PENDING_PAYMENT|\n",
      "|3       |2013-07-25 00:00:00.0|12111            |COMPLETE       |\n",
      "|4       |2013-07-25 00:00:00.0|8827             |CLOSED         |\n",
      "|5       |2013-07-25 00:00:00.0|11318            |COMPLETE       |\n",
      "|6       |2013-07-25 00:00:00.0|7130             |COMPLETE       |\n",
      "|7       |2013-07-25 00:00:00.0|4530             |COMPLETE       |\n",
      "|8       |2013-07-25 00:00:00.0|2911             |PROCESSING     |\n",
      "|9       |2013-07-25 00:00:00.0|5657             |PENDING_PAYMENT|\n",
      "|10      |2013-07-25 00:00:00.0|5648             |PENDING_PAYMENT|\n",
      "|11      |2013-07-25 00:00:00.0|918              |PAYMENT_REVIEW |\n",
      "|12      |2013-07-25 00:00:00.0|1837             |CLOSED         |\n",
      "|13      |2013-07-25 00:00:00.0|9149             |PENDING_PAYMENT|\n",
      "|14      |2013-07-25 00:00:00.0|9842             |PROCESSING     |\n",
      "|15      |2013-07-25 00:00:00.0|2568             |COMPLETE       |\n",
      "|16      |2013-07-25 00:00:00.0|7276             |PENDING_PAYMENT|\n",
      "|17      |2013-07-25 00:00:00.0|2667             |COMPLETE       |\n",
      "|18      |2013-07-25 00:00:00.0|1205             |CLOSED         |\n",
      "|19      |2013-07-25 00:00:00.0|9488             |PENDING_PAYMENT|\n",
      "|20      |2013-07-25 00:00:00.0|9198             |PROCESSING     |\n",
      "+--------+---------------------+-----------------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import date_format\n",
    "\n",
    "orders.alias(\"u\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c255801f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------+\n",
      "|date_format(order_date, yyyyMM)|\n",
      "+-------------------------------+\n",
      "|                         201307|\n",
      "|                         201307|\n",
      "|                         201307|\n",
      "|                         201307|\n",
      "|                         201307|\n",
      "|                         201307|\n",
      "|                         201307|\n",
      "|                         201307|\n",
      "|                         201307|\n",
      "|                         201307|\n",
      "|                         201307|\n",
      "|                         201307|\n",
      "|                         201307|\n",
      "|                         201307|\n",
      "|                         201307|\n",
      "|                         201307|\n",
      "|                         201307|\n",
      "|                         201307|\n",
      "|                         201307|\n",
      "|                         201307|\n",
      "+-------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orders.select(date_format(\"order_date\",\"yyyyMM\")).alias(\"order\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8c658454",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+-----------------+---------------+------------+\n",
      "|order_id|          order_date|order_customer_id|   order_status|order_months|\n",
      "+--------+--------------------+-----------------+---------------+------------+\n",
      "|       1|2013-07-25 00:00:...|            11599|         CLOSED|     2013-07|\n",
      "|       2|2013-07-25 00:00:...|              256|PENDING_PAYMENT|     2013-07|\n",
      "|       3|2013-07-25 00:00:...|            12111|       COMPLETE|     2013-07|\n",
      "|       4|2013-07-25 00:00:...|             8827|         CLOSED|     2013-07|\n",
      "|       5|2013-07-25 00:00:...|            11318|       COMPLETE|     2013-07|\n",
      "|       6|2013-07-25 00:00:...|             7130|       COMPLETE|     2013-07|\n",
      "|       7|2013-07-25 00:00:...|             4530|       COMPLETE|     2013-07|\n",
      "|       8|2013-07-25 00:00:...|             2911|     PROCESSING|     2013-07|\n",
      "|       9|2013-07-25 00:00:...|             5657|PENDING_PAYMENT|     2013-07|\n",
      "|      10|2013-07-25 00:00:...|             5648|PENDING_PAYMENT|     2013-07|\n",
      "|      11|2013-07-25 00:00:...|              918| PAYMENT_REVIEW|     2013-07|\n",
      "|      12|2013-07-25 00:00:...|             1837|         CLOSED|     2013-07|\n",
      "|      13|2013-07-25 00:00:...|             9149|PENDING_PAYMENT|     2013-07|\n",
      "|      14|2013-07-25 00:00:...|             9842|     PROCESSING|     2013-07|\n",
      "|      15|2013-07-25 00:00:...|             2568|       COMPLETE|     2013-07|\n",
      "|      16|2013-07-25 00:00:...|             7276|PENDING_PAYMENT|     2013-07|\n",
      "|      17|2013-07-25 00:00:...|             2667|       COMPLETE|     2013-07|\n",
      "|      18|2013-07-25 00:00:...|             1205|         CLOSED|     2013-07|\n",
      "|      19|2013-07-25 00:00:...|             9488|PENDING_PAYMENT|     2013-07|\n",
      "|      20|2013-07-25 00:00:...|             9198|     PROCESSING|     2013-07|\n",
      "+--------+--------------------+-----------------+---------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## with column adds a new column\n",
    "\n",
    "orders.withColumn(\"order_months\",date_format(\"order_date\",\"yyyy-MM\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b3623de8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+-----------------+---------------+\n",
      "|order_id|          order_date|order_customer_id|   order_status|\n",
      "+--------+--------------------+-----------------+---------------+\n",
      "|   25876|2014-01-01 00:00:...|             3414|PENDING_PAYMENT|\n",
      "|   25877|2014-01-01 00:00:...|             5549|PENDING_PAYMENT|\n",
      "|   25878|2014-01-01 00:00:...|             9084|        PENDING|\n",
      "|   25879|2014-01-01 00:00:...|             5118|        PENDING|\n",
      "|   25880|2014-01-01 00:00:...|            10146|       CANCELED|\n",
      "|   25881|2014-01-01 00:00:...|             3205|PENDING_PAYMENT|\n",
      "|   25882|2014-01-01 00:00:...|             4598|       COMPLETE|\n",
      "|   25883|2014-01-01 00:00:...|            11764|        PENDING|\n",
      "|   25884|2014-01-01 00:00:...|             7904|PENDING_PAYMENT|\n",
      "|   25885|2014-01-01 00:00:...|             7253|        PENDING|\n",
      "|   25886|2014-01-01 00:00:...|             8195|     PROCESSING|\n",
      "|   25887|2014-01-01 00:00:...|            10062|        PENDING|\n",
      "|   25888|2014-01-01 00:00:...|             6735|       COMPLETE|\n",
      "|   25889|2014-01-01 00:00:...|            10045|       COMPLETE|\n",
      "|   25890|2014-01-01 00:00:...|             2581|        PENDING|\n",
      "|   25891|2014-01-01 00:00:...|             3037|         CLOSED|\n",
      "|   25892|2014-01-01 00:00:...|             3853|        ON_HOLD|\n",
      "|   25893|2014-01-01 00:00:...|             8679|PENDING_PAYMENT|\n",
      "|   25894|2014-01-01 00:00:...|             7839|     PROCESSING|\n",
      "|   25895|2014-01-01 00:00:...|             1044|       COMPLETE|\n",
      "+--------+--------------------+-----------------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# To filter\n",
    "\n",
    "orders.\\\n",
    "filter(date_format(\"order_date\",\"yyyyMM\") ==201401).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ff40c015",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "| order|count|\n",
      "+------+-----+\n",
      "|201401| 5908|\n",
      "|201405| 5467|\n",
      "|201312| 5892|\n",
      "|201310| 5335|\n",
      "|201311| 6381|\n",
      "|201307| 1533|\n",
      "|201407| 4468|\n",
      "|201403| 5778|\n",
      "|201404| 5657|\n",
      "|201402| 5635|\n",
      "|201309| 5841|\n",
      "|201406| 5308|\n",
      "|201308| 5680|\n",
      "+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Group By\n",
    "\n",
    "orders.\\\n",
    "    groupBy(date_format(\"order_date\",\"yyyyMM\").alias(\"order\"))\\\n",
    "    .count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "326a95b3",
   "metadata": {},
   "source": [
    "## Create Dummy Spark Data Frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "b8e9f328",
   "metadata": {},
   "outputs": [],
   "source": [
    "l = [ (\"x\",)]\n",
    "\n",
    "df = spark.createDataFrame(l,\"dummy string\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "f7106284",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|dummy|\n",
      "+-----+\n",
      "|    x|\n",
      "|    y|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "f43069d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---+\n",
      "|current_date| , |\n",
      "+------------+---+\n",
      "|  2023-06-30| , |\n",
      "+------------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import current_date, lit\n",
    "\n",
    "df.select(current_date().alias(\"current_date\"),lit(\", \")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "5c24f873",
   "metadata": {},
   "outputs": [],
   "source": [
    "employees = [\n",
    "    (1, \"Scott\", \"Tiger\", 1000.0, \n",
    "      \"united states\", \"+1 123 456 7890\", \"123 45 6789\"\n",
    "    ),\n",
    "     (2, \"Henry\", \"Ford\", 1250.0, \n",
    "      \"India\", \"+91 234 567 8901\", \"456 78 9123\"\n",
    "     ),\n",
    "     (3, \"Nick\", \"Junior\", 750.0, \n",
    "      \"united KINGDOM\", \"+44 111 111 1111\", \"222 33 4444\"\n",
    "     ),\n",
    "     (4, \"Bill\", \"Gomes\", 1500.0, \n",
    "      \"AUSTRALIA\", \"+61 987 654 3210\", \"789 12 6118\"\n",
    "     )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "708334b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+---------+------+--------------+----------------+-----------+\n",
      "| id|first_name|last_name|  cost|   nationality|           phone|        ssn|\n",
      "+---+----------+---------+------+--------------+----------------+-----------+\n",
      "|  1|     Scott|    Tiger|1000.0| united states| +1 123 456 7890|123 45 6789|\n",
      "|  2|     Henry|     Ford|1250.0|         India|+91 234 567 8901|456 78 9123|\n",
      "|  3|      Nick|   Junior| 750.0|united KINGDOM|+44 111 111 1111|222 33 4444|\n",
      "|  4|      Bill|    Gomes|1500.0|     AUSTRALIA|+61 987 654 3210|789 12 6118|\n",
      "+---+----------+---------+------+--------------+----------------+-----------+\n",
      "\n",
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- first_name: string (nullable = true)\n",
      " |-- last_name: string (nullable = true)\n",
      " |-- cost: float (nullable = true)\n",
      " |-- nationality: string (nullable = true)\n",
      " |-- phone: string (nullable = true)\n",
      " |-- ssn: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "e_df = spark.createDataFrame(employees,\"\"\"\n",
    "\n",
    "id int,first_name string,last_name string,cost float,nationality string,\n",
    "phone string,ssn string\n",
    "\n",
    "\"\"\")\n",
    "\n",
    "e_df.show()\n",
    "\n",
    "e_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a3065eb",
   "metadata": {},
   "source": [
    "## Categories of Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "98a92ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are approximately 300 functions under pyspark.sql.functions. At a higher level they can be grouped into a few categories.\n",
    "\n",
    "# String Manipulation Functions\n",
    "\n",
    "# Case Conversion - lower,  upper\n",
    "# Getting Length -  length\n",
    "# Extracting substrings - substring, split\n",
    "# Trimming - trim, ltrim, rtrim\n",
    "# Padding - lpad, rpad\n",
    "# Concatenating string - concat, concat_ws\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Date Manipulation Functions\n",
    "\n",
    "# Getting current date and time - current_date, current_timestamp\n",
    "# Date Arithmetic - date_add, date_sub, datediff, months_between, add_months, next_day\n",
    "# Beginning and Ending Date or Time - last_day, trunc, date_trunc\n",
    "# Formatting Date - date_format, to_date\n",
    "# Extracting Information - dayofyear, dayofmonth, dayofweek, year, month\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Aggregate Functions\n",
    "\n",
    "# count, countDistinct\n",
    "# sum, avg\n",
    "# min, max\n",
    "# Other Functions - We will explore depending on the use cases.\n",
    "# CASE and WHEN\n",
    "# CAST for type casting\n",
    "# Functions to manage special types such as ARRAY, MAP, STRUCT type columns\n",
    "# Many others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "6cf9d9d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function lower in module pyspark.sql.functions:\n",
      "\n",
      "lower(col)\n",
      "    Converts a string expression to lower case.\n",
      "    \n",
      "    .. versionadded:: 1.5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lower\n",
    "\n",
    "help(lower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "f63f8093",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|       country|\n",
      "+--------------+\n",
      "| united states|\n",
      "|         india|\n",
      "|united kingdom|\n",
      "|     australia|\n",
      "+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "e_df.select(lower(\"nationality\").alias(\"country\")).show()\n",
    "\n",
    "# converted all letters to smaller case"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e71480a3",
   "metadata": {},
   "source": [
    "## help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "cf76b5f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function date_format in module pyspark.sql.functions:\n",
      "\n",
      "date_format(date, format)\n",
      "    Converts a date/timestamp/string to a value of string in the format specified by the date\n",
      "    format given by the second argument.\n",
      "    \n",
      "    A pattern could be for instance `dd.MM.yyyy` and could return a string like '18.03.1993'. All\n",
      "    pattern letters of `datetime pattern`_. can be used.\n",
      "    \n",
      "    .. _datetime pattern: https://spark.apache.org/docs/latest/sql-ref-datetime-pattern.html\n",
      "    \n",
      "    .. versionadded:: 1.5.0\n",
      "    \n",
      "    Notes\n",
      "    -----\n",
      "    Whenever possible, use specialized functions like `year`.\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> df = spark.createDataFrame([('2015-04-08',)], ['dt'])\n",
      "    >>> df.select(date_format('dt', 'MM/dd/yyy').alias('date')).collect()\n",
      "    [Row(date='04/08/2015')]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(date_format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "e18f0e7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function concat in module pyspark.sql.functions:\n",
      "\n",
      "concat(*cols)\n",
      "    Concatenates multiple input columns together into a single column.\n",
      "    The function works with strings, binary and compatible array columns.\n",
      "    \n",
      "    .. versionadded:: 1.5.0\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> df = spark.createDataFrame([('abcd','123')], ['s', 'd'])\n",
      "    >>> df.select(concat(df.s, df.d).alias('s')).collect()\n",
      "    [Row(s='abcd123')]\n",
      "    \n",
      "    >>> df = spark.createDataFrame([([1, 2], [3, 4], [5]), ([1, 2], None, [3])], ['a', 'b', 'c'])\n",
      "    >>> df.select(concat(df.a, df.b, df.c).alias(\"arr\")).collect()\n",
      "    [Row(arr=[1, 2, 3, 4, 5]), Row(arr=None)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import concat,lit,date_format,upper,col,concat_ws\n",
    "\n",
    "\n",
    "help(concat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "7d45ffdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|    srr|\n",
      "+-------+\n",
      "|abcd123|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([('abcd','123')], ['ins', 'd'])\n",
    "df.select(concat(df.ins, df.d).alias('srr')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "b0804731",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|    srr|\n",
      "+-------+\n",
      "|abcd123|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([('abcd','123')], ['ins STRING ', 'd STRING'])\n",
    "\n",
    "\n",
    "# using the data types in schema, the column name even have those data types\n",
    "\n",
    "\n",
    "df.select(concat('ins STRING ', 'd STRING').alias('srr')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "818a7054",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ins STRING ', 'd STRING']"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "d8ba12f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function concat_ws in module pyspark.sql.functions:\n",
      "\n",
      "concat_ws(sep, *cols)\n",
      "    Concatenates multiple input string columns together into a single string column,\n",
      "    using the given separator.\n",
      "    \n",
      "    .. versionadded:: 1.5.0\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> df = spark.createDataFrame([('abcd','123')], ['s', 'd'])\n",
      "    >>> df.select(concat_ws('-', df.s, df.d).alias('s')).collect()\n",
      "    [Row(s='abcd-123')]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(concat_ws)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "c63216aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|                 s|\n",
      "+------------------+\n",
      "|abcd   pppp ---123|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([('abcd','123')], ['s', 'd'])\n",
    "df.select(concat_ws('   pppp ---', df.s, df.d).alias('s')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e26f123",
   "metadata": {},
   "source": [
    "## Special Functions - col and lit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "f4417b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us understand special functions such as col and lit. \n",
    "# These functions are typically used to convert the strings to column type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "63bc0752",
   "metadata": {},
   "outputs": [],
   "source": [
    "employees = [(1, \"Scott\", \"Tiger\", 1000.0, \n",
    "                      \"united states\", \"+1 123 456 7890\", \"123 45 6789\"\n",
    "                     ),\n",
    "                     (4, \"Henry\", \"Ford\", 1250.0, \n",
    "                      \"India\", \"+91 234 567 8901\", \"456 78 9123\"\n",
    "                     ),\n",
    "                     (3, \"Nick\", \"Junior\", 750.0, \n",
    "                      \"united KINGDOM\", \"+44 111 111 1111\", \"222 33 4444\"\n",
    "                     ),\n",
    "                     (4, \"Bill\", \"Gomes\", 1500.0, \n",
    "                      \"AUSTRALIA\", \"+61 987 654 3210\", \"789 12 6118\"\n",
    "                     )\n",
    "                ]\n",
    "\n",
    "\n",
    "e2_df = spark. \\\n",
    "    createDataFrame(employees,\n",
    "                    schema=\"\"\"employee_id INT, first_name STRING, \n",
    "                    last_name STRING, salary FLOAT, nationality STRING,\n",
    "                    phone_number STRING, ssn STRING\"\"\"\n",
    "                   )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "5e223b10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----+\n",
      "|   nationality|count|\n",
      "+--------------+-----+\n",
      "| united states|    1|\n",
      "|         India|    1|\n",
      "|united KINGDOM|    1|\n",
      "|     AUSTRALIA|    1|\n",
      "+--------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "e2_df.groupBy(\"nationality\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "9d1e7e12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+------+--------------+----------------+-----------+\n",
      "|employee_id|first_name|last_name|salary|   nationality|    phone_number|        ssn|\n",
      "+-----------+----------+---------+------+--------------+----------------+-----------+\n",
      "|          4|      Bill|    Gomes|1500.0|     AUSTRALIA|+61 987 654 3210|789 12 6118|\n",
      "|          4|     Henry|     Ford|1250.0|         India|+91 234 567 8901|456 78 9123|\n",
      "|          3|      Nick|   Junior| 750.0|united KINGDOM|+44 111 111 1111|222 33 4444|\n",
      "|          1|     Scott|    Tiger|1000.0| united states| +1 123 456 7890|123 45 6789|\n",
      "+-----------+----------+---------+------+--------------+----------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "e2_df.orderBy(\"first_name\",\"employee_id\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "77533264",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function col in module pyspark.sql.functions:\n",
      "\n",
      "col(col)\n",
      "    Returns a :class:`~pyspark.sql.Column` based on the given column name.'\n",
      "    Examples\n",
      "    --------\n",
      "    >>> col('x')\n",
      "    Column<'x'>\n",
      "    >>> column('x')\n",
      "    Column<'x'>\n",
      "    \n",
      "    .. versionadded:: 1.3\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "2e758b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also, if we want to use functions such as alias, desc etc on columns \n",
    "# then we have to pass the column names as column type object (not as strings)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "558f7141",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'desc'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/7z/zf4v6y0d1q5bnv1v6p7tmvq80000gn/T/ipykernel_88737/3265669319.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0me2_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0morderBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"employee_id\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdesc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'desc'"
     ]
    }
   ],
   "source": [
    "e2_df. \\\n",
    "    orderBy(\"employee_id\".desc()). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "253bb052",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+------+--------------+----------------+-----------+\n",
      "|employee_id|first_name|last_name|salary|   nationality|    phone_number|        ssn|\n",
      "+-----------+----------+---------+------+--------------+----------------+-----------+\n",
      "|          4|      Bill|    Gomes|1500.0|     AUSTRALIA|+61 987 654 3210|789 12 6118|\n",
      "|          4|     Henry|     Ford|1250.0|         India|+91 234 567 8901|456 78 9123|\n",
      "|          3|      Nick|   Junior| 750.0|united KINGDOM|+44 111 111 1111|222 33 4444|\n",
      "|          1|     Scott|    Tiger|1000.0| united states| +1 123 456 7890|123 45 6789|\n",
      "+-----------+----------+---------+------+--------------+----------------+-----------+\n",
      "\n",
      "+-----------+----------+---------+------+--------------+----------------+-----------+\n",
      "|employee_id|first_name|last_name|salary|   nationality|    phone_number|        ssn|\n",
      "+-----------+----------+---------+------+--------------+----------------+-----------+\n",
      "|          1|     Scott|    Tiger|1000.0| united states| +1 123 456 7890|123 45 6789|\n",
      "|          3|      Nick|   Junior| 750.0|united KINGDOM|+44 111 111 1111|222 33 4444|\n",
      "|          4|     Henry|     Ford|1250.0|         India|+91 234 567 8901|456 78 9123|\n",
      "|          4|      Bill|    Gomes|1500.0|     AUSTRALIA|+61 987 654 3210|789 12 6118|\n",
      "+-----------+----------+---------+------+--------------+----------------+-----------+\n",
      "\n",
      "+-----------+----------+---------+------+--------------+----------------+-----------+\n",
      "|employee_id|first_name|last_name|salary|   nationality|    phone_number|        ssn|\n",
      "+-----------+----------+---------+------+--------------+----------------+-----------+\n",
      "|          4|     Henry|     Ford|1250.0|         India|+91 234 567 8901|456 78 9123|\n",
      "|          4|      Bill|    Gomes|1500.0|     AUSTRALIA|+61 987 654 3210|789 12 6118|\n",
      "|          3|      Nick|   Junior| 750.0|united KINGDOM|+44 111 111 1111|222 33 4444|\n",
      "|          1|     Scott|    Tiger|1000.0| united states| +1 123 456 7890|123 45 6789|\n",
      "+-----------+----------+---------+------+--------------+----------------+-----------+\n",
      "\n",
      "+-----------+----------+---------+------+--------------+----------------+-----------+\n",
      "|employee_id|first_name|last_name|salary|   nationality|    phone_number|        ssn|\n",
      "+-----------+----------+---------+------+--------------+----------------+-----------+\n",
      "|          4|      Bill|    Gomes|1500.0|     AUSTRALIA|+61 987 654 3210|789 12 6118|\n",
      "|          4|     Henry|     Ford|1250.0|         India|+91 234 567 8901|456 78 9123|\n",
      "|          3|      Nick|   Junior| 750.0|united KINGDOM|+44 111 111 1111|222 33 4444|\n",
      "|          1|     Scott|    Tiger|1000.0| united states| +1 123 456 7890|123 45 6789|\n",
      "+-----------+----------+---------+------+--------------+----------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "e2_df. \\\n",
    "    orderBy(col(\"employee_id\").desc()). \\\n",
    "    show()\n",
    "\n",
    "e2_df. \\\n",
    "    orderBy(col(\"first_name\").desc()). \\\n",
    "    show()\n",
    "\n",
    "# using dataframe name to access the columns\n",
    "e2_df. \\\n",
    "    orderBy(e2_df.employee_id.desc()). \\\n",
    "    show()\n",
    "\n",
    "# upper,desc,alias\n",
    "\n",
    "e2_df. \\\n",
    "    orderBy(upper(e2_df.first_name).alias(\"name\").asc()). \\\n",
    "    show()\n",
    "\n",
    "# for the above one,conversion to Upper is used in sorting. so the projection didnt happen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "c9a2fb8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+------+--------------+----------------+-----------+\n",
      "|employee_id|first_name|last_name|salary|   nationality|    phone_number|        ssn|\n",
      "+-----------+----------+---------+------+--------------+----------------+-----------+\n",
      "|          4|      Bill|    Gomes|1500.0|     AUSTRALIA|+61 987 654 3210|789 12 6118|\n",
      "|          4|     Henry|     Ford|1250.0|         India|+91 234 567 8901|456 78 9123|\n",
      "|          3|      Nick|   Junior| 750.0|united KINGDOM|+44 111 111 1111|222 33 4444|\n",
      "|          1|     Scott|    Tiger|1000.0| united states| +1 123 456 7890|123 45 6789|\n",
      "+-----------+----------+---------+------+--------------+----------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "e2_df. \\\n",
    "    orderBy(upper(e2_df[\"first_name\"]).alias(\"name\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "68086cdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function upper in module pyspark.sql.functions:\n",
      "\n",
      "upper(col)\n",
      "    Converts a string expression to upper case.\n",
      "    \n",
      "    .. versionadded:: 1.5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(upper)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "2c9ad2d6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function alias in module pyspark.sql.column:\n",
      "\n",
      "alias(self, *alias, **kwargs)\n",
      "    Returns this column aliased with a new name or names (in the case of expressions that\n",
      "    return more than one column, such as explode).\n",
      "    \n",
      "    .. versionadded:: 1.3.0\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    alias : str\n",
      "        desired column names (collects all positional arguments passed)\n",
      "    \n",
      "    Other Parameters\n",
      "    ----------------\n",
      "    metadata: dict\n",
      "        a dict of information to be stored in ``metadata`` attribute of the\n",
      "        corresponding :class:`StructField <pyspark.sql.types.StructField>` (optional, keyword\n",
      "        only argument)\n",
      "    \n",
      "        .. versionchanged:: 2.2.0\n",
      "           Added optional ``metadata`` argument.\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> df.select(df.age.alias(\"age2\")).collect()\n",
      "    [Row(age2=2), Row(age2=5)]\n",
      "    >>> df.select(df.age.alias(\"age3\", metadata={'max': 99})).schema['age3'].metadata['max']\n",
      "    99\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.column import Column\n",
    "\n",
    "help(Column.alias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b89aa8c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4af167",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "231bd741",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "87b4ca2e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "cannot resolve '`, `' given input columns: [employee_id, first_name, last_name, nationality, phone_number, salary, ssn];\n'Project [unresolvedalias(concat(first_name#1757, ', , last_name#1758), Some(org.apache.spark.sql.Column$$Lambda$2698/0x00000008011aa040@48273533))]\n+- LogicalRDD [employee_id#1756, first_name#1757, last_name#1758, salary#1759, nationality#1760, phone_number#1761, ssn#1762], false\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/7z/zf4v6y0d1q5bnv1v6p7tmvq80000gn/T/ipykernel_88737/2155205284.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0me2_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"first_name\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\", \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"last_name\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, *cols)\u001b[0m\n\u001b[1;32m   1683\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Alice'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Bob'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1684\u001b[0m         \"\"\"\n\u001b[0;32m-> 1685\u001b[0;31m         \u001b[0mjdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jcols\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1686\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.9/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1321\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1322\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.9/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: cannot resolve '`, `' given input columns: [employee_id, first_name, last_name, nationality, phone_number, salary, ssn];\n'Project [unresolvedalias(concat(first_name#1757, ', , last_name#1758), Some(org.apache.spark.sql.Column$$Lambda$2698/0x00000008011aa040@48273533))]\n+- LogicalRDD [employee_id#1756, first_name#1757, last_name#1758, salary#1759, nationality#1760, phone_number#1761, ssn#1762], false\n"
     ]
    }
   ],
   "source": [
    "# Sometimes, we want to add a literal to the column values. \n",
    "# For example, we might want to concatenate first_name and last_name separated by comma and space in between.\n",
    "\n",
    "\n",
    "# Gives error as \", \" is not a column\n",
    "\n",
    "e2_df. \\\n",
    "    select(concat(col(\"first_name\"), \", \", col(\"last_name\"))). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "3087f1c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|   full_name|\n",
      "+------------+\n",
      "|Scott, Tiger|\n",
      "| Henry, Ford|\n",
      "|Nick, Junior|\n",
      "| Bill, Gomes|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "e2_df. \\\n",
    "    select(concat(col(\"first_name\"), \n",
    "                  lit(\", \"), \n",
    "                  col(\"last_name\")).alias(\"full_name\")). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "e32298a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.column import Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "b8fa36b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class Column in module pyspark.sql.column:\n",
      "\n",
      "class Column(builtins.object)\n",
      " |  Column(jc)\n",
      " |  \n",
      " |  A column in a DataFrame.\n",
      " |  \n",
      " |  :class:`Column` instances can be created by::\n",
      " |  \n",
      " |      # 1. Select a column out of a DataFrame\n",
      " |  \n",
      " |      df.colName\n",
      " |      df[\"colName\"]\n",
      " |  \n",
      " |      # 2. Create from an expression\n",
      " |      df.colName + 1\n",
      " |      1 / df.colName\n",
      " |  \n",
      " |  .. versionadded:: 1.3.0\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __add__ = _(self, other)\n",
      " |      binary operator\n",
      " |  \n",
      " |  __and__ = _(self, other)\n",
      " |      binary operator\n",
      " |  \n",
      " |  __bool__ = __nonzero__(self)\n",
      " |  \n",
      " |  __contains__(self, item)\n",
      " |      # container operators\n",
      " |  \n",
      " |  __div__ = _(self, other)\n",
      " |      binary operator\n",
      " |  \n",
      " |  __eq__ = _(self, other)\n",
      " |      binary operator\n",
      " |  \n",
      " |  __ge__ = _(self, other)\n",
      " |      binary operator\n",
      " |  \n",
      " |  __getattr__(self, item)\n",
      " |  \n",
      " |  __getitem__(self, k)\n",
      " |  \n",
      " |  __gt__ = _(self, other)\n",
      " |      binary operator\n",
      " |  \n",
      " |  __init__(self, jc)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  __invert__ = _(self)\n",
      " |  \n",
      " |  __iter__(self)\n",
      " |  \n",
      " |  __le__ = _(self, other)\n",
      " |      binary operator\n",
      " |  \n",
      " |  __lt__ = _(self, other)\n",
      " |      binary operator\n",
      " |  \n",
      " |  __mod__ = _(self, other)\n",
      " |      binary operator\n",
      " |  \n",
      " |  __mul__ = _(self, other)\n",
      " |      binary operator\n",
      " |  \n",
      " |  __ne__ = _(self, other)\n",
      " |      binary operator\n",
      " |  \n",
      " |  __neg__ = _(self)\n",
      " |  \n",
      " |  __nonzero__(self)\n",
      " |  \n",
      " |  __or__ = _(self, other)\n",
      " |      binary operator\n",
      " |  \n",
      " |  __pow__ = _(self, other)\n",
      " |      binary function\n",
      " |  \n",
      " |  __radd__ = _(self, other)\n",
      " |      binary operator\n",
      " |  \n",
      " |  __rand__ = _(self, other)\n",
      " |      binary operator\n",
      " |  \n",
      " |  __rdiv__ = _(self, other)\n",
      " |      binary operator\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __rmod__ = _(self, other)\n",
      " |      binary operator\n",
      " |  \n",
      " |  __rmul__ = _(self, other)\n",
      " |      binary operator\n",
      " |  \n",
      " |  __ror__ = _(self, other)\n",
      " |      binary operator\n",
      " |  \n",
      " |  __rpow__ = _(self, other)\n",
      " |      binary function\n",
      " |  \n",
      " |  __rsub__ = _(self, other)\n",
      " |      binary operator\n",
      " |  \n",
      " |  __rtruediv__ = _(self, other)\n",
      " |      binary operator\n",
      " |  \n",
      " |  __sub__ = _(self, other)\n",
      " |      binary operator\n",
      " |  \n",
      " |  __truediv__ = _(self, other)\n",
      " |      binary operator\n",
      " |  \n",
      " |  alias(self, *alias, **kwargs)\n",
      " |      Returns this column aliased with a new name or names (in the case of expressions that\n",
      " |      return more than one column, such as explode).\n",
      " |      \n",
      " |      .. versionadded:: 1.3.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      alias : str\n",
      " |          desired column names (collects all positional arguments passed)\n",
      " |      \n",
      " |      Other Parameters\n",
      " |      ----------------\n",
      " |      metadata: dict\n",
      " |          a dict of information to be stored in ``metadata`` attribute of the\n",
      " |          corresponding :class:`StructField <pyspark.sql.types.StructField>` (optional, keyword\n",
      " |          only argument)\n",
      " |      \n",
      " |          .. versionchanged:: 2.2.0\n",
      " |             Added optional ``metadata`` argument.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df.select(df.age.alias(\"age2\")).collect()\n",
      " |      [Row(age2=2), Row(age2=5)]\n",
      " |      >>> df.select(df.age.alias(\"age3\", metadata={'max': 99})).schema['age3'].metadata['max']\n",
      " |      99\n",
      " |  \n",
      " |  asc = _(self)\n",
      " |      Returns a sort expression based on ascending order of the column.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> from pyspark.sql import Row\n",
      " |      >>> df = spark.createDataFrame([('Tom', 80), ('Alice', None)], [\"name\", \"height\"])\n",
      " |      >>> df.select(df.name).orderBy(df.name.asc()).collect()\n",
      " |      [Row(name='Alice'), Row(name='Tom')]\n",
      " |  \n",
      " |  asc_nulls_first = _(self)\n",
      " |      Returns a sort expression based on ascending order of the column, and null values\n",
      " |      return before non-null values.\n",
      " |      \n",
      " |      .. versionadded:: 2.4.0\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> from pyspark.sql import Row\n",
      " |      >>> df = spark.createDataFrame([('Tom', 80), (None, 60), ('Alice', None)], [\"name\", \"height\"])\n",
      " |      >>> df.select(df.name).orderBy(df.name.asc_nulls_first()).collect()\n",
      " |      [Row(name=None), Row(name='Alice'), Row(name='Tom')]\n",
      " |  \n",
      " |  asc_nulls_last = _(self)\n",
      " |      Returns a sort expression based on ascending order of the column, and null values\n",
      " |      appear after non-null values.\n",
      " |      \n",
      " |      .. versionadded:: 2.4.0\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> from pyspark.sql import Row\n",
      " |      >>> df = spark.createDataFrame([('Tom', 80), (None, 60), ('Alice', None)], [\"name\", \"height\"])\n",
      " |      >>> df.select(df.name).orderBy(df.name.asc_nulls_last()).collect()\n",
      " |      [Row(name='Alice'), Row(name='Tom'), Row(name=None)]\n",
      " |  \n",
      " |  astype = cast(self, dataType)\n",
      " |      :func:`astype` is an alias for :func:`cast`.\n",
      " |      \n",
      " |      .. versionadded:: 1.4\n",
      " |  \n",
      " |  between(self, lowerBound, upperBound)\n",
      " |      True if the current column is between the lower bound and upper bound, inclusive.\n",
      " |      \n",
      " |      .. versionadded:: 1.3.0\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df.select(df.name, df.age.between(2, 4)).show()\n",
      " |      +-----+---------------------------+\n",
      " |      | name|((age >= 2) AND (age <= 4))|\n",
      " |      +-----+---------------------------+\n",
      " |      |Alice|                       true|\n",
      " |      |  Bob|                      false|\n",
      " |      +-----+---------------------------+\n",
      " |  \n",
      " |  bitwiseAND = _(self, other)\n",
      " |      Compute bitwise AND of this expression with another expression.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      other\n",
      " |          a value or :class:`Column` to calculate bitwise and(&) with\n",
      " |          this :class:`Column`.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> from pyspark.sql import Row\n",
      " |      >>> df = spark.createDataFrame([Row(a=170, b=75)])\n",
      " |      >>> df.select(df.a.bitwiseAND(df.b)).collect()\n",
      " |      [Row((a & b)=10)]\n",
      " |  \n",
      " |  bitwiseOR = _(self, other)\n",
      " |      Compute bitwise OR of this expression with another expression.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      other\n",
      " |          a value or :class:`Column` to calculate bitwise or(|) with\n",
      " |          this :class:`Column`.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> from pyspark.sql import Row\n",
      " |      >>> df = spark.createDataFrame([Row(a=170, b=75)])\n",
      " |      >>> df.select(df.a.bitwiseOR(df.b)).collect()\n",
      " |      [Row((a | b)=235)]\n",
      " |  \n",
      " |  bitwiseXOR = _(self, other)\n",
      " |      Compute bitwise XOR of this expression with another expression.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      other\n",
      " |          a value or :class:`Column` to calculate bitwise xor(^) with\n",
      " |          this :class:`Column`.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> from pyspark.sql import Row\n",
      " |      >>> df = spark.createDataFrame([Row(a=170, b=75)])\n",
      " |      >>> df.select(df.a.bitwiseXOR(df.b)).collect()\n",
      " |      [Row((a ^ b)=225)]\n",
      " |  \n",
      " |  cast(self, dataType)\n",
      " |      Casts the column into type ``dataType``.\n",
      " |      \n",
      " |      .. versionadded:: 1.3.0\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df.select(df.age.cast(\"string\").alias('ages')).collect()\n",
      " |      [Row(ages='2'), Row(ages='5')]\n",
      " |      >>> df.select(df.age.cast(StringType()).alias('ages')).collect()\n",
      " |      [Row(ages='2'), Row(ages='5')]\n",
      " |  \n",
      " |  contains = _(self, other)\n",
      " |      Contains the other element. Returns a boolean :class:`Column` based on a string match.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      other\n",
      " |          string in line. A value as a literal or a :class:`Column`.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df.filter(df.name.contains('o')).collect()\n",
      " |      [Row(age=5, name='Bob')]\n",
      " |  \n",
      " |  desc = _(self)\n",
      " |      Returns a sort expression based on the descending order of the column.\n",
      " |      \n",
      " |      .. versionadded:: 2.4.0\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> from pyspark.sql import Row\n",
      " |      >>> df = spark.createDataFrame([('Tom', 80), ('Alice', None)], [\"name\", \"height\"])\n",
      " |      >>> df.select(df.name).orderBy(df.name.desc()).collect()\n",
      " |      [Row(name='Tom'), Row(name='Alice')]\n",
      " |  \n",
      " |  desc_nulls_first = _(self)\n",
      " |      Returns a sort expression based on the descending order of the column, and null values\n",
      " |      appear before non-null values.\n",
      " |      \n",
      " |      .. versionadded:: 2.4.0\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> from pyspark.sql import Row\n",
      " |      >>> df = spark.createDataFrame([('Tom', 80), (None, 60), ('Alice', None)], [\"name\", \"height\"])\n",
      " |      >>> df.select(df.name).orderBy(df.name.desc_nulls_first()).collect()\n",
      " |      [Row(name=None), Row(name='Tom'), Row(name='Alice')]\n",
      " |  \n",
      " |  desc_nulls_last = _(self)\n",
      " |      Returns a sort expression based on the descending order of the column, and null values\n",
      " |      appear after non-null values.\n",
      " |      \n",
      " |      .. versionadded:: 2.4.0\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> from pyspark.sql import Row\n",
      " |      >>> df = spark.createDataFrame([('Tom', 80), (None, 60), ('Alice', None)], [\"name\", \"height\"])\n",
      " |      >>> df.select(df.name).orderBy(df.name.desc_nulls_last()).collect()\n",
      " |      [Row(name='Tom'), Row(name='Alice'), Row(name=None)]\n",
      " |  \n",
      " |  dropFields(self, *fieldNames)\n",
      " |      An expression that drops fields in :class:`StructType` by name.\n",
      " |      This is a no-op if schema doesn't contain field name(s).\n",
      " |      \n",
      " |      .. versionadded:: 3.1.0\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> from pyspark.sql import Row\n",
      " |      >>> from pyspark.sql.functions import col, lit\n",
      " |      >>> df = spark.createDataFrame([\n",
      " |      ...     Row(a=Row(b=1, c=2, d=3, e=Row(f=4, g=5, h=6)))])\n",
      " |      >>> df.withColumn('a', df['a'].dropFields('b')).show()\n",
      " |      +-----------------+\n",
      " |      |                a|\n",
      " |      +-----------------+\n",
      " |      |{2, 3, {4, 5, 6}}|\n",
      " |      +-----------------+\n",
      " |      \n",
      " |      >>> df.withColumn('a', df['a'].dropFields('b', 'c')).show()\n",
      " |      +--------------+\n",
      " |      |             a|\n",
      " |      +--------------+\n",
      " |      |{3, {4, 5, 6}}|\n",
      " |      +--------------+\n",
      " |      \n",
      " |      This method supports dropping multiple nested fields directly e.g.\n",
      " |      \n",
      " |      >>> df.withColumn(\"a\", col(\"a\").dropFields(\"e.g\", \"e.h\")).show()\n",
      " |      +--------------+\n",
      " |      |             a|\n",
      " |      +--------------+\n",
      " |      |{1, 2, 3, {4}}|\n",
      " |      +--------------+\n",
      " |      \n",
      " |      However, if you are going to add/replace multiple nested fields,\n",
      " |      it is preferred to extract out the nested struct before\n",
      " |      adding/replacing multiple fields e.g.\n",
      " |      \n",
      " |      >>> df.select(col(\"a\").withField(\n",
      " |      ...     \"e\", col(\"a.e\").dropFields(\"g\", \"h\")).alias(\"a\")\n",
      " |      ... ).show()\n",
      " |      +--------------+\n",
      " |      |             a|\n",
      " |      +--------------+\n",
      " |      |{1, 2, 3, {4}}|\n",
      " |      +--------------+\n",
      " |  \n",
      " |  endswith = _(self, other)\n",
      " |      String ends with. Returns a boolean :class:`Column` based on a string match.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      other : :class:`Column` or str\n",
      " |          string at end of line (do not use a regex `$`)\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df.filter(df.name.endswith('ice')).collect()\n",
      " |      [Row(age=2, name='Alice')]\n",
      " |      >>> df.filter(df.name.endswith('ice$')).collect()\n",
      " |      []\n",
      " |  \n",
      " |  eqNullSafe = _(self, other)\n",
      " |      Equality test that is safe for null values.\n",
      " |      \n",
      " |      .. versionadded:: 2.3.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      other\n",
      " |          a value or :class:`Column`\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> from pyspark.sql import Row\n",
      " |      >>> df1 = spark.createDataFrame([\n",
      " |      ...     Row(id=1, value='foo'),\n",
      " |      ...     Row(id=2, value=None)\n",
      " |      ... ])\n",
      " |      >>> df1.select(\n",
      " |      ...     df1['value'] == 'foo',\n",
      " |      ...     df1['value'].eqNullSafe('foo'),\n",
      " |      ...     df1['value'].eqNullSafe(None)\n",
      " |      ... ).show()\n",
      " |      +-------------+---------------+----------------+\n",
      " |      |(value = foo)|(value <=> foo)|(value <=> NULL)|\n",
      " |      +-------------+---------------+----------------+\n",
      " |      |         true|           true|           false|\n",
      " |      |         null|          false|            true|\n",
      " |      +-------------+---------------+----------------+\n",
      " |      >>> df2 = spark.createDataFrame([\n",
      " |      ...     Row(value = 'bar'),\n",
      " |      ...     Row(value = None)\n",
      " |      ... ])\n",
      " |      >>> df1.join(df2, df1[\"value\"] == df2[\"value\"]).count()\n",
      " |      0\n",
      " |      >>> df1.join(df2, df1[\"value\"].eqNullSafe(df2[\"value\"])).count()\n",
      " |      1\n",
      " |      >>> df2 = spark.createDataFrame([\n",
      " |      ...     Row(id=1, value=float('NaN')),\n",
      " |      ...     Row(id=2, value=42.0),\n",
      " |      ...     Row(id=3, value=None)\n",
      " |      ... ])\n",
      " |      >>> df2.select(\n",
      " |      ...     df2['value'].eqNullSafe(None),\n",
      " |      ...     df2['value'].eqNullSafe(float('NaN')),\n",
      " |      ...     df2['value'].eqNullSafe(42.0)\n",
      " |      ... ).show()\n",
      " |      +----------------+---------------+----------------+\n",
      " |      |(value <=> NULL)|(value <=> NaN)|(value <=> 42.0)|\n",
      " |      +----------------+---------------+----------------+\n",
      " |      |           false|           true|           false|\n",
      " |      |           false|          false|            true|\n",
      " |      |            true|          false|           false|\n",
      " |      +----------------+---------------+----------------+\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      Unlike Pandas, PySpark doesn't consider NaN values to be NULL. See the\n",
      " |      `NaN Semantics <https://spark.apache.org/docs/latest/sql-ref-datatypes.html#nan-semantics>`_\n",
      " |      for details.\n",
      " |  \n",
      " |  getField(self, name)\n",
      " |      An expression that gets a field by name in a :class:`StructType`.\n",
      " |      \n",
      " |      .. versionadded:: 1.3.0\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> from pyspark.sql import Row\n",
      " |      >>> df = spark.createDataFrame([Row(r=Row(a=1, b=\"b\"))])\n",
      " |      >>> df.select(df.r.getField(\"b\")).show()\n",
      " |      +---+\n",
      " |      |r.b|\n",
      " |      +---+\n",
      " |      |  b|\n",
      " |      +---+\n",
      " |      >>> df.select(df.r.a).show()\n",
      " |      +---+\n",
      " |      |r.a|\n",
      " |      +---+\n",
      " |      |  1|\n",
      " |      +---+\n",
      " |  \n",
      " |  getItem(self, key)\n",
      " |      An expression that gets an item at position ``ordinal`` out of a list,\n",
      " |      or gets an item by key out of a dict.\n",
      " |      \n",
      " |      .. versionadded:: 1.3.0\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = spark.createDataFrame([([1, 2], {\"key\": \"value\"})], [\"l\", \"d\"])\n",
      " |      >>> df.select(df.l.getItem(0), df.d.getItem(\"key\")).show()\n",
      " |      +----+------+\n",
      " |      |l[0]|d[key]|\n",
      " |      +----+------+\n",
      " |      |   1| value|\n",
      " |      +----+------+\n",
      " |  \n",
      " |  isNotNull = _(self)\n",
      " |      True if the current expression is NOT null.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> from pyspark.sql import Row\n",
      " |      >>> df = spark.createDataFrame([Row(name='Tom', height=80), Row(name='Alice', height=None)])\n",
      " |      >>> df.filter(df.height.isNotNull()).collect()\n",
      " |      [Row(name='Tom', height=80)]\n",
      " |  \n",
      " |  isNull = _(self)\n",
      " |      True if the current expression is null.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> from pyspark.sql import Row\n",
      " |      >>> df = spark.createDataFrame([Row(name='Tom', height=80), Row(name='Alice', height=None)])\n",
      " |      >>> df.filter(df.height.isNull()).collect()\n",
      " |      [Row(name='Alice', height=None)]\n",
      " |  \n",
      " |  isin(self, *cols)\n",
      " |      A boolean expression that is evaluated to true if the value of this\n",
      " |      expression is contained by the evaluated values of the arguments.\n",
      " |      \n",
      " |      .. versionadded:: 1.5.0\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df[df.name.isin(\"Bob\", \"Mike\")].collect()\n",
      " |      [Row(age=5, name='Bob')]\n",
      " |      >>> df[df.age.isin([1, 2, 3])].collect()\n",
      " |      [Row(age=2, name='Alice')]\n",
      " |  \n",
      " |  like = _(self, other)\n",
      " |      SQL like expression. Returns a boolean :class:`Column` based on a SQL LIKE match.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      other : str\n",
      " |          a SQL LIKE pattern\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      pyspark.sql.Column.rlike\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df.filter(df.name.like('Al%')).collect()\n",
      " |      [Row(age=2, name='Alice')]\n",
      " |  \n",
      " |  name = alias(self, *alias, **kwargs)\n",
      " |      :func:`name` is an alias for :func:`alias`.\n",
      " |      \n",
      " |      .. versionadded:: 2.0\n",
      " |  \n",
      " |  otherwise(self, value)\n",
      " |      Evaluates a list of conditions and returns one of multiple possible result expressions.\n",
      " |      If :func:`Column.otherwise` is not invoked, None is returned for unmatched conditions.\n",
      " |      \n",
      " |      .. versionadded:: 1.4.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      value\n",
      " |          a literal value, or a :class:`Column` expression.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> from pyspark.sql import functions as F\n",
      " |      >>> df.select(df.name, F.when(df.age > 3, 1).otherwise(0)).show()\n",
      " |      +-----+-------------------------------------+\n",
      " |      | name|CASE WHEN (age > 3) THEN 1 ELSE 0 END|\n",
      " |      +-----+-------------------------------------+\n",
      " |      |Alice|                                    0|\n",
      " |      |  Bob|                                    1|\n",
      " |      +-----+-------------------------------------+\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      pyspark.sql.functions.when\n",
      " |  \n",
      " |  over(self, window)\n",
      " |      Define a windowing column.\n",
      " |      \n",
      " |      .. versionadded:: 1.4.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      window : :class:`WindowSpec`\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`Column`\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> from pyspark.sql import Window\n",
      " |      >>> window = Window.partitionBy(\"name\").orderBy(\"age\")                 .rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
      " |      >>> from pyspark.sql.functions import rank, min\n",
      " |      >>> from pyspark.sql.functions import desc\n",
      " |      >>> df.withColumn(\"rank\", rank().over(window))                 .withColumn(\"min\", min('age').over(window)).sort(desc(\"age\")).show()\n",
      " |      +---+-----+----+---+\n",
      " |      |age| name|rank|min|\n",
      " |      +---+-----+----+---+\n",
      " |      |  5|  Bob|   1|  5|\n",
      " |      |  2|Alice|   1|  2|\n",
      " |      +---+-----+----+---+\n",
      " |  \n",
      " |  rlike = _(self, other)\n",
      " |      SQL RLIKE expression (LIKE with Regex). Returns a boolean :class:`Column` based on a regex\n",
      " |      match.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      other : str\n",
      " |          an extended regex expression\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df.filter(df.name.rlike('ice$')).collect()\n",
      " |      [Row(age=2, name='Alice')]\n",
      " |  \n",
      " |  startswith = _(self, other)\n",
      " |      String starts with. Returns a boolean :class:`Column` based on a string match.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      other : :class:`Column` or str\n",
      " |          string at start of line (do not use a regex `^`)\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df.filter(df.name.startswith('Al')).collect()\n",
      " |      [Row(age=2, name='Alice')]\n",
      " |      >>> df.filter(df.name.startswith('^Al')).collect()\n",
      " |      []\n",
      " |  \n",
      " |  substr(self, startPos, length)\n",
      " |      Return a :class:`Column` which is a substring of the column.\n",
      " |      \n",
      " |      .. versionadded:: 1.3.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      startPos : :class:`Column` or int\n",
      " |          start position\n",
      " |      length : :class:`Column` or int\n",
      " |          length of the substring\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df.select(df.name.substr(1, 3).alias(\"col\")).collect()\n",
      " |      [Row(col='Ali'), Row(col='Bob')]\n",
      " |  \n",
      " |  when(self, condition, value)\n",
      " |      Evaluates a list of conditions and returns one of multiple possible result expressions.\n",
      " |      If :func:`Column.otherwise` is not invoked, None is returned for unmatched conditions.\n",
      " |      \n",
      " |      .. versionadded:: 1.4.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      condition : :class:`Column`\n",
      " |          a boolean :class:`Column` expression.\n",
      " |      value\n",
      " |          a literal value, or a :class:`Column` expression.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> from pyspark.sql import functions as F\n",
      " |      >>> df.select(df.name, F.when(df.age > 4, 1).when(df.age < 3, -1).otherwise(0)).show()\n",
      " |      +-----+------------------------------------------------------------+\n",
      " |      | name|CASE WHEN (age > 4) THEN 1 WHEN (age < 3) THEN -1 ELSE 0 END|\n",
      " |      +-----+------------------------------------------------------------+\n",
      " |      |Alice|                                                          -1|\n",
      " |      |  Bob|                                                           1|\n",
      " |      +-----+------------------------------------------------------------+\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      pyspark.sql.functions.when\n",
      " |  \n",
      " |  withField(self, fieldName, col)\n",
      " |      An expression that adds/replaces a field in :class:`StructType` by name.\n",
      " |      \n",
      " |      .. versionadded:: 3.1.0\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> from pyspark.sql import Row\n",
      " |      >>> from pyspark.sql.functions import lit\n",
      " |      >>> df = spark.createDataFrame([Row(a=Row(b=1, c=2))])\n",
      " |      >>> df.withColumn('a', df['a'].withField('b', lit(3))).select('a.b').show()\n",
      " |      +---+\n",
      " |      |  b|\n",
      " |      +---+\n",
      " |      |  3|\n",
      " |      +---+\n",
      " |      >>> df.withColumn('a', df['a'].withField('d', lit(4))).select('a.d').show()\n",
      " |      +---+\n",
      " |      |  d|\n",
      " |      +---+\n",
      " |      |  4|\n",
      " |      +---+\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __hash__ = None\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(Column)\n",
    "\n",
    "\n",
    "# on top of column object, we have alias,asc,desc.asc_null_list,astype\n",
    "\n",
    "# col(\"column name\") - returns a column object\n",
    "\n",
    "# \"column name\" - is a string not a column\n",
    "\n",
    "# we can use these function on clumn object. no compulsory to use only on col(\"column name\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "e181b2ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+------+--------------+----------------+-----------+-----+\n",
      "|employee_id|first_name|last_name|salary|   nationality|    phone_number|        ssn|bonus|\n",
      "+-----------+----------+---------+------+--------------+----------------+-----------+-----+\n",
      "|          1|     Scott|    Tiger|1000.0| united states| +1 123 456 7890|123 45 6789| null|\n",
      "|          4|     Henry|     Ford|1250.0|         India|+91 234 567 8901|456 78 9123| null|\n",
      "|          3|      Nick|   Junior| 750.0|united KINGDOM|+44 111 111 1111|222 33 4444| null|\n",
      "|          4|      Bill|    Gomes|1500.0|     AUSTRALIA|+61 987 654 3210|789 12 6118| null|\n",
      "+-----------+----------+---------+------+--------------+----------------+-----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "e2_df.withColumn(\"bonus\",\"salary\"*lit(0.2)).show()\n",
    "\n",
    "# it will not work as \"salary\" is string and lit(0.2) is a column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "bd8cb73c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+------+--------------+----------------+-----------+-----+\n",
      "|employee_id|first_name|last_name|salary|   nationality|    phone_number|        ssn|bonus|\n",
      "+-----------+----------+---------+------+--------------+----------------+-----------+-----+\n",
      "|          1|     Scott|    Tiger|1000.0| united states| +1 123 456 7890|123 45 6789|200.0|\n",
      "|          4|     Henry|     Ford|1250.0|         India|+91 234 567 8901|456 78 9123|250.0|\n",
      "|          3|      Nick|   Junior| 750.0|united KINGDOM|+44 111 111 1111|222 33 4444|150.0|\n",
      "|          4|      Bill|    Gomes|1500.0|     AUSTRALIA|+61 987 654 3210|789 12 6118|300.0|\n",
      "+-----------+----------+---------+------+--------------+----------------+-----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "e2_df.withColumn(\"bonus\",col(\"salary\")*lit(0.2)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "926a5d6d",
   "metadata": {},
   "source": [
    "## Common String Manipulation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "b3acc9f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenating strings\n",
    "\n",
    "# We can pass a variable number of strings to concat function.\n",
    "# It will return one string concatenating all the strings.\n",
    "# If we have to concatenate literal in between then we have to use lit function.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Case Conversion and Length\n",
    "\n",
    "# Convert all the alphabetic characters in a string to uppercase - upper\n",
    "# Convert all the alphabetic characters in a string to lowercase - lower\n",
    "# Convert first character in a string to uppercase - initcap\n",
    "# Get number of characters in a string - length\n",
    "\n",
    "\n",
    "# All the 4 functions take column type argument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "a486815a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "| full_name|\n",
      "+----------+\n",
      "|ScottTiger|\n",
      "| HenryFord|\n",
      "|NickJunior|\n",
      "| BillGomes|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import concat\n",
    "e2_df. \\\n",
    "    select(concat(\"first_name\", \"last_name\").alias(\"full_name\")). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "be43057f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import concat_ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "ea2b14c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function concat_ws in module pyspark.sql.functions:\n",
      "\n",
      "concat_ws(sep, *cols)\n",
      "    Concatenates multiple input string columns together into a single string column,\n",
      "    using the given separator.\n",
      "    \n",
      "    .. versionadded:: 1.5.0\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> df = spark.createDataFrame([('abcd','123')], ['s', 'd'])\n",
      "    >>> df.select(concat_ws('-', df.s, df.d).alias('s')).collect()\n",
      "    [Row(s='abcd-123')]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(concat_ws)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "f24f8318",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|    concat_ws|\n",
      "+-------------+\n",
      "|Scott, -Tiger|\n",
      "| Henry, -Ford|\n",
      "|Nick, -Junior|\n",
      "| Bill, -Gomes|\n",
      "+-------------+\n",
      "\n",
      "+--------------------------------------+\n",
      "|concat_ws                             |\n",
      "+--------------------------------------+\n",
      "|Scott, -Tiger, -1000.0, -united states|\n",
      "|Henry, -Ford, -1250.0, -India         |\n",
      "|Nick, -Junior, -750.0, -united KINGDOM|\n",
      "|Bill, -Gomes, -1500.0, -AUSTRALIA     |\n",
      "+--------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "e2_df.select(concat_ws(\", -\",\"first_name\",\"last_name\").alias(\"concat_ws\")).show()\n",
    "\n",
    "\n",
    "e2_df.select(concat_ws(\", -\",\"first_name\",\"last_name\",\"salary\",\"nationality\").alias(\"concat_ws\")).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "957cf8f3",
   "metadata": {},
   "source": [
    "### Case Conversion and Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "482bde16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col,lower,upper,initcap,length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "0fc34a05",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function lower in module pyspark.sql.functions:\n",
      "\n",
      "lower(col)\n",
      "    Converts a string expression to lower case.\n",
      "    \n",
      "    .. versionadded:: 1.5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(lower) # - returns a column  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "ae5d0b67",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function initcap in module pyspark.sql.functions:\n",
      "\n",
      "initcap(col)\n",
      "    Translate the first letter of each word to upper case in the sentence.\n",
      "    \n",
      "    .. versionadded:: 1.5.0\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> spark.createDataFrame([('ab cd',)], ['a']).select(initcap(\"a\").alias('v')).collect()\n",
      "    [Row(v='Ab Cd')]\n",
      "\n",
      "Help on function length in module pyspark.sql.functions:\n",
      "\n",
      "length(col)\n",
      "    Computes the character length of string data or number of bytes of binary data.\n",
      "    The length of character data includes the trailing spaces. The length of binary data\n",
      "    includes binary zeros.\n",
      "    \n",
      "    .. versionadded:: 1.5.0\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> spark.createDataFrame([('ABC ',)], ['a']).select(length('a').alias('length')).collect()\n",
      "    [Row(length=4)]\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "help(initcap),help(length) # -both returns a column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "d9b99543",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------+-----------------+-------------------+------+\n",
      "|employee_id|   nationality|nationality_upper|nationality initcap|lenght|\n",
      "+-----------+--------------+-----------------+-------------------+------+\n",
      "|          1| united states|    UNITED STATES|      United States|    13|\n",
      "|          4|         India|            INDIA|              India|     5|\n",
      "|          3|united KINGDOM|   UNITED KINGDOM|     United Kingdom|    14|\n",
      "|          4|     AUSTRALIA|        AUSTRALIA|          Australia|     9|\n",
      "+-----------+--------------+-----------------+-------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "e2_df.select(\"employee_id\",\"nationality\").\\\n",
    "        withColumn(\"nationality_upper\",upper(col(\"nationality\"))).\\\n",
    "        withColumn(\"nationality initcap\",initcap(col(\"nationality\"))).\\\n",
    "        withColumn(\"lenght\",length(col(\"nationality\"))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837313d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7edda75c",
   "metadata": {},
   "source": [
    "## Extracting Strings using substring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "37329e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we are processing fixed length columns then we use substring to extract the information.\n",
    "\n",
    "# Here are some of the examples for fixed length columns and the use cases for which we typically extract information..\n",
    "\n",
    "# 9 Digit Social Security Number. We typically extract last 4 digits and provide it to the tele verification applications..\n",
    "\n",
    "# 16 Digit Credit Card Number. We typically use first 4 digit number to identify Credit Card Provider and last 4 digits for the purpose of tele verification.\n",
    "\n",
    "# Data coming from MainFrames systems are quite often fixed length. We might have to extract the information and store in multiple columns.\n",
    "\n",
    "# substring function takes 3 arguments, column, position, length. We can also provide position from the end by passing negative value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "417cf05d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function substring in module pyspark.sql.functions:\n",
      "\n",
      "substring(str, pos, len)\n",
      "    Substring starts at `pos` and is of length `len` when str is String type or\n",
      "    returns the slice of byte array that starts at `pos` in byte and is of length `len`\n",
      "    when str is Binary type.\n",
      "    \n",
      "    .. versionadded:: 1.5.0\n",
      "    \n",
      "    Notes\n",
      "    -----\n",
      "    The position is not zero based, but 1 based index.\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> df = spark.createDataFrame([('abcd',)], ['s',])\n",
      "    >>> df.select(substring(df.s, 1, 2).alias('s')).collect()\n",
      "    [Row(s='ab')]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import substring\n",
    "\n",
    "help(substring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "a106f2d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|dummy|\n",
      "+-----+\n",
      "|    x|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "l= [ (\"x\",)]\n",
    " \n",
    "df_e = spark.createDataFrame(l,[\"dummy\"])\n",
    "\n",
    "# or \n",
    "\n",
    "df_e = spark.createDataFrame(l,\"dummy string\")\n",
    "\n",
    "df_e.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "f931b4f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------+\n",
      "|substring(hello world, -5, 5)|\n",
      "+-----------------------------+\n",
      "|                        world|\n",
      "+-----------------------------+\n",
      "\n",
      "+-----------------------------+\n",
      "|substring(hello world, -2, 5)|\n",
      "+-----------------------------+\n",
      "|                           ld|\n",
      "+-----------------------------+\n",
      "\n",
      "+----------------------------+\n",
      "|substring(hello world, 7, 4)|\n",
      "+----------------------------+\n",
      "|                        worl|\n",
      "+----------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(substring(lit(\"hello world\"),-5,5)).show()\n",
    "\n",
    "df.select(substring(lit(\"hello world\"),-2,5)).show()\n",
    "\n",
    "# from -2 position, it take 5 chars\n",
    "\n",
    "df.select(substring(lit(\"hello world\"),7,4)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501f18c2",
   "metadata": {},
   "source": [
    "### Tasks - substring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "5f37a13e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Let us perform few tasks to extract information from fixed length strings.\n",
    "\n",
    "        # Create a list for employees with name, ssn and phone_number.\n",
    "    \n",
    "        # SSN Format 3 2 4 - Fixed Length with 11 characters(3 digits,2digits,4 digits)\n",
    "        \n",
    "        # Phone Number Format - Country Code is variable and remaining phone number have 10 digits:\n",
    "                # Country Code - one to 3 digits\n",
    "                # Area Code - 3 digits\n",
    "                # Phone Number Prefix - 3 digits\n",
    "                # Phone Number Remaining - 4 digits\n",
    "                # All the 4 parts are separated by spaces\n",
    "                \n",
    "        # Create a Dataframe with column names name, ssn and phone_number\n",
    "        # Extract last 4 digits from the phone number.\n",
    "        # Extract last 4 digits from SSN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "09e9e914",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+------+--------------+----------------+-----------+\n",
      "|employee_id|first_name|last_name|salary|   nationality|    phone_number|        ssn|\n",
      "+-----------+----------+---------+------+--------------+----------------+-----------+\n",
      "|          1|     Scott|    Tiger|1000.0| united states| +1 123 456 7890|123 45 6789|\n",
      "|          4|     Henry|     Ford|1250.0|         India|+91 234 567 8901|456 78 9123|\n",
      "|          3|      Nick|   Junior| 750.0|united KINGDOM|+44 111 111 1111|222 33 4444|\n",
      "|          4|      Bill|    Gomes|1500.0|     AUSTRALIA|+61 987 654 3210|789 12 6118|\n",
      "+-----------+----------+---------+------+--------------+----------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import substring, col\n",
    "e2_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "ee2a0bfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+----------+---------+------------+\n",
      "|ssn_m|phone_m|first_name|last_name|   full_name|\n",
      "+-----+-------+----------+---------+------------+\n",
      "| 6789|   7890|     Scott|    Tiger|Scott, Tiger|\n",
      "| 9123|   8901|     Henry|     Ford| Henry, Ford|\n",
      "| 4444|   1111|      Nick|   Junior|Nick, Junior|\n",
      "| 6118|   3210|      Bill|    Gomes| Bill, Gomes|\n",
      "+-----+-------+----------+---------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "e2_df.select(\n",
    "    substring(col(\"ssn\"),-4,4).alias(\"ssn_m\").cast(\"int\"),\\\n",
    "             substring(col(\"phone_number\"),-4,4).alias(\"phone_m\"),\\\n",
    "             \"first_name\",\"last_name\").\\\n",
    "withColumn(\"full_name\",concat(col(\"first_name\"),lit(\", \"),col(\"last_name\"))).\\\n",
    "show()\n",
    "\n",
    "\n",
    "# The withColumn creates a new column.It needs columns that are present in select or withput select( as withput select, all coliumns are implied)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ebf642e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad9fbae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c864cd3b",
   "metadata": {},
   "source": [
    "\n",
    "## Extracting Strings using split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "168b2944",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we are processing variable length columns with delimiter then we use split to extract the information.\n",
    "\n",
    "# Here are some of the examples for variable length columns and the use cases for which we typically extract information.\n",
    "\n",
    "        # Address where we store House Number, Street Name, City, State and Zip Code comma separated. We might want to extract City and State for demographics reports.\n",
    "\n",
    "# split takes 2 arguments, column and delimiter.\n",
    "\n",
    "# split convert each string into array and we can access the elements using index.\n",
    "\n",
    "# We can also use explode in conjunction with split to explode the list or array into records in Data Frame. It can be used in cases such as word count, phone count etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "9187ce47",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import split,explode,lit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "id": "ec1077ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function split in module pyspark.sql.functions:\n",
      "\n",
      "split(str, pattern, limit=-1)\n",
      "    Splits str around matches of the given pattern.\n",
      "    \n",
      "    .. versionadded:: 1.5.0\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    str : :class:`~pyspark.sql.Column` or str\n",
      "        a string expression to split\n",
      "    pattern : str\n",
      "        a string representing a regular expression. The regex string should be\n",
      "        a Java regular expression.\n",
      "    limit : int, optional\n",
      "        an integer which controls the number of times `pattern` is applied.\n",
      "    \n",
      "        * ``limit > 0``: The resulting array's length will not be more than `limit`, and the\n",
      "                         resulting array's last entry will contain all input beyond the last\n",
      "                         matched pattern.\n",
      "        * ``limit <= 0``: `pattern` will be applied as many times as possible, and the resulting\n",
      "                          array can be of any size.\n",
      "    \n",
      "        .. versionchanged:: 3.0\n",
      "           `split` now takes an optional `limit` field. If not provided, default limit value is -1.\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> df = spark.createDataFrame([('oneAtwoBthreeC',)], ['s',])\n",
      "    >>> df.select(split(df.s, '[ABC]', 2).alias('s')).collect()\n",
      "    [Row(s=['one', 'twoBthreeC'])]\n",
      "    >>> df.select(split(df.s, '[ABC]', -1).alias('s')).collect()\n",
      "    [Row(s=['one', 'two', 'three', ''])]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "id": "327d9e99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function explode in module pyspark.sql.functions:\n",
      "\n",
      "explode(col)\n",
      "    Returns a new row for each element in the given array or map.\n",
      "    Uses the default column name `col` for elements in the array and\n",
      "    `key` and `value` for elements in the map unless specified otherwise.\n",
      "    \n",
      "    .. versionadded:: 1.4.0\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> from pyspark.sql import Row\n",
      "    >>> eDF = spark.createDataFrame([Row(a=1, intlist=[1,2,3], mapfield={\"a\": \"b\"})])\n",
      "    >>> eDF.select(explode(eDF.intlist).alias(\"anInt\")).collect()\n",
      "    [Row(anInt=1), Row(anInt=2), Row(anInt=3)]\n",
      "    \n",
      "    >>> eDF.select(explode(eDF.mapfield).alias(\"key\", \"value\")).show()\n",
      "    +---+-----+\n",
      "    |key|value|\n",
      "    +---+-----+\n",
      "    |  a|    b|\n",
      "    +---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(explode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "id": "74595729",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------+\n",
      "|split(hello world, how are u,  , 1)|\n",
      "+-----------------------------------+\n",
      "|[hello world, how are u]           |\n",
      "+-----------------------------------+\n",
      "\n",
      "+-----------------------------------+\n",
      "|split(hello world, how are u,  , 3)|\n",
      "+-----------------------------------+\n",
      "|[hello, world,, how are u]         |\n",
      "+-----------------------------------+\n",
      "\n",
      "+------------------------------------+\n",
      "|split(hello world, how are u,  , -1)|\n",
      "+------------------------------------+\n",
      "|[hello, world,, how, are, u]        |\n",
      "+------------------------------------+\n",
      "\n",
      "+---------------------------------------+\n",
      "|split(hello world, how are u,  , -1)[3]|\n",
      "+---------------------------------------+\n",
      "|are                                    |\n",
      "+---------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_e.select(split(lit(\"hello world, how are u\"),\" \",limit=1)).show(truncate=False)\n",
    "\n",
    "df_e.select(split(lit(\"hello world, how are u\"),\" \",limit=3)).show(truncate=False)\n",
    "\n",
    "df_e.select(split(lit(\"hello world, how are u\"),\" \")).show(truncate=False)\n",
    "\n",
    "df_e.select(split(lit(\"hello world, how are u\"),\" \")[3]).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "id": "8d145515",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|col   |\n",
      "+------+\n",
      "|hello |\n",
      "|world,|\n",
      "|how   |\n",
      "|are   |\n",
      "|u     |\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_e.select(explode(split(lit(\"hello world, how are u\"),\" \"))).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "id": "4294f2e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+------+--------------+---------------------------------+-----------+\n",
      "|employee_id|first_name|last_name|salary|nationality   |phone_numbers                    |ssn        |\n",
      "+-----------+----------+---------+------+--------------+---------------------------------+-----------+\n",
      "|1          |Scott     |Tiger    |1000.0|united states |+1 123 456 7890,+1 234 567 8901  |123 45 6789|\n",
      "|2          |Henry     |Ford     |1250.0|India         |+91 234 567 8901                 |456 78 9123|\n",
      "|3          |Nick      |Junior   |750.0 |united KINGDOM|+44 111 111 1111,+44 222 222 2222|222 33 4444|\n",
      "|4          |Bill      |Gomes    |1500.0|AUSTRALIA     |+61 987 654 3210,+61 876 543 2109|789 12 6118|\n",
      "+-----------+----------+---------+------+--------------+---------------------------------+-----------+\n",
      "\n",
      "+-----------+---------------------------------+-----------+------------------------------------+\n",
      "|employee_id|phone_numbers                    |ssn        |phone                               |\n",
      "+-----------+---------------------------------+-----------+------------------------------------+\n",
      "|1          |+1 123 456 7890,+1 234 567 8901  |123 45 6789|[+1 123 456 7890, +1 234 567 8901]  |\n",
      "|2          |+91 234 567 8901                 |456 78 9123|[+91 234 567 8901]                  |\n",
      "|3          |+44 111 111 1111,+44 222 222 2222|222 33 4444|[+44 111 111 1111, +44 222 222 2222]|\n",
      "|4          |+61 987 654 3210,+61 876 543 2109|789 12 6118|[+61 987 654 3210, +61 876 543 2109]|\n",
      "+-----------+---------------------------------+-----------+------------------------------------+\n",
      "\n",
      "+-----------+---------------------------------+-----------+----------------+\n",
      "|employee_id|phone_numbers                    |ssn        |phone           |\n",
      "+-----------+---------------------------------+-----------+----------------+\n",
      "|1          |+1 123 456 7890,+1 234 567 8901  |123 45 6789|+1 123 456 7890 |\n",
      "|1          |+1 123 456 7890,+1 234 567 8901  |123 45 6789|+1 234 567 8901 |\n",
      "|2          |+91 234 567 8901                 |456 78 9123|+91 234 567 8901|\n",
      "|3          |+44 111 111 1111,+44 222 222 2222|222 33 4444|+44 111 111 1111|\n",
      "|3          |+44 111 111 1111,+44 222 222 2222|222 33 4444|+44 222 222 2222|\n",
      "|4          |+61 987 654 3210,+61 876 543 2109|789 12 6118|+61 987 654 3210|\n",
      "|4          |+61 987 654 3210,+61 876 543 2109|789 12 6118|+61 876 543 2109|\n",
      "+-----------+---------------------------------+-----------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employees = [(1, \"Scott\", \"Tiger\", 1000.0, \n",
    "                      \"united states\", \"+1 123 456 7890,+1 234 567 8901\", \"123 45 6789\"\n",
    "                     ),\n",
    "                     (2, \"Henry\", \"Ford\", 1250.0, \n",
    "                      \"India\", \"+91 234 567 8901\", \"456 78 9123\"\n",
    "                     ),\n",
    "                     (3, \"Nick\", \"Junior\", 750.0, \n",
    "                      \"united KINGDOM\", \"+44 111 111 1111,+44 222 222 2222\", \"222 33 4444\"\n",
    "                     ),\n",
    "                     (4, \"Bill\", \"Gomes\", 1500.0, \n",
    "                      \"AUSTRALIA\", \"+61 987 654 3210,+61 876 543 2109\", \"789 12 6118\"\n",
    "                     )\n",
    "                ]\n",
    "\n",
    "\n",
    "\n",
    "employeesDF = spark. \\\n",
    "    createDataFrame(employees,\n",
    "                    schema=\"\"\"employee_id INT, first_name STRING, \n",
    "                    last_name STRING, salary FLOAT, nationality STRING,\n",
    "                    phone_numbers STRING, ssn STRING\"\"\"\n",
    "                   )\n",
    "\n",
    "employeesDF.show(truncate=False)\n",
    "\n",
    "\n",
    "# example for split and explode\n",
    "\n",
    "employeesDF.select(\"employee_id\",\"phone_numbers\",\"ssn\").\\\n",
    "            withColumn(\"phone\",split(\"phone_numbers\",\",\")).show(truncate=False)\n",
    "\n",
    "\n",
    "# explode will be used to get new row for every element in a list\n",
    "\n",
    "employeesDF.select(\"employee_id\",\"phone_numbers\",\"ssn\").\\\n",
    "            withColumn(\"phone\",explode(split(\"phone_numbers\",\",\"))).show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcdcaab0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e650ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26da2de0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "06ddb049",
   "metadata": {},
   "source": [
    "\n",
    "## Padding Characters around Strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "id": "0b6730f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We typically pad characters to build fixed length values or records.\n",
    "\n",
    "# Fixed length values or records are extensively used in Mainframes based systems.\n",
    "\n",
    "# Length of each and every field in fixed length records is predetermined and if the value of the field is less than the predetermined length then we pad with a standard character.\n",
    "\n",
    "# In terms of numeric fields we pad with zero on the leading or left side. For non numeric fields, we pad with some standard character on leading or trailing side.\n",
    "\n",
    "# We use lpad to pad a string with a specific character on leading or left side and rpad to pad on trailing or right side.\n",
    "\n",
    "# Both lpad and rpad, take 3 arguments - column or expression, desired length and the character need to be padded."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afefd50f",
   "metadata": {},
   "source": [
    "### Tasks - Padding Strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "id": "49293bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Let us perform simple tasks to understand the syntax of lpad or rpad.\n",
    "\n",
    "        # Create a Dataframe with single value and single column.\n",
    "        # Apply lpad to pad with - to Hello to make it 10 characters.\n",
    "        \n",
    "\n",
    "# Both lpad and rpad, take 3 arguments - \n",
    "                #  column or expression, desired length and the character \n",
    "                                #  need to be padded.\n",
    "        \n",
    "        \n",
    "# lpad(col, len, pad)\n",
    "#     Left-pad the string column to width `len` with `pad`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "id": "2e65fbf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function lpad in module pyspark.sql.functions:\n",
      "\n",
      "lpad(col, len, pad)\n",
      "    Left-pad the string column to width `len` with `pad`.\n",
      "    \n",
      "    .. versionadded:: 1.5.0\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> df = spark.createDataFrame([('abcd',)], ['s',])\n",
      "    >>> df.select(lpad(df.s, 6, '#').alias('s')).collect()\n",
      "    [Row(s='##abcd')]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(lpad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "id": "b948d7af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lpad, rpad, concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "id": "588c344d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------------------------------+\n",
      "|employee                                                                   |\n",
      "+---------------------------------------------------------------------------+\n",
      "|00001 Scott-----Tiger-----1000 UNITED STATES-- +1 123 456 7890,+123 45 6789|\n",
      "|00002 Henry-----Ford------1250 INDIA---------- +91 234 567 8901-456 78 9123|\n",
      "|00003 Nick------Junior----750 UNITED KINGDOM- +44 111 111 1111,222 33 4444 |\n",
      "|00004 Bill------Gomes-----1500 AUSTRALIA------ +61 987 654 3210,789 12 6118|\n",
      "+---------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "empFixedDF = employeesDF.select(\n",
    "    concat(\n",
    "        lpad(\"employee_id\", 5, \"0\"), lit(\" \"),\n",
    "        rpad(\"first_name\", 10, \"-\"), \n",
    "        rpad(\"last_name\", 10, \"-\"),\n",
    "        lpad(\"salary\", 10, \"0\").cast(\"int\"), lit(\" \"),\n",
    "        rpad(upper(\"nationality\"), 15, \"-\"), lit(\" \"),\n",
    "        rpad(\"phone_numbers\", 17, \"-\"), \n",
    "        \"ssn\"\n",
    "    ).alias(\"employee\")\n",
    ")\n",
    "empFixedDF.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "id": "b9abeb28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|lpad(dummy, 10, 0)|\n",
      "+------------------+\n",
      "|        000000000X|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "l= [(\"X\",)]\n",
    "\n",
    "df = spark.createDataFrame(l,\"dummy string\")\n",
    "\n",
    "# we need a sparkdataframe to use these functions\n",
    "\n",
    "df.select(lpad(col(\"dummy\"),10,\"0\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e582d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01975bc2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59fbf4aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5a34989a",
   "metadata": {},
   "source": [
    "## Trimming Characters from Strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "id": "d758ccc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We typically use trimming to remove unnecessary characters from fixed length records.\n",
    "\n",
    "# Fixed length records are extensively used in Mainframes and we might have to process it using Spark.\n",
    "\n",
    "# As part of processing we might want to remove leading or trailing characters such as 0 in case of numeric types and \n",
    "                        #  space or some standard character in case of alphanumeric types.\n",
    "\n",
    "# As of now Spark trim functions take the column as argument and remove leading or trailing spaces. \n",
    "                #  However, we can use expr or selectExpr to use Spark SQL based trim functions to remove leading or trailing spaces or any other such characters.\n",
    "\n",
    "#         Trim spaces towards left - ltrim\n",
    "#         Trim spaces towards right - rtrim\n",
    "#         Trim spaces on both sides - trim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "id": "b84540e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|               dummy|\n",
      "+--------------------+\n",
      "|  --Hello----....   |\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "l = [ (\"  --Hello----....   \",) ]\n",
    "\n",
    "df = spark.createDataFrame(l).toDF(\"dummy\")\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "id": "1ad897b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function trim in module pyspark.sql.functions:\n",
      "\n",
      "trim(col)\n",
      "    Trim the spaces from both ends for the specified string column.\n",
      "    \n",
      "    .. versionadded:: 1.5\n",
      "\n",
      "Help on function rtrim in module pyspark.sql.functions:\n",
      "\n",
      "rtrim(col)\n",
      "    Trim the spaces from right end for the specified string value.\n",
      "    \n",
      "    .. versionadded:: 1.5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import trim,rtrim,ltrim\n",
    "help(trim)\n",
    "help(rtrim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "id": "05bbe789",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+------------------+---------------+--------------------+\n",
      "|rtrim(dummy)     |ltrim(dummy)      |trim(dummy)    |dummy               |\n",
      "+-----------------+------------------+---------------+--------------------+\n",
      "|  --Hello----....|--Hello----....   |--Hello----....|  --Hello----....   |\n",
      "+-----------------+------------------+---------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(rtrim(\"dummy\"),ltrim(\"dummy\"),trim(\"dummy\"),\"dummy\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "id": "80b435bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import expr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "id": "e2a71f48",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|function_desc                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |\n",
      "+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|Function: trim                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |\n",
      "|Class: org.apache.spark.sql.catalyst.expressions.StringTrim                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |\n",
      "|Usage: \\n    trim(str) - Removes the leading and trailing space characters from `str`.\\n\\n    trim(BOTH FROM str) - Removes the leading and trailing space characters from `str`.\\n\\n    trim(LEADING FROM str) - Removes the leading space characters from `str`.\\n\\n    trim(TRAILING FROM str) - Removes the trailing space characters from `str`.\\n\\n    trim(trimStr FROM str) - Remove the leading and trailing `trimStr` characters from `str`.\\n\\n    trim(BOTH trimStr FROM str) - Remove the leading and trailing `trimStr` characters from `str`.\\n\\n    trim(LEADING trimStr FROM str) - Remove the leading `trimStr` characters from `str`.\\n\\n    trim(TRAILING trimStr FROM str) - Remove the trailing `trimStr` characters from `str`.\\n  |\n",
      "+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# help in spark.sql\n",
    "\n",
    "spark.sql(\"DESCRIBE FUNCTION trim\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "id": "d05c8377",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+-------------+---------------+\n",
      "|dummy               |ltrim             |rtrim        |trim           |\n",
      "+--------------------+------------------+-------------+---------------+\n",
      "|  --Hello----....   |--Hello----....   |  --Hello----|--Hello----....|\n",
      "+--------------------+------------------+-------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# if we do not specify trimStr, it will be defaulted to space\n",
    "df.withColumn(\"ltrim\", expr(\"ltrim(dummy)\")). \\\n",
    "  withColumn(\"rtrim\", expr(\"rtrim('.', rtrim(dummy))\")). \\\n",
    "  withColumn(\"trim\", trim(col(\"dummy\"))). \\\n",
    "  show(truncate=False\n",
    "      )\n",
    "\n",
    "\n",
    "# here thrid line removes the dots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c26fcc17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee324a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a62dee6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c572e1e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a53296a2",
   "metadata": {},
   "source": [
    "\n",
    "## Date and Time Manipulation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "id": "cbf486c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can use current_date to get todays server date.\n",
    "\n",
    "        # Date will be returned using yyyy-MM-dd format.\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "# We can use current_timestamp to get current server time.\n",
    "\n",
    "        # Timestamp will be returned using yyyy-MM-dd HH:mm:ss:SSS format.\n",
    "    \n",
    "        # Hours will be by default in 24 hour format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "id": "d729a7ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|current_date()|\n",
      "+--------------+\n",
      "|    2023-06-30|\n",
      "+--------------+\n",
      "\n",
      "+-------------------------+\n",
      "|current_timestamp()      |\n",
      "+-------------------------+\n",
      "|2023-06-30 17:08:10.74095|\n",
      "+-------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "l = [ (\"m\",)]\n",
    "\n",
    "df = spark.createDataFrame(l).toDF(\"dummy\")\n",
    "\n",
    "from pyspark.sql.functions import current_date,current_timestamp\n",
    "\n",
    "df.select(current_date()).show()\n",
    "\n",
    "df.select(current_timestamp()).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "id": "4ecdc3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can convert a string which contain date (\"20210128\") or timestamp (20210123122500125) in non-standard format to standard date or time \n",
    "#        using to_date or to_timestamp function respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "id": "85284f22",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function to_date in module pyspark.sql.functions:\n",
      "\n",
      "to_date(col, format=None)\n",
      "    Converts a :class:`~pyspark.sql.Column` into :class:`pyspark.sql.types.DateType`\n",
      "    using the optionally specified format. Specify formats according to `datetime pattern`_.\n",
      "    By default, it follows casting rules to :class:`pyspark.sql.types.DateType` if the format\n",
      "    is omitted. Equivalent to ``col.cast(\"date\")``.\n",
      "    \n",
      "    .. _datetime pattern: https://spark.apache.org/docs/latest/sql-ref-datetime-pattern.html\n",
      "    \n",
      "    .. versionadded:: 2.2.0\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> df = spark.createDataFrame([('1997-02-28 10:30:00',)], ['t'])\n",
      "    >>> df.select(to_date(df.t).alias('date')).collect()\n",
      "    [Row(date=datetime.date(1997, 2, 28))]\n",
      "    \n",
      "    >>> df = spark.createDataFrame([('1997-02-28 10:30:00',)], ['t'])\n",
      "    >>> df.select(to_date(df.t, 'yyyy-MM-dd HH:mm:ss').alias('date')).collect()\n",
      "    [Row(date=datetime.date(1997, 2, 28))]\n",
      "\n",
      "Help on function to_timestamp in module pyspark.sql.functions:\n",
      "\n",
      "to_timestamp(col, format=None)\n",
      "    Converts a :class:`~pyspark.sql.Column` into :class:`pyspark.sql.types.TimestampType`\n",
      "    using the optionally specified format. Specify formats according to `datetime pattern`_.\n",
      "    By default, it follows casting rules to :class:`pyspark.sql.types.TimestampType` if the format\n",
      "    is omitted. Equivalent to ``col.cast(\"timestamp\")``.\n",
      "    \n",
      "    .. _datetime pattern: https://spark.apache.org/docs/latest/sql-ref-datetime-pattern.html\n",
      "    \n",
      "    .. versionadded:: 2.2.0\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> df = spark.createDataFrame([('1997-02-28 10:30:00',)], ['t'])\n",
      "    >>> df.select(to_timestamp(df.t).alias('dt')).collect()\n",
      "    [Row(dt=datetime.datetime(1997, 2, 28, 10, 30))]\n",
      "    \n",
      "    >>> df = spark.createDataFrame([('1997-02-28 10:30:00',)], ['t'])\n",
      "    >>> df.select(to_timestamp(df.t, 'yyyy-MM-dd HH:mm:ss').alias('dt')).collect()\n",
      "    [Row(dt=datetime.datetime(1997, 2, 28, 10, 30))]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lit, to_date, to_timestamp\n",
    "\n",
    "\n",
    "help(to_date)\n",
    "\n",
    "help(to_timestamp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "id": "a2715aeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------+\n",
      "|to_date(20210228, yyyyMMdd)|\n",
      "+---------------------------+\n",
      "|                 2021-02-28|\n",
      "+---------------------------+\n",
      "\n",
      "+--------------------------------------------+\n",
      "|to_timestamp(20210123122500, yyyyMMddHHmmss)|\n",
      "+--------------------------------------------+\n",
      "|                         2021-01-23 12:25:00|\n",
      "+--------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(to_date(lit(\"20210228\"),\"yyyyMMdd\")).show()\n",
    "\n",
    "\n",
    "# M - months\n",
    "# m - minutes\n",
    "\n",
    "df.select(to_timestamp(lit(\"20210123122500\"),\"yyyyMMddHHmmss\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "382af565",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8395576",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8fe9aeb2",
   "metadata": {},
   "source": [
    "#  Date and Time Arithmetic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "id": "042ee163",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Date and Time Arithmetic\n",
    "\n",
    "\n",
    "# Adding days to a date or timestamp - date_add\n",
    "\n",
    "# Subtracting days from a date or timestamp - date_sub\n",
    "\n",
    "# Getting difference between 2 dates or timestamps - datediff\n",
    "\n",
    "# Getting the number of months between 2 dates or timestamps - months_between\n",
    "\n",
    "# Adding months to a date or timestamp - add_months\n",
    "\n",
    "# Getting next day from a given date - next_day\n",
    "\n",
    "# All the functions are self explanatory. We can apply these on standard date or timestamp. \n",
    "#                           All the functions return date even when applied on timestamp field.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "id": "42f044fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function date_add in module pyspark.sql.functions:\n",
      "\n",
      "date_add(start, days)\n",
      "    Returns the date that is `days` days after `start`\n",
      "    \n",
      "    .. versionadded:: 1.5.0\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> df = spark.createDataFrame([('2015-04-08',)], ['dt'])\n",
      "    >>> df.select(date_add(df.dt, 1).alias('next_date')).collect()\n",
      "    [Row(next_date=datetime.date(2015, 4, 9))]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import date_add,date_sub\n",
    "\n",
    "help(date_add)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "id": "39a211be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------------+\n",
      "|date      |Time                   |\n",
      "+----------+-----------------------+\n",
      "|2014-02-28|2014-02-28 10:00:00.123|\n",
      "|2016-02-29|2016-02-29 08:08:08.999|\n",
      "|2017-10-31|2017-12-31 11:59:59.123|\n",
      "|2019-11-30|2019-08-31 00:00:00.000|\n",
      "+----------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "datetimes = [(\"2014-02-28\", \"2014-02-28 10:00:00.123\"),\n",
    "                     (\"2016-02-29\", \"2016-02-29 08:08:08.999\"),\n",
    "                     (\"2017-10-31\", \"2017-12-31 11:59:59.123\"),\n",
    "                     (\"2019-11-30\", \"2019-08-31 00:00:00.000\")\n",
    "                ]\n",
    "\n",
    "from pyspark.sql.functions import date_add,date_sub,next_day\n",
    "\n",
    "d = spark.createDataFrame(datetimes,\"date string,Time string\")\n",
    "\n",
    "d.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "id": "7fb7bbc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------------+-------------+-------------+-------------+-------------+----------+\n",
      "|date      |Time                   |date_add_date|date_subtract|date_add_time|date_sub_time|next_day  |\n",
      "+----------+-----------------------+-------------+-------------+-------------+-------------+----------+\n",
      "|2014-02-28|2014-02-28 10:00:00.123|2014-03-10   |2014-02-18   |2014-03-10   |2014-02-18   |2014-03-02|\n",
      "|2016-02-29|2016-02-29 08:08:08.999|2016-03-10   |2016-02-19   |2016-03-10   |2016-02-19   |2016-03-06|\n",
      "|2017-10-31|2017-12-31 11:59:59.123|2017-11-10   |2017-10-21   |2018-01-10   |2017-12-21   |2017-11-05|\n",
      "|2019-11-30|2019-08-31 00:00:00.000|2019-12-10   |2019-11-20   |2019-09-10   |2019-08-21   |2019-12-01|\n",
      "+----------+-----------------------+-------------+-------------+-------------+-------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "d.\\\n",
    "    withColumn(\"date_add_date\",date_add(\"date\",10)).\\\n",
    "    withColumn(\"date_subtract\",date_sub(\"date\",10)).\\\n",
    "\\\n",
    "    withColumn(\"date_add_time\",date_add(\"Time\",10)).\\\n",
    "    withColumn(\"date_sub_time\",date_sub(\"Time\",10)).\\\n",
    "    withColumn(\"next_day\",next_day(\"date\",\"Sun\")).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "id": "cdb15587",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+-------------+-------------+\n",
      "|      date|                Time|datediff_date|datediff_time|\n",
      "+----------+--------------------+-------------+-------------+\n",
      "|2014-02-28|2014-02-28 10:00:...|         3409|         3409|\n",
      "|2016-02-29|2016-02-29 08:08:...|         2678|         2678|\n",
      "|2017-10-31|2017-12-31 11:59:...|         2068|         2007|\n",
      "|2019-11-30|2019-08-31 00:00:...|         1308|         1399|\n",
      "+----------+--------------------+-------------+-------------+\n",
      "\n",
      "+----------+-----------------------+-------------------+-------------------+---------------+---------------+\n",
      "|date      |Time                   |months_between_date|months_between_time|add_months_date|add_months_time|\n",
      "+----------+-----------------------+-------------------+-------------------+---------------+---------------+\n",
      "|2014-02-28|2014-02-28 10:00:00.123|112.0              |112.0              |2014-05-28     |2014-05-28     |\n",
      "|2016-02-29|2016-02-29 08:08:08.999|88.0               |88.0               |2016-05-29     |2016-05-29     |\n",
      "|2017-10-31|2017-12-31 11:59:59.123|68.0               |66.0               |2018-01-31     |2018-03-31     |\n",
      "|2019-11-30|2019-08-31 00:00:00.000|43.0               |46.0               |2020-02-29     |2019-11-30     |\n",
      "+----------+-----------------------+-------------------+-------------------+---------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Get the difference between current_date and date values as well as current_timestamp and time values.\n",
    "\n",
    "# datediff(end, start) :-   Returns the number of days from `start` to `end`.\n",
    "\n",
    "from pyspark.sql.functions import current_date, current_timestamp, datediff\n",
    "\n",
    "d. \\\n",
    "    withColumn(\"datediff_date\", datediff(current_date(), \"date\")). \\\n",
    "    withColumn(\"datediff_time\", datediff(current_timestamp(), \"time\")). \\\n",
    "    show()\n",
    "\n",
    "\n",
    "# Get the number of months between current_date and date values as well as current_timestamp and time values.\n",
    "\n",
    "# Add 3 months to both date values as well as time values.\n",
    "\n",
    "from pyspark.sql.functions import months_between, add_months, round\n",
    "\n",
    "d. \\\n",
    "    withColumn(\"months_between_date\", round(months_between(current_date(), \"date\"), 2)). \\\n",
    "    withColumn(\"months_between_time\", round(months_between(current_timestamp(), \"time\"), 2)). \\\n",
    "    withColumn(\"add_months_date\", add_months(\"date\", 3)). \\\n",
    "    withColumn(\"add_months_time\", add_months(\"time\", 3)). \\\n",
    "    show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7990b2c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "907ee56d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c13c6c06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "03199519",
   "metadata": {},
   "source": [
    "## Using Date and Time Trunc Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "id": "bbe96635",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In Data Warehousing we quite often run to date reports such as week to date, month to date, year to date etc. Let us understand how we can take care of such requirements using appropriate functions over Spark Data Frames.\n",
    "\n",
    "# We can use trunc or date_trunc for the same to get the beginning date of the week, month, current year etc by passing date or timestamp to it.\n",
    "\n",
    "# We can use trunc to get beginning date of the month or year by passing date or timestamp to it - for example trunc(current_date(), \"MM\") will give the first of the current month.\n",
    "\n",
    "# We can use date_trunc to get beginning date of the month or year as well as beginning time of the day or hour by passing timestamp to it.\n",
    "\n",
    "                # Get beginning date based on month - date_trunc(\"MM\", current_timestamp())\n",
    "\n",
    "                # Get beginning time based on day - date_trunc(\"DAY\", current_timestamp())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "id": "c5a7ebd9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function trunc in module pyspark.sql.functions:\n",
      "\n",
      "trunc(date, format)\n",
      "    Returns date truncated to the unit specified by the format.\n",
      "    \n",
      "    .. versionadded:: 1.5.0\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    date : :class:`~pyspark.sql.Column` or str\n",
      "    format : str\n",
      "        'year', 'yyyy', 'yy' to truncate by year,\n",
      "        or 'month', 'mon', 'mm' to truncate by month\n",
      "        Other options are: 'week', 'quarter'\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> df = spark.createDataFrame([('1997-02-28',)], ['d'])\n",
      "    >>> df.select(trunc(df.d, 'year').alias('year')).collect()\n",
      "    [Row(year=datetime.date(1997, 1, 1))]\n",
      "    >>> df.select(trunc(df.d, 'mon').alias('month')).collect()\n",
      "    [Row(month=datetime.date(1997, 2, 1))]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(trunc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "id": "aaa003ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import trunc, date_trunc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "id": "4d6ca9b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------------+\n",
      "|date      |time                   |\n",
      "+----------+-----------------------+\n",
      "|2014-02-28|2014-02-28 10:00:00.123|\n",
      "|2016-02-29|2016-02-29 08:08:08.999|\n",
      "|2017-10-31|2017-12-31 11:59:59.123|\n",
      "|2019-11-30|2019-08-31 00:00:00.000|\n",
      "+----------+-----------------------+\n",
      "\n",
      "+----------+-----------------------+----------+---------------+----------+---------------+\n",
      "|date      |time                   |date_trunc|date_week_trunc|time_trunc|time_week_trunc|\n",
      "+----------+-----------------------+----------+---------------+----------+---------------+\n",
      "|2014-02-28|2014-02-28 10:00:00.123|2014-02-01|2014-02-24     |2014-01-01|2014-02-24     |\n",
      "|2016-02-29|2016-02-29 08:08:08.999|2016-02-01|2016-02-29     |2016-01-01|2016-02-29     |\n",
      "|2017-10-31|2017-12-31 11:59:59.123|2017-10-01|2017-10-30     |2017-01-01|2017-12-25     |\n",
      "|2019-11-30|2019-08-31 00:00:00.000|2019-11-01|2019-11-25     |2019-01-01|2019-08-26     |\n",
      "+----------+-----------------------+----------+---------------+----------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "datetimes = [(\"2014-02-28\", \"2014-02-28 10:00:00.123\"),\n",
    "                     (\"2016-02-29\", \"2016-02-29 08:08:08.999\"),\n",
    "                     (\"2017-10-31\", \"2017-12-31 11:59:59.123\"),\n",
    "                     (\"2019-11-30\", \"2019-08-31 00:00:00.000\")\n",
    "                ]\n",
    "\n",
    "datetimesDF = spark.createDataFrame(datetimes, schema=\"date STRING, time STRING\")\n",
    "\n",
    "datetimesDF.show(truncate=False)\n",
    "\n",
    "# ---- trunc with date column - give first day of that specified (month,year,week...)\n",
    "# ---- trunc with time column - give first day of that specified (month,year,week...)\n",
    "\n",
    "# Trunc with both time and date gives first day of that specified.\n",
    "\n",
    "# trunc with date on week - gives first day of that week\n",
    "\n",
    "datetimesDF. \\\n",
    "    withColumn(\"date_trunc\", trunc(\"date\", \"MM\")). \\\n",
    "    withColumn(\"date_week_trunc\", trunc(\"date\", \"week\")). \\\n",
    "    withColumn(\"time_trunc\", trunc(\"time\", \"yy\")). \\\n",
    "    withColumn(\"time_week_trunc\", trunc(\"time\", \"week\")). \\\n",
    "    show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "id": "a668311e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------------+-------------------+-------------------+-------------------+\n",
      "|date      |time                   |date_dt            |time_dt            |time_dt1           |\n",
      "+----------+-----------------------+-------------------+-------------------+-------------------+\n",
      "|2014-02-28|2014-02-28 10:00:00.123|2014-02-28 00:00:00|2014-02-28 10:00:00|2014-02-28 00:00:00|\n",
      "|2016-02-29|2016-02-29 08:08:08.999|2016-02-29 00:00:00|2016-02-29 08:00:00|2016-02-29 00:00:00|\n",
      "|2017-10-31|2017-12-31 11:59:59.123|2017-10-31 00:00:00|2017-12-31 11:00:00|2017-12-31 00:00:00|\n",
      "|2019-11-30|2019-08-31 00:00:00.000|2019-11-30 00:00:00|2019-08-31 00:00:00|2019-08-31 00:00:00|\n",
      "+----------+-----------------------+-------------------+-------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## date_trunc\n",
    "\n",
    "# the syntax is specified type and column\n",
    "\n",
    "\n",
    "datetimesDF. \\\n",
    "    withColumn(\"date_dt\", date_trunc(\"HOUR\", \"date\")). \\\n",
    "    withColumn(\"time_dt\", date_trunc(\"HOUR\", \"time\")). \\\n",
    "    withColumn(\"time_dt1\", date_trunc(\"dd\", \"time\")). \\\n",
    "    show(truncate=False)\n",
    "\n",
    "\n",
    "# date_trunc with hour on date - give first hour of that date\n",
    "# date_trunc with hour on time - give first hour of that time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a45f9379",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee7e2e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9bd9d65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e5a71b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bd29cd34",
   "metadata": {},
   "source": [
    "## Date and Time Extract Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "id": "ba7c4b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here are the extract functions that are useful which are self explanatory.\n",
    "\n",
    "        # year\n",
    "        # month\n",
    "        # weekofyear\n",
    "        # dayofyear\n",
    "        # dayofmonth\n",
    "        # dayofweek\n",
    "        # hour\n",
    "        # minute\n",
    "        # second\n",
    "\n",
    "        \n",
    "l = [ (\"m\",)]\n",
    "\n",
    "df = spark.createDataFrame(l).toDF(\"dummy\")\n",
    "\n",
    "\n",
    "from pyspark.sql.functions import year,weekofyear,dayofmonth,dayofyear,dayofweek,current_date,month,hour,minute,second\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "id": "d9befa8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function current_date in module pyspark.sql.functions:\n",
      "\n",
      "current_date()\n",
      "    Returns the current date at the start of query evaluation as a :class:`DateType` column.\n",
      "    All calls of current_date within the same query return the same value.\n",
      "    \n",
      "    .. versionadded:: 1.5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(current_date) # - returns a column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "id": "efb5c24e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function year in module pyspark.sql.functions:\n",
      "\n",
      "year(col)\n",
      "    Extract the year of a given date as integer.\n",
      "    \n",
      "    .. versionadded:: 1.5.0\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> df = spark.createDataFrame([('2015-04-08',)], ['dt'])\n",
      "    >>> df.select(year('dt').alias('year')).collect()\n",
      "    [Row(year=2015)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "id": "0a00c247",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------+----+-----+----------+----+------+------+---------+---------+\n",
      "|current_timestamp         |year|month|dayofmonth|hour|minute|second|dayofweek|dayofyear|\n",
      "+--------------------------+----+-----+----------+----+------+------+---------+---------+\n",
      "|2023-06-30 18:21:39.886247|2023|6    |30        |18  |21    |39    |6        |181      |\n",
      "+--------------------------+----+-----+----------+----+------+------+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\n",
    "    current_timestamp().alias('current_timestamp'), \n",
    "    year(current_timestamp()).alias('year'),\n",
    "    month(current_timestamp()).alias('month'),\n",
    "    dayofmonth(current_timestamp()).alias('dayofmonth'),\n",
    "    hour(current_timestamp()).alias('hour'),\n",
    "    minute(current_timestamp()).alias('minute'),\n",
    "    second(current_timestamp()).alias('second'),\n",
    "    dayofweek(current_timestamp()).alias('dayofweek'),\n",
    "    dayofyear(current_timestamp()).alias('dayofyear'),\n",
    ").show(truncate=False) #yyyy-MM-dd HH:mm:ss.SSS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18dbcc05",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b474aef9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f07262c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "97946782",
   "metadata": {},
   "source": [
    "## Using to_date and to_timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "id": "0cd503ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# how to convert non standard dates and timestamps to standard dates and timestamps.\n",
    "\n",
    "# yyyy-MM-dd is the standard date format\n",
    "\n",
    "# yyyy-MM-dd HH:mm:ss.SSS is the standard timestamp format\n",
    "\n",
    "# Most of the date manipulation functions expect date and time using standard format.\n",
    "#           However, we might not have data in the expected standard format.\n",
    "\n",
    "\n",
    "# In those scenarios we can use \n",
    "#                to_date and to_timestamp to convert non standard dates and timestamps to standard ones \n",
    "# respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "id": "b32ecb0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function to_date in module pyspark.sql.functions:\n",
      "\n",
      "to_date(col, format=None)\n",
      "    Converts a :class:`~pyspark.sql.Column` into :class:`pyspark.sql.types.DateType`\n",
      "    using the optionally specified format. Specify formats according to `datetime pattern`_.\n",
      "    By default, it follows casting rules to :class:`pyspark.sql.types.DateType` if the format\n",
      "    is omitted. Equivalent to ``col.cast(\"date\")``.\n",
      "    \n",
      "    .. _datetime pattern: https://spark.apache.org/docs/latest/sql-ref-datetime-pattern.html\n",
      "    \n",
      "    .. versionadded:: 2.2.0\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> df = spark.createDataFrame([('1997-02-28 10:30:00',)], ['t'])\n",
      "    >>> df.select(to_date(df.t).alias('date')).collect()\n",
      "    [Row(date=datetime.date(1997, 2, 28))]\n",
      "    \n",
      "    >>> df = spark.createDataFrame([('1997-02-28 10:30:00',)], ['t'])\n",
      "    >>> df.select(to_date(df.t, 'yyyy-MM-dd HH:mm:ss').alias('date')).collect()\n",
      "    [Row(date=datetime.date(1997, 2, 28))]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(to_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "id": "34028aee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------------------+\n",
      "|date    |time                    |\n",
      "+--------+------------------------+\n",
      "|20140228|28-Feb-2014 10:00:00.123|\n",
      "|20160229|20-Feb-2016 08:08:08.999|\n",
      "|20171031|31-Dec-2017 11:59:59.123|\n",
      "|20191130|31-Aug-2019 00:00:00.000|\n",
      "+--------+------------------------+\n",
      "\n",
      "+--------+------------------------+-------+------------+\n",
      "|date    |time                    |to_date|to_timestamp|\n",
      "+--------+------------------------+-------+------------+\n",
      "|20140228|28-Feb-2014 10:00:00.123|null   |null        |\n",
      "|20160229|20-Feb-2016 08:08:08.999|null   |null        |\n",
      "|20171031|31-Dec-2017 11:59:59.123|null   |null        |\n",
      "|20191130|31-Aug-2019 00:00:00.000|null   |null        |\n",
      "+--------+------------------------+-------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "datetimes = [(20140228, \"28-Feb-2014 10:00:00.123\"),\n",
    "                     (20160229, \"20-Feb-2016 08:08:08.999\"),\n",
    "                     (20171031, \"31-Dec-2017 11:59:59.123\"),\n",
    "                     (20191130, \"31-Aug-2019 00:00:00.000\")\n",
    "                ]\n",
    "\n",
    "datetimesDF = spark.createDataFrame(datetimes, schema=\"date BIGINT, time STRING\")\n",
    "\n",
    "datetimesDF.show(truncate=False)\n",
    "\n",
    "from pyspark.sql.functions import lit, to_date, to_timestamp\n",
    "\n",
    "# to_date and to_timestamp -  takes only date,string,timestamp not long,int,float\n",
    "\n",
    "datetimesDF.\\\n",
    "    withColumn(\"to_date\", to_date(col('date').cast('string'), 'yyyyMMMdd')).\\\n",
    "    withColumn(\"to_timestamp\", to_timestamp(col(\"time\"),\"yyyy-MM-dd HH:mm:ss.SSS\")).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "id": "2801a3ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------------------+----------+-----------------------+\n",
      "|date    |time                    |to_date   |to_timestamp           |\n",
      "+--------+------------------------+----------+-----------------------+\n",
      "|20140228|28-Feb-2014 10:00:00.123|2014-02-28|2014-02-28 10:00:00.123|\n",
      "|20160229|20-Feb-2016 08:08:08.999|2016-02-29|2016-02-20 08:08:08.999|\n",
      "|20171031|31-Dec-2017 11:59:59.123|2017-10-31|2017-12-31 11:59:59.123|\n",
      "|20191130|31-Aug-2019 00:00:00.000|2019-11-30|2019-08-31 00:00:00    |\n",
      "+--------+------------------------+----------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "datetimesDF. \\\n",
    "    withColumn('to_date', to_date(col('date').cast('string'), 'yyyyMMdd')). \\\n",
    "    withColumn('to_timestamp', to_timestamp(col('time'), 'dd-MMM-yyyy HH:mm:ss.SSS')). \\\n",
    "    show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db4507de",
   "metadata": {},
   "source": [
    "## Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "id": "5ad76d3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|   to_date|\n",
      "+----------+\n",
      "|2021-03-02|\n",
      "|2021-03-02|\n",
      "|2021-03-02|\n",
      "|2021-03-02|\n",
      "+----------+\n",
      "\n",
      "+-------+\n",
      "|to_date|\n",
      "+-------+\n",
      "|   null|\n",
      "|   null|\n",
      "|   null|\n",
      "|   null|\n",
      "+-------+\n",
      "\n",
      "+----------+\n",
      "|   to_date|\n",
      "+----------+\n",
      "|2021-01-06|\n",
      "|2021-01-06|\n",
      "|2021-01-06|\n",
      "|2021-01-06|\n",
      "+----------+\n",
      "\n",
      "+----------+\n",
      "|   to_date|\n",
      "+----------+\n",
      "|2021-03-02|\n",
      "|2021-03-02|\n",
      "|2021-03-02|\n",
      "|2021-03-02|\n",
      "+----------+\n",
      "\n",
      "+----------+\n",
      "|   to_date|\n",
      "+----------+\n",
      "|2021-03-02|\n",
      "+----------+\n",
      "\n",
      "+----------+\n",
      "|   to_date|\n",
      "+----------+\n",
      "|2021-03-02|\n",
      "+----------+\n",
      "\n",
      "+-------------------+\n",
      "|            to_date|\n",
      "+-------------------+\n",
      "|2021-03-02 17:30:15|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# convert non standard dates and timestamps to standard dates and timestamps.\n",
    "\n",
    "\n",
    "datetimesDF.\\\n",
    "    select(to_date(lit('02-03-2021'), 'dd-MM-yyyy').alias('to_date')).show()\n",
    "\n",
    "datetimesDF.\\\n",
    "    select(to_date(lit('02-Mar-2021'), 'dd/MMM/yyyy').alias('to_date')).show()\n",
    "\n",
    "\n",
    "datetimesDF.select(to_date(lit('20210601'), 'yyyyddMM').alias('to_date')).show()\n",
    "\n",
    "# 2021 is year and 061 is day of the year in julian calendar\n",
    "\n",
    "datetimesDF.select(to_date(lit('2021061'), 'yyyyDDD').alias('to_date')).show()\n",
    "\n",
    "\n",
    "# if u have 3 OR Less char month - use MMM or MM, M\n",
    "# if u have full chars of the month - use MMMM ( 4 Ms)\n",
    "\n",
    "df.select(to_date(lit('02-Mar-2021'), 'dd-MMM-yyyy').alias('to_date')).show()\n",
    "\n",
    "df.select(to_date(lit('02-March-2021'), 'dd-MMMM-yyyy').alias('to_date')).show()\n",
    "\n",
    "# timestamp\n",
    "\n",
    "df.select(to_timestamp(lit('02-Mar-2021 17:30:15'), 'dd-MMM-yyyy HH:mm:ss').alias('to_date')).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31978267",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d236a1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa4ae03b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c139ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "61e67022",
   "metadata": {},
   "source": [
    "\n",
    "## Using date_format Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 551,
   "id": "163867cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import date_format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "id": "4d41e3ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# how to extract information from dates or times using date_format function.\n",
    "\n",
    "# We can use date_format to extract the required information in a desired format from standard date or timestamp. \n",
    "#                    Earlier we have explored to_date and to_timestamp to convert non standard date or timestamp to standard ones respectively.\n",
    "\n",
    "\n",
    "# There are also specific functions to extract year, month, day with in a week, a day with in a month, day with in a year etc. \n",
    "#                     These are covered as part of earlier topics in this section or module.\n",
    "\n",
    "\n",
    "# yyyy\n",
    "# MM\n",
    "# dd\n",
    "# DD\n",
    "# HH\n",
    "# hh\n",
    "# mm\n",
    "# ss\n",
    "# SSS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 556,
   "id": "88e99c5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------------+-------+-------+\n",
      "|date      |time                   |date_ym|time_ym|\n",
      "+----------+-----------------------+-------+-------+\n",
      "|2014-02-28|2014-02-28 10:00:00.123|201402 |201402 |\n",
      "|2016-02-29|2016-02-29 08:08:08.999|201602 |201602 |\n",
      "|2017-10-31|2017-12-31 11:59:59.123|201710 |201712 |\n",
      "|2019-11-30|2019-08-31 00:00:00.000|201911 |201908 |\n",
      "+----------+-----------------------+-------+-------+\n",
      "\n",
      "******************************************************************************************\n",
      "root\n",
      " |-- date: string (nullable = true)\n",
      " |-- time: string (nullable = true)\n",
      " |-- date_ym: string (nullable = true)\n",
      " |-- time_ym: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- date: string (nullable = true)\n",
      " |-- time: string (nullable = true)\n",
      " |-- date_ym: integer (nullable = true)\n",
      " |-- time_ym: integer (nullable = true)\n",
      "\n",
      "******************************************************************************************\n",
      "+----------+-----------------------+--------------+--------------+\n",
      "|date      |time                   |date_dt       |date_ts       |\n",
      "+----------+-----------------------+--------------+--------------+\n",
      "|2014-02-28|2014-02-28 10:00:00.123|20140228000000|20140228100000|\n",
      "|2016-02-29|2016-02-29 08:08:08.999|20160229000000|20160229080808|\n",
      "|2017-10-31|2017-12-31 11:59:59.123|20171031000000|20171231115959|\n",
      "|2019-11-30|2019-08-31 00:00:00.000|20191130000000|20190831000000|\n",
      "+----------+-----------------------+--------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "datetimes = [(\"2014-02-28\", \"2014-02-28 10:00:00.123\"),\n",
    "                     (\"2016-02-29\", \"2016-02-29 08:08:08.999\"),\n",
    "                     (\"2017-10-31\", \"2017-12-31 11:59:59.123\"),\n",
    "                     (\"2019-11-30\", \"2019-08-31 00:00:00.000\")\n",
    "                ]\n",
    "\n",
    "datetimesDF = spark.createDataFrame(datetimes, schema=\"date STRING, time STRING\")\n",
    "\n",
    "\n",
    "datetimesDF. \\\n",
    "    withColumn(\"date_ym\", date_format(\"date\", \"yyyyMM\")). \\\n",
    "    withColumn(\"time_ym\", date_format(\"time\", \"yyyyMM\")). \\\n",
    "    show(truncate=False)\n",
    "\n",
    "print(\"***\"*30)\n",
    "\n",
    "datetimesDF. \\\n",
    "    withColumn(\"date_ym\", date_format(\"date\", \"yyyyMM\")). \\\n",
    "    withColumn(\"time_ym\", date_format(\"time\", \"yyyyMM\")). \\\n",
    "    printSchema()\n",
    "\n",
    "datetimesDF. \\\n",
    "    withColumn(\"date_ym\", date_format(\"date\", \"yyyyMM\").cast('int')). \\\n",
    "    withColumn(\"time_ym\", date_format(\"time\", \"yyyyMM\").cast('int')). \\\n",
    "    printSchema()\n",
    "\n",
    "print(\"***\"*30)\n",
    "\n",
    "datetimesDF. \\\n",
    "    withColumn(\"date_dt\", date_format(\"date\", \"yyyyMMddHHmmss\")). \\\n",
    "    withColumn(\"date_ts\", date_format(\"time\", \"yyyyMMddHHmmss\")). \\\n",
    "    show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 562,
   "id": "50be61ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------------+-------+-------+\n",
      "|date      |time                   |date_yd|time_yd|\n",
      "+----------+-----------------------+-------+-------+\n",
      "|2014-02-28|2014-02-28 10:00:00.123|2014059|2014059|\n",
      "|2016-02-29|2016-02-29 08:08:08.999|2016060|2016060|\n",
      "|2017-10-31|2017-12-31 11:59:59.123|2017304|2017365|\n",
      "|2019-11-30|2019-08-31 00:00:00.000|2019334|2019243|\n",
      "+----------+-----------------------+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get year and day of year using yyyyDDD format.\n",
    "\n",
    "# yyyyDDD - Get year and day of year in 3 digits\n",
    "# yyyyDD - Get year and day of year in 2 digits\n",
    "\n",
    "datetimesDF. \\\n",
    "    withColumn(\"date_yd\", date_format(\"date\", \"yyyyDDD\").cast('int')). \\\n",
    "    withColumn(\"time_yd\", date_format(\"time\", \"yyyyDDD\").cast('int')). \\\n",
    "    show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 558,
   "id": "fc658633",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------------+-----------------+\n",
      "|date      |time                   |date_desc        |\n",
      "+----------+-----------------------+-----------------+\n",
      "|2014-02-28|2014-02-28 10:00:00.123|February 28, 2014|\n",
      "|2016-02-29|2016-02-29 08:08:08.999|February 29, 2016|\n",
      "|2017-10-31|2017-12-31 11:59:59.123|October 31, 2017 |\n",
      "|2019-11-30|2019-08-31 00:00:00.000|November 30, 2019|\n",
      "+----------+-----------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get complete description of the date.\n",
    "datetimesDF. \\\n",
    "    withColumn(\"date_desc\", date_format(\"date\", \"MMMM d, yyyy\")). \\\n",
    "    show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 563,
   "id": "d783726d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------------+-------------+\n",
      "|date      |time                   |day_name_full|\n",
      "+----------+-----------------------+-------------+\n",
      "|2014-02-28|2014-02-28 10:00:00.123|Friday       |\n",
      "|2016-02-29|2016-02-29 08:08:08.999|Monday       |\n",
      "|2017-10-31|2017-12-31 11:59:59.123|Tuesday      |\n",
      "|2019-11-30|2019-08-31 00:00:00.000|Saturday     |\n",
      "+----------+-----------------------+-------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/07/01 00:06:55 WARN HeartbeatReceiver: Removing executor driver with no recent heartbeats: 935129 ms exceeds timeout 120000 ms\n",
      "23/07/01 00:06:55 WARN SparkContext: Killing executors is not supported by current scheduler.\n"
     ]
    }
   ],
   "source": [
    "datetimesDF. \\\n",
    "    withColumn(\"day_name_full\", date_format(\"date\", \"EEEE\")). \\\n",
    "    show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ad553b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e805b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b753e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "591f8c7f",
   "metadata": {},
   "source": [
    "## Dealing with Unix Timestamp\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 568,
   "id": "7c59f416",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us understand how to deal with Unix Timestamp in Spark.\n",
    "\n",
    "\n",
    "\n",
    "# It is an integer and started from January 1st 1970 Midnight UTC.\n",
    "# Beginning time is also known as epoch and is incremented by 1 every second.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# We can convert Unix Timestamp to regular date or timestamp and vice versa.\n",
    "\n",
    "# We can use unix_timestamp to convert regular date or timestamp to a unix timestamp value. \n",
    "#                For example unix_timestamp(lit(\"2019-11-19 00:00:00\"))\n",
    "\n",
    "# We can use from_unixtime to convert unix timestamp to regular date or timestamp.\n",
    "#                For example from_unixtime(lit(1574101800))\n",
    "\n",
    "# We can also pass format to both the functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 565,
   "id": "70f4bf0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+-------------------+------------+----------+----------+\n",
      "|  dateid|      date|               time|unix_date_id| unix_date| unix_time|\n",
      "+--------+----------+-------------------+------------+----------+----------+\n",
      "|20140228|2014-02-28|2014-02-28 10:00:00|  1393525800|1393525800|1393561800|\n",
      "|20160229|2016-02-29|2016-02-29 08:08:08|  1456684200|1456684200|1456713488|\n",
      "|20171031|2017-10-31|2017-12-31 11:59:59|  1509388200|1509388200|1514701799|\n",
      "|20191130|2019-11-30|2019-08-31 00:00:00|  1575052200|1575052200|1567189800|\n",
      "+--------+----------+-------------------+------------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "datetimes = [(20140228, \"2014-02-28\", \"2014-02-28 10:00:00\"),\n",
    "                     (20160229, \"2016-02-29\", \"2016-02-29 08:08:08\"),\n",
    "                     (20171031, \"2017-10-31\", \"2017-12-31 11:59:59\"),\n",
    "                     (20191130, \"2019-11-30\", \"2019-08-31 00:00:00\")\n",
    "                ]\n",
    "datetimesDF = spark.createDataFrame(datetimes).toDF(\"dateid\", \"date\", \"time\")\n",
    "\n",
    "\n",
    "from pyspark.sql.functions import unix_timestamp, col\n",
    "datetimesDF. \\\n",
    "    withColumn(\"unix_date_id\", unix_timestamp(col(\"dateid\").cast(\"string\"), \"yyyyMMdd\")). \\\n",
    "    withColumn(\"unix_date\", unix_timestamp(\"date\", \"yyyy-MM-dd\")). \\\n",
    "    withColumn(\"unix_time\", unix_timestamp(\"time\")). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 567,
   "id": "4f767376",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|  unixtime|\n",
      "+----------+\n",
      "|1393561800|\n",
      "|1456713488|\n",
      "|1514701799|\n",
      "|1567189800|\n",
      "+----------+\n",
      "\n",
      "root\n",
      " |-- unixtime: long (nullable = true)\n",
      "\n",
      "+----------+--------+-------------------+\n",
      "|  unixtime|    date|               time|\n",
      "+----------+--------+-------------------+\n",
      "|1393561800|20140228|2014-02-28 10:00:00|\n",
      "|1456713488|20160229|2016-02-29 08:08:08|\n",
      "|1514701799|20171231|2017-12-31 11:59:59|\n",
      "|1567189800|20190831|2019-08-31 00:00:00|\n",
      "+----------+--------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Create a Dataframe by name unixtimesDF with one column unixtime using 4 values.\n",
    "#            You can use the unix timestamp generated for time column in previous task.\n",
    "\n",
    "\n",
    "\n",
    "unixtimes = [(1393561800, ),\n",
    "             (1456713488, ),\n",
    "             (1514701799, ),\n",
    "             (1567189800, )\n",
    "            ]\n",
    "unixtimesDF = spark.createDataFrame(unixtimes).toDF(\"unixtime\")\n",
    "unixtimesDF.show()\n",
    "\n",
    "\n",
    "unixtimesDF.printSchema()\n",
    "\n",
    "\n",
    "# Get date in yyyyMMdd format and also complete timestamp.\n",
    "\n",
    "\n",
    "from pyspark.sql.functions import from_unixtime\n",
    "\n",
    "\n",
    "unixtimesDF. \\\n",
    "    withColumn(\"date\", from_unixtime(\"unixtime\", \"yyyyMMdd\")). \\\n",
    "    withColumn(\"time\", from_unixtime(\"unixtime\")). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 569,
   "id": "5be4293d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function from_unixtime in module pyspark.sql.functions:\n",
      "\n",
      "from_unixtime(timestamp, format='yyyy-MM-dd HH:mm:ss')\n",
      "    Converts the number of seconds from unix epoch (1970-01-01 00:00:00 UTC) to a string\n",
      "    representing the timestamp of that moment in the current system time zone in the given\n",
      "    format.\n",
      "    \n",
      "    .. versionadded:: 1.5.0\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> spark.conf.set(\"spark.sql.session.timeZone\", \"America/Los_Angeles\")\n",
      "    >>> time_df = spark.createDataFrame([(1428476400,)], ['unix_time'])\n",
      "    >>> time_df.select(from_unixtime('unix_time').alias('ts')).collect()\n",
      "    [Row(ts='2015-04-08 00:00:00')]\n",
      "    >>> spark.conf.unset(\"spark.sql.session.timeZone\")\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(from_unixtime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 570,
   "id": "052c8352",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method cast in module pyspark.sql.column:\n",
      "\n",
      "cast(dataType) method of pyspark.sql.column.Column instance\n",
      "    Casts the column into type ``dataType``.\n",
      "    \n",
      "    .. versionadded:: 1.3.0\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> df.select(df.age.cast(\"string\").alias('ages')).collect()\n",
      "    [Row(ages='2'), Row(ages='5')]\n",
      "    >>> df.select(df.age.cast(StringType()).alias('ages')).collect()\n",
      "    [Row(ages='2'), Row(ages='5')]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(col(\"unixtime\").cast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 578,
   "id": "f2b54449",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------+\n",
      "|from_unixtime(unixtime, yyyy-MM-dd HH:mm:ss)|\n",
      "+--------------------------------------------+\n",
      "|                         2014-02-28 10:00:00|\n",
      "|                         2016-02-29 08:08:08|\n",
      "|                         2017-12-31 11:59:59|\n",
      "|                         2019-08-31 00:00:00|\n",
      "+--------------------------------------------+\n",
      "\n"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "cannot resolve 'CAST(unixtime AS DATE)' due to data type mismatch: cannot cast bigint to date;\n'Project [unresolvedalias(cast(unixtime#7953L as date), None)]\n+- Project [_1#7951L AS unixtime#7953L]\n   +- LogicalRDD [_1#7951L], false\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/7z/zf4v6y0d1q5bnv1v6p7tmvq80000gn/T/ipykernel_88737/1482174367.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# the below doesnt work\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0munixtimesDF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"unixtime\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"date\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, *cols)\u001b[0m\n\u001b[1;32m   1683\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Alice'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Bob'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1684\u001b[0m         \"\"\"\n\u001b[0;32m-> 1685\u001b[0;31m         \u001b[0mjdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jcols\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1686\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.9/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1321\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1322\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.9/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: cannot resolve 'CAST(unixtime AS DATE)' due to data type mismatch: cannot cast bigint to date;\n'Project [unresolvedalias(cast(unixtime#7953L as date), None)]\n+- Project [_1#7951L AS unixtime#7953L]\n   +- LogicalRDD [_1#7951L], false\n"
     ]
    }
   ],
   "source": [
    "unixtimesDF.select(from_unixtime(col(\"unixtime\"))).show()\n",
    "\n",
    "# the below doesnt work\n",
    "\n",
    "unixtimesDF.select(col(\"unixtime\").cast(\"date\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 573,
   "id": "e2495380",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|           unixtime|\n",
      "+-------------------+\n",
      "|2014-02-28 10:00:00|\n",
      "|2016-02-29 08:08:08|\n",
      "|2017-12-31 11:59:59|\n",
      "|2019-08-31 00:00:00|\n",
      "+-------------------+\n",
      "\n",
      "+-------------------+\n",
      "|           unixtime|\n",
      "+-------------------+\n",
      "|2014-02-28 10:00:00|\n",
      "|2016-02-29 08:08:08|\n",
      "|2017-12-31 11:59:59|\n",
      "|2019-08-31 00:00:00|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "unixtimesDF.select(col(\"unixtime\").cast(\"TIMESTAMP\")).show()\n",
    "\n",
    "# or\n",
    "unixtimesDF.select(col(\"unixtime\").cast(\"Timestamp\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23008b87",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b56bc9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0a8a2e34",
   "metadata": {},
   "source": [
    "## Dealing with Nulls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 580,
   "id": "eaa52a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Let us understand how to deal with nulls using functions that are available in Spark.\n",
    "\n",
    "        # We can use coalesce to return first non null value.\n",
    "        # We also have traditional SQL style functions such as nvl. However, they can be used either with expr or selectExpr."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76efeb98",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
