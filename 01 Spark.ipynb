{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86d760c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/Users/manideep/miniconda3/lib/python3.9/site-packages/pyspark/jars/spark-unsafe_2.12-3.2.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/06/25 21:40:25 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "81612a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To create spark dataframe, \n",
    "#.      u need \n",
    "#              list of dicts, list of lists, list of Row, list of string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "189242e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "| name|\n",
      "+-----+\n",
      "|Alice|\n",
      "|  wad|\n",
      "|    1|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "l = [('Alice', ),(\"wad\",),(1,)]\n",
    "spark.createDataFrame(l,'name string').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4486f71b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "| no| name|\n",
      "+---+-----+\n",
      "|  2|Alice|\n",
      "|  1| mani|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "l= [(2,\"Alice\"),(1,\"mani\")]\n",
    "spark.createDataFrame(l,\"no int,name string\").show("
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f1d244b",
   "metadata": {},
   "source": [
    "## Overview of Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "33d23672",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "| id|  name|\n",
      "+---+------+\n",
      "|  1| scott|\n",
      "|  2|donald|\n",
      "|  3| elvis|\n",
      "+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "users = [(1,\"scott\"),(2,\"donald\"),(3,\"elvis\")]\n",
    "\n",
    "df = spark.createDataFrame(users,'id int,name string')\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8597d99a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(id=1, name='scott'), Row(id=2, name='donald'), Row(id=3, name='elvis')]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# collect() converts dataframe to python list\n",
    "\n",
    "print(df.collect())\n",
    "\n",
    "type(df.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0c7675ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "285d0bb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class Row in module pyspark.sql.types:\n",
      "\n",
      "class Row(builtins.tuple)\n",
      " |  Row(*args, **kwargs)\n",
      " |  \n",
      " |  A row in :class:`DataFrame`.\n",
      " |  The fields in it can be accessed:\n",
      " |  \n",
      " |  * like attributes (``row.key``)\n",
      " |  * like dictionary values (``row[key]``)\n",
      " |  \n",
      " |  ``key in row`` will search through row keys.\n",
      " |  \n",
      " |  Row can be used to create a row object by using named arguments.\n",
      " |  It is not allowed to omit a named argument to represent that the value is\n",
      " |  None or missing. This should be explicitly set to None in this case.\n",
      " |  \n",
      " |  .. versionchanged:: 3.0.0\n",
      " |      Rows created from named arguments no longer have\n",
      " |      field names sorted alphabetically and will be ordered in the position as\n",
      " |      entered.\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> row = Row(name=\"Alice\", age=11)\n",
      " |  >>> row\n",
      " |  Row(name='Alice', age=11)\n",
      " |  >>> row['name'], row['age']\n",
      " |  ('Alice', 11)\n",
      " |  >>> row.name, row.age\n",
      " |  ('Alice', 11)\n",
      " |  >>> 'name' in row\n",
      " |  True\n",
      " |  >>> 'wrong_key' in row\n",
      " |  False\n",
      " |  \n",
      " |  Row also can be used to create another Row like class, then it\n",
      " |  could be used to create Row objects, such as\n",
      " |  \n",
      " |  >>> Person = Row(\"name\", \"age\")\n",
      " |  >>> Person\n",
      " |  <Row('name', 'age')>\n",
      " |  >>> 'name' in Person\n",
      " |  True\n",
      " |  >>> 'wrong_key' in Person\n",
      " |  False\n",
      " |  >>> Person(\"Alice\", 11)\n",
      " |  Row(name='Alice', age=11)\n",
      " |  \n",
      " |  This form can also be used to create rows as tuple values, i.e. with unnamed\n",
      " |  fields.\n",
      " |  \n",
      " |  >>> row1 = Row(\"Alice\", 11)\n",
      " |  >>> row2 = Row(name=\"Alice\", age=11)\n",
      " |  >>> row1 == row2\n",
      " |  True\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      Row\n",
      " |      builtins.tuple\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __call__(self, *args)\n",
      " |      create new Row object\n",
      " |  \n",
      " |  __contains__(self, item)\n",
      " |      Return key in self.\n",
      " |  \n",
      " |  __getattr__(self, item)\n",
      " |  \n",
      " |  __getitem__(self, item)\n",
      " |      Return self[key].\n",
      " |  \n",
      " |  __reduce__(self)\n",
      " |      Returns a tuple so Python knows how to pickle Row.\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Printable representation of Row used in Python REPL.\n",
      " |  \n",
      " |  __setattr__(self, key, value)\n",
      " |      Implement setattr(self, name, value).\n",
      " |  \n",
      " |  asDict(self, recursive=False)\n",
      " |      Return as a dict\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      recursive : bool, optional\n",
      " |          turns the nested Rows to dict (default: False).\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      If a row contains duplicate field names, e.g., the rows of a join\n",
      " |      between two :class:`DataFrame` that both have the fields of same names,\n",
      " |      one of the duplicate fields will be selected by ``asDict``. ``__getitem__``\n",
      " |      will also return one of the duplicate fields, however returned value might\n",
      " |      be different to ``asDict``.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> Row(name=\"Alice\", age=11).asDict() == {'name': 'Alice', 'age': 11}\n",
      " |      True\n",
      " |      >>> row = Row(key=1, value=Row(name='a', age=2))\n",
      " |      >>> row.asDict() == {'key': 1, 'value': Row(name='a', age=2)}\n",
      " |      True\n",
      " |      >>> row.asDict(True) == {'key': 1, 'value': {'name': 'a', 'age': 2}}\n",
      " |      True\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods defined here:\n",
      " |  \n",
      " |  __new__(cls, *args, **kwargs)\n",
      " |      Create and return a new object.  See help(type) for accurate signature.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from builtins.tuple:\n",
      " |  \n",
      " |  __add__(self, value, /)\n",
      " |      Return self+value.\n",
      " |  \n",
      " |  __eq__(self, value, /)\n",
      " |      Return self==value.\n",
      " |  \n",
      " |  __ge__(self, value, /)\n",
      " |      Return self>=value.\n",
      " |  \n",
      " |  __getattribute__(self, name, /)\n",
      " |      Return getattr(self, name).\n",
      " |  \n",
      " |  __getnewargs__(self, /)\n",
      " |  \n",
      " |  __gt__(self, value, /)\n",
      " |      Return self>value.\n",
      " |  \n",
      " |  __hash__(self, /)\n",
      " |      Return hash(self).\n",
      " |  \n",
      " |  __iter__(self, /)\n",
      " |      Implement iter(self).\n",
      " |  \n",
      " |  __le__(self, value, /)\n",
      " |      Return self<=value.\n",
      " |  \n",
      " |  __len__(self, /)\n",
      " |      Return len(self).\n",
      " |  \n",
      " |  __lt__(self, value, /)\n",
      " |      Return self<value.\n",
      " |  \n",
      " |  __mul__(self, value, /)\n",
      " |      Return self*value.\n",
      " |  \n",
      " |  __ne__(self, value, /)\n",
      " |      Return self!=value.\n",
      " |  \n",
      " |  __rmul__(self, value, /)\n",
      " |      Return value*self.\n",
      " |  \n",
      " |  count(self, value, /)\n",
      " |      Return number of occurrences of value.\n",
      " |  \n",
      " |  index(self, value, start=0, stop=9223372036854775807, /)\n",
      " |      Return first index of value.\n",
      " |      \n",
      " |      Raises ValueError if the value is not present.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from builtins.tuple:\n",
      " |  \n",
      " |  __class_getitem__(...) from builtins.type\n",
      " |      See PEP 585\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(Row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0bd2577f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Alice'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Row with key word arguments\n",
    "\n",
    "row = Row(name=\"Alice\", age=11)\n",
    "\n",
    "row.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "eb6b0965",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Row('Alice', 11)>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Row with  arguments\n",
    "\n",
    "\n",
    "row2 = Row(\"Alice\", 11)\n",
    "\n",
    "row2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f48d09e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "row2 = row2(\"mani\",2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "70beda9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mani'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row2.Alice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b458a7",
   "metadata": {},
   "source": [
    "## list of list to Spark Dataframe using Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "d53e58a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: int, name: string]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users = [[1,\"mani\"],[2,\"jaya\"]]\n",
    "\n",
    "# without using ROW\n",
    "\n",
    "spark.createDataFrame(users,'id int,name string')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "a71201e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<Row(1, 'mani')>, <Row(2, 'jaya')>]\n",
      "+---+----+\n",
      "| id|name|\n",
      "+---+----+\n",
      "|  1|mani|\n",
      "|  2|jaya|\n",
      "+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# with ROW\n",
    "\n",
    "users = [ Row(*row) for row in users]\n",
    "\n",
    "## or\n",
    "\n",
    "users = [ Row(row[0],row[1]) for row in users]\n",
    "\n",
    "print(users)\n",
    "\n",
    "spark.createDataFrame(users,'id int,name string').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "a9a1cade",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1,) <class 'tuple'> {}\n",
      "(1, 'hello') <class 'tuple'> {}\n",
      "([1, 'hello'],) <class 'tuple'> {}\n",
      "(1, 'hello') <class 'tuple'> {}\n",
      "('1', '3', '5') <class 'tuple'> {}\n",
      "() <class 'tuple'> {'1': 2, '3': 4, '5': 6}\n"
     ]
    }
   ],
   "source": [
    "def dummy(*arg,**kwargs):\n",
    "    print(arg,type(arg),kwargs)\n",
    "    \n",
    "dummy(1)\n",
    "dummy(1,\"hello\")\n",
    "list1= [1,\"hello\"]\n",
    "dummy(list1)       # ------ whole list as a single argument\n",
    "dummy(*list1)      # ------- list as two arguments\n",
    "\n",
    "d= {\"1\":2,\"3\":4,\"5\":6}\n",
    "dummy(*d)           # ------- passing only keys\n",
    "dummy(**d)          # ----- passing both key and values to kwargs arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "0cd8c3c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(id=1, name='mani'), Row(id=2, name='jaya'), Row(id=3, name='mahesh')]\n",
      "+---+------+\n",
      "| id|  name|\n",
      "+---+------+\n",
      "|  1|  mani|\n",
      "|  2|  jaya|\n",
      "|  3|mahesh|\n",
      "+---+------+\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "## list of tuple into spark dataframe using Row\n",
    "\n",
    "users2 =[(1,\"mani\"),(2,\"jaya\"),(3,\"mahesh\")]\n",
    "\n",
    "users2 = [Row(*row) for row in users2]\n",
    "\n",
    "print(users2)\n",
    "\n",
    "print(spark.createDataFrame(users2).show())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1126e56e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7740592",
   "metadata": {},
   "source": [
    "## list of Dicts into SparkDataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "47b0978f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+\n",
      "|username| id|\n",
      "+--------+---+\n",
      "|    mani|  2|\n",
      "|   divya|  3|\n",
      "|  ravali|  4|\n",
      "+--------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# inferring schema from dict is going to be deprecated, we have to use Row\n",
    "users_list = [\n",
    "    {\"username\":\"mani\",\"id\":2},\n",
    "    {\"username\":\"divya\",\"id\":3},\n",
    "    {\"username\":\"ravali\",\"id\":4}\n",
    "]\n",
    "\n",
    "from pyspark.sql import Row\n",
    "\n",
    "schema = [Row(**row) for row in users_list]\n",
    "\n",
    "# convert each one to row and create dataframe\n",
    "\n",
    "df = spark.createDataFrame(schema)\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "7b151e99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+\n",
      "|    _1| _2|\n",
      "+------+---+\n",
      "|  mani|  2|\n",
      "| divya|  3|\n",
      "|ravali|  4|\n",
      "+------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "schema = [Row(*row.values()) for row in users_list]\n",
    "\n",
    "# *row only pass the keys of dictionary\n",
    "\n",
    "# convert each one to row and create dataframe\n",
    "\n",
    "spark.createDataFrame(schema).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "ec9ab7c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-------+\n",
      "|                _1|     _2|\n",
      "+------------------+-------+\n",
      "|  {username, mani}|{id, 2}|\n",
      "| {username, divya}|{id, 3}|\n",
      "|{username, ravali}|{id, 4}|\n",
      "+------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# schema = [Row(row.values()) for row in users_list]\n",
    "schema = [Row(*row.items()) for row in users_list]\n",
    "schema\n",
    "\n",
    "spark.createDataFrame(schema).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "5636dd29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_values(['divya', 3])"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users_list[1].values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "898e7882",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('username', 'mani') ('id', 2)\n",
      "('username', 'divya') ('id', 3)\n",
      "('username', 'ravali') ('id', 4)\n"
     ]
    }
   ],
   "source": [
    "for row in users_list:\n",
    "    print(*row.items())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fabe9fe",
   "metadata": {},
   "source": [
    "## Basic Data Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "11a3a17c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['username', 'id']"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "22e47dc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('username', 'string'), ('id', 'bigint')]"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes\n",
    "# to get both column names and datatypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "c354ced8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- username: string (nullable = true)\n",
      " |-- id: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e582d42",
   "metadata": {},
   "source": [
    "## specifying the Schema as String for list of dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "c9665a26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+\n",
      "|username| id|\n",
      "+--------+---+\n",
      "|    mani|  2|\n",
      "|   divya|  3|\n",
      "|  ravali|  4|\n",
      "+--------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "users_schema = '''\n",
    "   username STRING,\n",
    "   id BIGINT\n",
    "'''\n",
    "\n",
    "spark.createDataFrame(users_list,users_schema).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "fec1444a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[username STRING: bigint, id BIGINT: string]"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.createDataFrame(users_list,schema = users_schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d448d1",
   "metadata": {},
   "source": [
    "## SCHEMA using a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "0d97d24e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+\n",
      "|username|    id|\n",
      "+--------+------+\n",
      "|       2|  mani|\n",
      "|       3| divya|\n",
      "|       4|ravali|\n",
      "+--------+------+\n",
      "\n",
      "None\n",
      "+------------+---------+\n",
      "|username INT|id STRING|\n",
      "+------------+---------+\n",
      "|           2|     mani|\n",
      "|           3|    divya|\n",
      "|           4|   ravali|\n",
      "+------------+---------+\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "users_schema = [\n",
    "    'username',\n",
    "    'id'\n",
    "]\n",
    "\n",
    "# with datatype with column name, the datatype is also indulged in the column names in dataframe\n",
    "#        as the list should contain cloumn names only  \n",
    "users_schema2 = [\n",
    "    'username INT',\n",
    "    'id STRING'\n",
    "] \n",
    "\n",
    "\n",
    "print(spark.createDataFrame(users_list,schema = users_schema).show())\n",
    "\n",
    "print(spark.createDataFrame(users_list,schema = users_schema2).show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "86a4327a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'username': 'mani', 'id': 2},\n",
       " {'username': 'divya', 'id': 3},\n",
       " {'username': 'ravali', 'id': 4}]"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d56e65da",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd7ea75e",
   "metadata": {},
   "source": [
    "## schema with spark types\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "cffeceec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "2013edef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[username: string, id: int]\n",
      "+--------+---+\n",
      "|username| id|\n",
      "+--------+---+\n",
      "|    mani|  2|\n",
      "|   divya|  3|\n",
      "|  ravali|  4|\n",
      "+--------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# we cant directly use these spark types\n",
    "# StructType has list of fields made of structField\n",
    "\n",
    "# StructType  is the data type representing a Row.\n",
    "\n",
    "# struct1 = StructType().add(\"f1\", StringType(), True).add(\"f2\", StringType(), True, None)\n",
    "# struct2 = StructType([StructField(\"f1\", StringType(), True),\n",
    "#     StructField(\"f2\", StringType(), True, None)])\n",
    "# struct1 == struct2\n",
    "# True\n",
    "\n",
    "fields = StructType([\n",
    "    StructField(\"username\",StringType()),\n",
    "    StructField(\"id\",IntegerType())\n",
    "])\n",
    "\n",
    "print(spark.createDataFrame(users_list,fields))\n",
    "\n",
    "spark.createDataFrame(users_list,fields).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "cc650690",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.types.StructType"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(fields)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb315858",
   "metadata": {},
   "source": [
    "\n",
    "## Spark DataFrame using pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "49c48b08",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>username</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mani</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>divya</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ravali</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  username  id\n",
       "0     mani   2\n",
       "1    divya   3\n",
       "2   ravali   4"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users_list = [\n",
    "    {'username': 'mani', 'id': 2},\n",
    " {'username': 'divya', 'id': 3},\n",
    " {'username': 'ravali', 'id': 4}\n",
    "]\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df1 = pd.DataFrame(users_list)\n",
    "\n",
    "df1.head()\n",
    "# pandas add NaN in empty spaces\n",
    "# which will be helpful in creating spark dataframe as we dont have to add the values explicitly in missed columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2bf8850d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+\n",
      "|username| id|\n",
      "+--------+---+\n",
      "|    mani|  2|\n",
      "|   divya|  3|\n",
      "|  ravali|  4|\n",
      "+--------+---+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.createDataFrame(df1).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73074934",
   "metadata": {},
   "source": [
    "## special types from spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "53f6fcba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ARRAY\n",
    "# STRUCT\n",
    "# MAP\n",
    "\n",
    "# Python structures such as list and dict can be implicitly converted to spark array and map respectively\n",
    "# we need spark APIs to convert python DataStructures to STRUCT type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "971a44f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+------------+--------------------+--------------------+-----------+-----------+-------------+-------------------+\n",
      "| id|first_name|   last_name|               email|       phone_numbers|is_customer|amount_paid|customer_from|    last_updated_ts|\n",
      "+---+----------+------------+--------------------+--------------------+-----------+-----------+-------------+-------------------+\n",
      "|  1|    Corrie|Van den Oord|cvandenoord0@etsy...|[+1 234 567 8901,...|       true|    1000.55|   2021-01-15|2021-02-10 01:15:00|\n",
      "|  2|  Nikolaus|     Brewitt|nbrewitt1@dailyma...|[+1 234 567 8923,...|       true|      900.0|   2021-02-14|2021-02-18 03:33:00|\n",
      "|  3|    Orelie|      Penney|openney2@vistapri...|[+1 714 512 9752,...|       true|     850.55|   2021-01-21|2021-03-15 15:16:55|\n",
      "|  4|     Ashby|    Maddocks|  amaddocks3@home.pl|                null|      false|       null|         null|2021-04-10 17:45:30|\n",
      "|  5|      Kurt|        Rome|krome4@shutterfly...|   [+1 817 934 7142]|      false|       null|         null|2021-04-02 00:55:18|\n",
      "+---+----------+------------+--------------------+--------------------+-----------+-----------+-------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import datetime\n",
    "from pyspark.sql import Row\n",
    "\n",
    "users = [\n",
    "    {\n",
    "        \"id\": 1,\n",
    "        \"first_name\": \"Corrie\",\n",
    "        \"last_name\": \"Van den Oord\",\n",
    "        \"email\": \"cvandenoord0@etsy.com\",\n",
    "        \"phone_numbers\": [\"+1 234 567 8901\", \"+1 234 567 8911\"],\n",
    "        \"is_customer\": True,\n",
    "        \"amount_paid\": 1000.55,\n",
    "        \"customer_from\": datetime.date(2021, 1, 15),\n",
    "        \"last_updated_ts\": datetime.datetime(2021, 2, 10, 1, 15, 0)\n",
    "    },\n",
    "    {\n",
    "        \"id\": 2,\n",
    "        \"first_name\": \"Nikolaus\",\n",
    "        \"last_name\": \"Brewitt\",\n",
    "        \"email\": \"nbrewitt1@dailymail.co.uk\",\n",
    "        \"phone_numbers\": [\"+1 234 567 8923\", \"+1 234 567 8934\"],\n",
    "        \"is_customer\": True,\n",
    "        \"amount_paid\": 900.0,\n",
    "        \"customer_from\": datetime.date(2021, 2, 14),\n",
    "        \"last_updated_ts\": datetime.datetime(2021, 2, 18, 3, 33, 0)\n",
    "    },\n",
    "    {\n",
    "        \"id\": 3,\n",
    "        \"first_name\": \"Orelie\",\n",
    "        \"last_name\": \"Penney\",\n",
    "        \"email\": \"openney2@vistaprint.com\",\n",
    "        \"phone_numbers\": [\"+1 714 512 9752\", \"+1 714 512 6601\"],\n",
    "        \"is_customer\": True,\n",
    "        \"amount_paid\": 850.55,\n",
    "        \"customer_from\": datetime.date(2021, 1, 21),\n",
    "        \"last_updated_ts\": datetime.datetime(2021, 3, 15, 15, 16, 55)\n",
    "    },\n",
    "    {\n",
    "        \"id\": 4,\n",
    "        \"first_name\": \"Ashby\",\n",
    "        \"last_name\": \"Maddocks\",\n",
    "        \"email\": \"amaddocks3@home.pl\",\n",
    "        \"phone_numbers\": None,\n",
    "        \"is_customer\": False,\n",
    "        \"amount_paid\": None,\n",
    "        \"customer_from\": None,\n",
    "        \"last_updated_ts\": datetime.datetime(2021, 4, 10, 17, 45, 30)\n",
    "    },\n",
    "    {\n",
    "        \"id\": 5,\n",
    "        \"first_name\": \"Kurt\",\n",
    "        \"last_name\": \"Rome\",\n",
    "        \"email\": \"krome4@shutterfly.com\",\n",
    "        \"phone_numbers\": [\"+1 817 934 7142\"],\n",
    "        \"is_customer\": False,\n",
    "        \"amount_paid\": None,\n",
    "        \"customer_from\": None,\n",
    "        \"last_updated_ts\": datetime.datetime(2021, 4, 2, 0, 55, 18)\n",
    "    }\n",
    "]\n",
    "\n",
    "users_df = spark.createDataFrame([Row(**user) for user in users])\n",
    "\n",
    "users_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3d3af6a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('id', 'bigint'),\n",
       " ('first_name', 'string'),\n",
       " ('last_name', 'string'),\n",
       " ('email', 'string'),\n",
       " ('phone_numbers', 'array<string>'),\n",
       " ('is_customer', 'boolean'),\n",
       " ('amount_paid', 'double'),\n",
       " ('customer_from', 'date'),\n",
       " ('last_updated_ts', 'timestamp')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ba2ed104",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- first_name: string (nullable = true)\n",
      " |-- last_name: string (nullable = true)\n",
      " |-- email: string (nullable = true)\n",
      " |-- phone_numbers: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- is_customer: boolean (nullable = true)\n",
      " |-- amount_paid: double (nullable = true)\n",
      " |-- customer_from: date (nullable = true)\n",
      " |-- last_updated_ts: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "users_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2ef17b7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------------------------------+\n",
      "|id |phone_numbers                     |\n",
      "+---+----------------------------------+\n",
      "|1  |[+1 234 567 8901, +1 234 567 8911]|\n",
      "|2  |[+1 234 567 8923, +1 234 567 8934]|\n",
      "|3  |[+1 714 512 9752, +1 714 512 6601]|\n",
      "|4  |null                              |\n",
      "|5  |[+1 817 934 7142]                 |\n",
      "+---+----------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "users_df.select(\"id\",\"phone_numbers\").show(truncate=False)\n",
    "\n",
    "# truncate= False to get all numbers without truncation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3c17a83d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['id',\n",
       " 'first_name',\n",
       " 'last_name',\n",
       " 'email',\n",
       " 'phone_numbers',\n",
       " 'is_customer',\n",
       " 'amount_paid',\n",
       " 'customer_from',\n",
       " 'last_updated_ts']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "74db2789",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+------------+-------------------------+----------------------------------+-----------+-----------+-------------+-------------------+---------------+\n",
      "|id |first_name|last_name   |email                    |phone_numbers                     |is_customer|amount_paid|customer_from|last_updated_ts    |phone_number   |\n",
      "+---+----------+------------+-------------------------+----------------------------------+-----------+-----------+-------------+-------------------+---------------+\n",
      "|1  |Corrie    |Van den Oord|cvandenoord0@etsy.com    |[+1 234 567 8901, +1 234 567 8911]|true       |1000.55    |2021-01-15   |2021-02-10 01:15:00|+1 234 567 8901|\n",
      "|1  |Corrie    |Van den Oord|cvandenoord0@etsy.com    |[+1 234 567 8901, +1 234 567 8911]|true       |1000.55    |2021-01-15   |2021-02-10 01:15:00|+1 234 567 8911|\n",
      "|2  |Nikolaus  |Brewitt     |nbrewitt1@dailymail.co.uk|[+1 234 567 8923, +1 234 567 8934]|true       |900.0      |2021-02-14   |2021-02-18 03:33:00|+1 234 567 8923|\n",
      "|2  |Nikolaus  |Brewitt     |nbrewitt1@dailymail.co.uk|[+1 234 567 8923, +1 234 567 8934]|true       |900.0      |2021-02-14   |2021-02-18 03:33:00|+1 234 567 8934|\n",
      "|3  |Orelie    |Penney      |openney2@vistaprint.com  |[+1 714 512 9752, +1 714 512 6601]|true       |850.55     |2021-01-21   |2021-03-15 15:16:55|+1 714 512 9752|\n",
      "|3  |Orelie    |Penney      |openney2@vistaprint.com  |[+1 714 512 9752, +1 714 512 6601]|true       |850.55     |2021-01-21   |2021-03-15 15:16:55|+1 714 512 6601|\n",
      "|5  |Kurt      |Rome        |krome4@shutterfly.com    |[+1 817 934 7142]                 |false      |null       |null         |2021-04-02 00:55:18|+1 817 934 7142|\n",
      "+---+----------+------------+-------------------------+----------------------------------+-----------+-----------+-------------+-------------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import explode\n",
    "\n",
    "# Explode - it is to explode a column with a Array type into a multiple records.\n",
    "#         - Returns a new row for each element in the given array or map\n",
    "\n",
    "# withColumn - Returns a new DataFrame by adding a column or replacing the existing column that has the same name.\n",
    "\n",
    "users_df. \\\n",
    "    withColumn(\"phone_number\",explode(\"phone_numbers\")).\\\n",
    "    show(truncate=False)\n",
    "               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "26380b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "94c6a5ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------------+---------------+\n",
      "| id|         mobile|           home|\n",
      "+---+---------------+---------------+\n",
      "|  1|+1 234 567 8901|+1 234 567 8911|\n",
      "|  2|+1 234 567 8923|+1 234 567 8934|\n",
      "|  3|+1 714 512 9752|+1 714 512 6601|\n",
      "|  4|           null|           null|\n",
      "|  5|+1 817 934 7142|           null|\n",
      "+---+---------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# to make individual columns from an array with aliases\n",
    "\n",
    "users_df.\\\n",
    "       select(\"id\",col('phone_numbers')[0].alias(\"mobile\"),col(\"phone_numbers\")[1].alias(\"home\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1de089c",
   "metadata": {},
   "source": [
    "## with explode_outer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2a601a0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+------------+--------------------+--------------------+-----------+-----------+-------------+-------------------+---------------+\n",
      "| id|first_name|   last_name|               email|       phone_numbers|is_customer|amount_paid|customer_from|    last_updated_ts|   phone_number|\n",
      "+---+----------+------------+--------------------+--------------------+-----------+-----------+-------------+-------------------+---------------+\n",
      "|  1|    Corrie|Van den Oord|cvandenoord0@etsy...|[+1 234 567 8901,...|       true|    1000.55|   2021-01-15|2021-02-10 01:15:00|+1 234 567 8901|\n",
      "|  1|    Corrie|Van den Oord|cvandenoord0@etsy...|[+1 234 567 8901,...|       true|    1000.55|   2021-01-15|2021-02-10 01:15:00|+1 234 567 8911|\n",
      "|  2|  Nikolaus|     Brewitt|nbrewitt1@dailyma...|[+1 234 567 8923,...|       true|      900.0|   2021-02-14|2021-02-18 03:33:00|+1 234 567 8923|\n",
      "|  2|  Nikolaus|     Brewitt|nbrewitt1@dailyma...|[+1 234 567 8923,...|       true|      900.0|   2021-02-14|2021-02-18 03:33:00|+1 234 567 8934|\n",
      "|  3|    Orelie|      Penney|openney2@vistapri...|[+1 714 512 9752,...|       true|     850.55|   2021-01-21|2021-03-15 15:16:55|+1 714 512 9752|\n",
      "|  3|    Orelie|      Penney|openney2@vistapri...|[+1 714 512 9752,...|       true|     850.55|   2021-01-21|2021-03-15 15:16:55|+1 714 512 6601|\n",
      "|  4|     Ashby|    Maddocks|  amaddocks3@home.pl|                null|      false|       null|         null|2021-04-10 17:45:30|           null|\n",
      "|  5|      Kurt|        Rome|krome4@shutterfly...|   [+1 817 934 7142]|      false|       null|         null|2021-04-02 00:55:18|+1 817 934 7142|\n",
      "+---+----------+------------+--------------------+--------------------+-----------+-----------+-------------+-------------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import explode_outer\n",
    "\n",
    "# Explode_outer - to get all non-Null and Null values present in exploding column\n",
    "\n",
    "users_df. \\\n",
    "    withColumn(\"phone_number\",explode_outer(\"phone_numbers\")).\\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c42ff8d",
   "metadata": {},
   "source": [
    "## Map Type Columns in Spark DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "be8caec2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+------------+-------------------------+----------------------------------------------------+-----------+-----------+-------------+-------------------+\n",
      "|id |first_name|last_name   |email                    |phone_numbers                                       |is_customer|amount_paid|customer_from|last_updated_ts    |\n",
      "+---+----------+------------+-------------------------+----------------------------------------------------+-----------+-----------+-------------+-------------------+\n",
      "|1  |Corrie    |Van den Oord|cvandenoord0@etsy.com    |{mobile -> +1 234 567 8901, home -> +1 234 567 8911}|true       |1000.55    |2021-01-15   |2021-02-10 01:15:00|\n",
      "|2  |Nikolaus  |Brewitt     |nbrewitt1@dailymail.co.uk|{mobile -> +1 234 567 8923, home -> +1 234 567 8934}|true       |900.0      |2021-02-14   |2021-02-18 03:33:00|\n",
      "|3  |Orelie    |Penney      |openney2@vistaprint.com  |{mobile -> +1 714 512 9752, home -> +1 714 512 6601}|true       |850.55     |2021-01-21   |2021-03-15 15:16:55|\n",
      "|4  |Ashby     |Maddocks    |amaddocks3@home.pl       |null                                                |false      |null       |null         |2021-04-10 17:45:30|\n",
      "|5  |Kurt      |Rome        |krome4@shutterfly.com    |{mobile -> +1 817 934 7142}                         |false      |null       |null         |2021-04-02 00:55:18|\n",
      "+---+----------+------------+-------------------------+----------------------------------------------------+-----------+-----------+-------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "users = [\n",
    "    {\n",
    "        \"id\": 1,\n",
    "        \"first_name\": \"Corrie\",\n",
    "        \"last_name\": \"Van den Oord\",\n",
    "        \"email\": \"cvandenoord0@etsy.com\",\n",
    "        \"phone_numbers\": {\"mobile\": \"+1 234 567 8901\", \"home\": \"+1 234 567 8911\"},\n",
    "        \"is_customer\": True,\n",
    "        \"amount_paid\": 1000.55,\n",
    "        \"customer_from\": datetime.date(2021, 1, 15),\n",
    "        \"last_updated_ts\": datetime.datetime(2021, 2, 10, 1, 15, 0)\n",
    "    },\n",
    "    {\n",
    "        \"id\": 2,\n",
    "        \"first_name\": \"Nikolaus\",\n",
    "        \"last_name\": \"Brewitt\",\n",
    "        \"email\": \"nbrewitt1@dailymail.co.uk\",\n",
    "        \"phone_numbers\": {\"mobile\": \"+1 234 567 8923\", \"home\": \"+1 234 567 8934\"},\n",
    "        \"is_customer\": True,\n",
    "        \"amount_paid\": 900.0,\n",
    "        \"customer_from\": datetime.date(2021, 2, 14),\n",
    "        \"last_updated_ts\": datetime.datetime(2021, 2, 18, 3, 33, 0)\n",
    "    },\n",
    "    {\n",
    "        \"id\": 3,\n",
    "        \"first_name\": \"Orelie\",\n",
    "        \"last_name\": \"Penney\",\n",
    "        \"email\": \"openney2@vistaprint.com\",\n",
    "        \"phone_numbers\": {\"mobile\": \"+1 714 512 9752\", \"home\": \"+1 714 512 6601\"},\n",
    "        \"is_customer\": True,\n",
    "        \"amount_paid\": 850.55,\n",
    "        \"customer_from\": datetime.date(2021, 1, 21),\n",
    "        \"last_updated_ts\": datetime.datetime(2021, 3, 15, 15, 16, 55)\n",
    "    },\n",
    "    {\n",
    "        \"id\": 4,\n",
    "        \"first_name\": \"Ashby\",\n",
    "        \"last_name\": \"Maddocks\",\n",
    "        \"email\": \"amaddocks3@home.pl\",\n",
    "        \"phone_numbers\": None,\n",
    "        \"is_customer\": False,\n",
    "        \"amount_paid\": None,\n",
    "        \"customer_from\": None,\n",
    "        \"last_updated_ts\": datetime.datetime(2021, 4, 10, 17, 45, 30)\n",
    "    },\n",
    "    {\n",
    "        \"id\": 5,\n",
    "        \"first_name\": \"Kurt\",\n",
    "        \"last_name\": \"Rome\",\n",
    "        \"email\": \"krome4@shutterfly.com\",\n",
    "        \"phone_numbers\": {\"mobile\": \"+1 817 934 7142\"},\n",
    "        \"is_customer\": False,\n",
    "        \"amount_paid\": None,\n",
    "        \"customer_from\": None,\n",
    "        \"last_updated_ts\": datetime.datetime(2021, 4, 2, 0, 55, 18)\n",
    "    }\n",
    "]\n",
    "\n",
    "\n",
    "# phone numbers are in dictionary\n",
    "\n",
    "users_df = spark.createDataFrame([Row(**user) for user in users])\n",
    "\n",
    "users_df.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d9d2d148",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- first_name: string (nullable = true)\n",
      " |-- last_name: string (nullable = true)\n",
      " |-- email: string (nullable = true)\n",
      " |-- phone_numbers: map (nullable = true)\n",
      " |    |-- key: string\n",
      " |    |-- value: string (valueContainsNull = true)\n",
      " |-- is_customer: boolean (nullable = true)\n",
      " |-- amount_paid: double (nullable = true)\n",
      " |-- customer_from: date (nullable = true)\n",
      " |-- last_updated_ts: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "users_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6912d346",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------------------------------------------------+\n",
      "|id |phone_numbers                                       |\n",
      "+---+----------------------------------------------------+\n",
      "|1  |{mobile -> +1 234 567 8901, home -> +1 234 567 8911}|\n",
      "|2  |{mobile -> +1 234 567 8923, home -> +1 234 567 8934}|\n",
      "|3  |{mobile -> +1 714 512 9752, home -> +1 714 512 6601}|\n",
      "|4  |null                                                |\n",
      "|5  |{mobile -> +1 817 934 7142}                         |\n",
      "+---+----------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "users_df.select(\"id\",\"phone_numbers\").show(truncate=False)\n",
    "\n",
    "# the dict is represe as key -> value pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1f684dbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('id', 'bigint'),\n",
       " ('first_name', 'string'),\n",
       " ('last_name', 'string'),\n",
       " ('email', 'string'),\n",
       " ('phone_numbers', 'map<string,string>'),\n",
       " ('is_customer', 'boolean'),\n",
       " ('amount_paid', 'double'),\n",
       " ('customer_from', 'date'),\n",
       " ('last_updated_ts', 'timestamp')]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users_df.dtypes\n",
    "\n",
    "# phone_numbers is a type of map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c13c1688",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------------------+\n",
      "| id|phone_numbers[mobile]|\n",
      "+---+---------------------+\n",
      "|  1|      +1 234 567 8901|\n",
      "|  2|      +1 234 567 8923|\n",
      "|  3|      +1 714 512 9752|\n",
      "|  4|                 null|\n",
      "|  5|      +1 817 934 7142|\n",
      "+---+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# To select only mobile numbers without alias\n",
    "\n",
    "users_df.select(\"id\",col(\"phone_numbers\")[\"mobile\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e459891c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+---------------+\n",
      "| id|key_new|      value_new|\n",
      "+---+-------+---------------+\n",
      "|  1| mobile|+1 234 567 8901|\n",
      "|  1|   home|+1 234 567 8911|\n",
      "|  2| mobile|+1 234 567 8923|\n",
      "|  2|   home|+1 234 567 8934|\n",
      "|  3| mobile|+1 714 512 9752|\n",
      "|  3|   home|+1 714 512 6601|\n",
      "|  5| mobile|+1 817 934 7142|\n",
      "+---+-------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# to get separate columns of map type column with alias\n",
    "users_df.select('id', explode('phone_numbers').alias('key_new', 'value_new')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "000026a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+---------------+\n",
      "| id|   key|          value|\n",
      "+---+------+---------------+\n",
      "|  1|mobile|+1 234 567 8901|\n",
      "|  1|  home|+1 234 567 8911|\n",
      "|  2|mobile|+1 234 567 8923|\n",
      "|  2|  home|+1 234 567 8934|\n",
      "|  3|mobile|+1 714 512 9752|\n",
      "|  3|  home|+1 714 512 6601|\n",
      "|  5|mobile|+1 817 934 7142|\n",
      "+---+------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import explode\n",
    "\n",
    "users_df.select(\"id\",explode(\"phone_numbers\")).show()\n",
    "\n",
    "# here phone_numbers is a map\n",
    "\n",
    "# we can also rename the columns as we needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f39941e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+---------------+\n",
      "| id|phone_type|   phone_number|\n",
      "+---+----------+---------------+\n",
      "|  1|    mobile|+1 234 567 8901|\n",
      "|  1|      home|+1 234 567 8911|\n",
      "|  2|    mobile|+1 234 567 8923|\n",
      "|  2|      home|+1 234 567 8934|\n",
      "|  3|    mobile|+1 714 512 9752|\n",
      "|  3|      home|+1 714 512 6601|\n",
      "|  5|    mobile|+1 817 934 7142|\n",
      "+---+----------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Renaming the columns of above DataFrame\n",
    "\n",
    "users_df.select(\"id\",explode(\"phone_numbers\")).\\\n",
    "            withColumnRenamed(\"key\",\"phone_type\").\\\n",
    "            withColumnRenamed(\"value\",\"phone_number\").\\\n",
    "            show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "accd7c44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+------------+--------------------+-----------+-----------+-------------+-------------------+----------+---------------+\n",
      "| id|first_name|   last_name|               email|is_customer|amount_paid|customer_from|    last_updated_ts|phone_type|   phone_number|\n",
      "+---+----------+------------+--------------------+-----------+-----------+-------------+-------------------+----------+---------------+\n",
      "|  1|    Corrie|Van den Oord|cvandenoord0@etsy...|       true|    1000.55|   2021-01-15|2021-02-10 01:15:00|    mobile|+1 234 567 8901|\n",
      "|  1|    Corrie|Van den Oord|cvandenoord0@etsy...|       true|    1000.55|   2021-01-15|2021-02-10 01:15:00|      home|+1 234 567 8911|\n",
      "|  2|  Nikolaus|     Brewitt|nbrewitt1@dailyma...|       true|      900.0|   2021-02-14|2021-02-18 03:33:00|    mobile|+1 234 567 8923|\n",
      "|  2|  Nikolaus|     Brewitt|nbrewitt1@dailyma...|       true|      900.0|   2021-02-14|2021-02-18 03:33:00|      home|+1 234 567 8934|\n",
      "|  3|    Orelie|      Penney|openney2@vistapri...|       true|     850.55|   2021-01-21|2021-03-15 15:16:55|    mobile|+1 714 512 9752|\n",
      "|  3|    Orelie|      Penney|openney2@vistapri...|       true|     850.55|   2021-01-21|2021-03-15 15:16:55|      home|+1 714 512 6601|\n",
      "|  5|      Kurt|        Rome|krome4@shutterfly...|      false|       null|         null|2021-04-02 00:55:18|    mobile|+1 817 934 7142|\n",
      "+---+----------+------------+--------------------+-----------+-----------+-------------+-------------------+----------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# To get all columns , use * \n",
    "\n",
    "users_df.select(\"*\",explode(\"phone_numbers\")).\\\n",
    "            withColumnRenamed(\"key\",\"phone_type\").\\\n",
    "            withColumnRenamed(\"value\",\"phone_number\").\\\n",
    "            drop(\"phone_numbers\").\\\n",
    "            show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "6dc8917c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+------------+--------------------+-----------+-----------+-------------+-------------------+------------------+--------------------+\n",
      "| id|first_name|   last_name|               email|is_customer|amount_paid|customer_from|    last_updated_ts|phone_numbers_home|phone_numbers_mobile|\n",
      "+---+----------+------------+--------------------+-----------+-----------+-------------+-------------------+------------------+--------------------+\n",
      "|  1|    Corrie|Van den Oord|cvandenoord0@etsy...|       true|    1000.55|   2021-01-15|2021-02-10 01:15:00|   +1 234 567 8911|     +1 234 567 8901|\n",
      "|  2|  Nikolaus|     Brewitt|nbrewitt1@dailyma...|       true|      900.0|   2021-02-14|2021-02-18 03:33:00|   +1 234 567 8934|     +1 234 567 8923|\n",
      "|  3|    Orelie|      Penney|openney2@vistapri...|       true|     850.55|   2021-01-21|2021-03-15 15:16:55|   +1 714 512 6601|     +1 714 512 9752|\n",
      "|  4|     Ashby|    Maddocks|  amaddocks3@home.pl|      false|       null|         null|2021-04-10 17:45:30|              null|                null|\n",
      "|  5|      Kurt|        Rome|krome4@shutterfly...|      false|       null|         null|2021-04-02 00:55:18|              null|     +1 817 934 7142|\n",
      "+---+----------+------------+--------------------+-----------+-----------+-------------+-------------------+------------------+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/06/26 01:49:13 WARN HeartbeatReceiver: Removing executor driver with no recent heartbeats: 4352152 ms exceeds timeout 120000 ms\n",
      "23/06/26 01:49:13 WARN SparkContext: Killing executors is not supported by current scheduler.\n"
     ]
    }
   ],
   "source": [
    "# withColumn - Returns a new DataFrame by adding a column or replacing the existing column that has the same name.\n",
    "\n",
    "users_df. \\\n",
    "    withColumn('phone_numbers_home', col('phone_numbers')['home']). \\\n",
    "    withColumn('phone_numbers_mobile', col('phone_numbers')['mobile'] ). \\\n",
    "    drop(\"phone_numbers\").\\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bfb5d02",
   "metadata": {},
   "source": [
    "## Struct type Columns in Spark DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f73c0a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
