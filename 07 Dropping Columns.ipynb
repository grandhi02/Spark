{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4e478f99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+------------+--------------------+--------------------+-------+-----------+-----------+-------------+-------------------+\n",
      "| id|first_name|   last_name|               email|       phone_numbers|courses|is_customer|amount_paid|customer_from|    last_updated_ts|\n",
      "+---+----------+------------+--------------------+--------------------+-------+-----------+-----------+-------------+-------------------+\n",
      "|  1|    Corrie|Van den Oord|cvandenoord0@etsy...|{+1 234 567 8901,...| [1, 2]|       true|    1000.55|   2021-01-15|2021-02-10 01:15:00|\n",
      "|  2|  Nikolaus|     Brewitt|nbrewitt1@dailyma...|{+1 234 567 8923,...|    [3]|       true|      900.0|   2021-02-14|2021-02-18 03:33:00|\n",
      "|  3|    Orelie|      Penney|openney2@vistapri...|{+1 714 512 9752,...| [2, 4]|       true|     850.55|   2021-01-21|2021-03-15 15:16:55|\n",
      "|  4|     Ashby|    Maddocks|  amaddocks3@home.pl|        {null, null}|     []|      false|        NaN|         null|2021-04-10 17:45:30|\n",
      "|  5|      Kurt|        Rome|krome4@shutterfly...|{+1 817 934 7142,...|     []|      false|        NaN|         null|2021-04-02 00:55:18|\n",
      "+---+----------+------------+--------------------+--------------------+-------+-----------+-----------+-------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col \n",
    "\n",
    "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
    "\n",
    "import datetime\n",
    "users = [\n",
    "    {\n",
    "        \"id\": 1,\n",
    "        \"first_name\": \"Corrie\",\n",
    "        \"last_name\": \"Van den Oord\",\n",
    "        \"email\": \"cvandenoord0@etsy.com\",\n",
    "        \"phone_numbers\": Row(mobile=\"+1 234 567 8901\", home=\"+1 234 567 8911\"),\n",
    "        \"courses\": [1, 2],\n",
    "        \"is_customer\": True,\n",
    "        \"amount_paid\": 1000.55,\n",
    "        \"customer_from\": datetime.date(2021, 1, 15),\n",
    "        \"last_updated_ts\": datetime.datetime(2021, 2, 10, 1, 15, 0)\n",
    "    },\n",
    "    {\n",
    "        \"id\": 2,\n",
    "        \"first_name\": \"Nikolaus\",\n",
    "        \"last_name\": \"Brewitt\",\n",
    "        \"email\": \"nbrewitt1@dailymail.co.uk\",\n",
    "        \"phone_numbers\":  Row(mobile=\"+1 234 567 8923\", home=\"1 234 567 8934\"),\n",
    "        \"courses\": [3],\n",
    "        \"is_customer\": True,\n",
    "        \"amount_paid\": 900.0,\n",
    "        \"customer_from\": datetime.date(2021, 2, 14),\n",
    "        \"last_updated_ts\": datetime.datetime(2021, 2, 18, 3, 33, 0)\n",
    "    },\n",
    "    {\n",
    "        \"id\": 3,\n",
    "        \"first_name\": \"Orelie\",\n",
    "        \"last_name\": \"Penney\",\n",
    "        \"email\": \"openney2@vistaprint.com\",\n",
    "        \"phone_numbers\": Row(mobile=\"+1 714 512 9752\", home=\"+1 714 512 6601\"),\n",
    "        \"courses\": [2, 4],\n",
    "        \"is_customer\": True,\n",
    "        \"amount_paid\": 850.55,\n",
    "        \"customer_from\": datetime.date(2021, 1, 21),\n",
    "        \"last_updated_ts\": datetime.datetime(2021, 3, 15, 15, 16, 55)\n",
    "    },\n",
    "    {\n",
    "        \"id\": 4,\n",
    "        \"first_name\": \"Ashby\",\n",
    "        \"last_name\": \"Maddocks\",\n",
    "        \"email\": \"amaddocks3@home.pl\",\n",
    "        \"phone_numbers\": Row(mobile=None, home=None),\n",
    "        \"courses\": [],\n",
    "        \"is_customer\": False,\n",
    "        \"amount_paid\": None,\n",
    "        \"customer_from\": None,\n",
    "        \"last_updated_ts\": datetime.datetime(2021, 4, 10, 17, 45, 30)\n",
    "    },\n",
    "    {\n",
    "        \"id\": 5,\n",
    "        \"first_name\": \"Kurt\",\n",
    "        \"last_name\": \"Rome\",\n",
    "        \"email\": \"krome4@shutterfly.com\",\n",
    "        \"phone_numbers\": Row(mobile=\"+1 817 934 7142\", home=None),\n",
    "        \"courses\": [],\n",
    "        \"is_customer\": False,\n",
    "        \"amount_paid\": None,\n",
    "        \"customer_from\": None,\n",
    "        \"last_updated_ts\": datetime.datetime(2021, 4, 2, 0, 55, 18)\n",
    "    }\n",
    "]\n",
    "     \n",
    "\n",
    "import pandas as pd\n",
    "     \n",
    "\n",
    "spark.conf.set('spark.sql.execution.arrow.pyspark.enabled', False)\n",
    "     \n",
    "\n",
    "users_df = spark.createDataFrame(pd.DataFrame(users))\n",
    "     \n",
    "\n",
    "users_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9a2c3fa6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method drop in module pyspark.sql.dataframe:\n",
      "\n",
      "drop(*cols) method of pyspark.sql.dataframe.DataFrame instance\n",
      "    Returns a new :class:`DataFrame` that drops the specified column.\n",
      "    This is a no-op if schema doesn't contain the given column name(s).\n",
      "    \n",
      "    .. versionadded:: 1.4.0\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    cols: str or :class:`Column`\n",
      "        a name of the column, or the :class:`Column` to drop\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> df.drop('age').collect()\n",
      "    [Row(name='Alice'), Row(name='Bob')]\n",
      "    \n",
      "    >>> df.drop(df.age).collect()\n",
      "    [Row(name='Alice'), Row(name='Bob')]\n",
      "    \n",
      "    >>> df.join(df2, df.name == df2.name, 'inner').drop(df.name).collect()\n",
      "    [Row(age=5, height=85, name='Bob')]\n",
      "    \n",
      "    >>> df.join(df2, df.name == df2.name, 'inner').drop(df2.name).collect()\n",
      "    [Row(age=5, name='Bob', height=85)]\n",
      "    \n",
      "    >>> df.join(df2, 'name', 'inner').drop('age', 'height').collect()\n",
      "    [Row(name='Bob')]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(users_df.drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1825ea05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- first_name: string (nullable = true)\n",
      " |-- last_name: string (nullable = true)\n",
      " |-- email: string (nullable = true)\n",
      " |-- phone_numbers: struct (nullable = true)\n",
      " |    |-- mobile: string (nullable = true)\n",
      " |    |-- home: string (nullable = true)\n",
      " |-- courses: array (nullable = true)\n",
      " |    |-- element: long (containsNull = true)\n",
      " |-- is_customer: boolean (nullable = true)\n",
      " |-- amount_paid: double (nullable = true)\n",
      " |-- customer_from: date (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "users_df.drop(\"last_updated_ts\").printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a5553eb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- first_name: string (nullable = true)\n",
      " |-- last_name: string (nullable = true)\n",
      " |-- email: string (nullable = true)\n",
      " |-- phone_numbers: struct (nullable = true)\n",
      " |    |-- mobile: string (nullable = true)\n",
      " |    |-- home: string (nullable = true)\n",
      " |-- courses: array (nullable = true)\n",
      " |    |-- element: long (containsNull = true)\n",
      " |-- is_customer: boolean (nullable = true)\n",
      " |-- amount_paid: double (nullable = true)\n",
      " |-- customer_from: date (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "users_df.drop(users_df[\"last_updated_ts\"]).printSchema()\n",
    "\n",
    "# users_df.drop(col(\"last_updated_ts\")).printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a1d4e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "feb71811",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- first_name: string (nullable = true)\n",
      " |-- last_name: string (nullable = true)\n",
      " |-- email: string (nullable = true)\n",
      " |-- phone_numbers: struct (nullable = true)\n",
      " |    |-- mobile: string (nullable = true)\n",
      " |    |-- home: string (nullable = true)\n",
      " |-- courses: array (nullable = true)\n",
      " |    |-- element: long (containsNull = true)\n",
      " |-- is_customer: boolean (nullable = true)\n",
      " |-- amount_paid: double (nullable = true)\n",
      " |-- customer_from: date (nullable = true)\n",
      " |-- last_updated_ts: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# If we have column name which does not exist, the column will be ignored\n",
    "\n",
    "users_df.drop(col('user_id')).printSchema()\n",
    "\n",
    "# this will not throw any error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de283a2",
   "metadata": {},
   "source": [
    "# Dropping multiple columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "86e2c381",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+--------------------+-------+-----------+-----------+-------------+-------------------+\n",
      "| id|               email|       phone_numbers|courses|is_customer|amount_paid|customer_from|    last_updated_ts|\n",
      "+---+--------------------+--------------------+-------+-----------+-----------+-------------+-------------------+\n",
      "|  1|cvandenoord0@etsy...|{+1 234 567 8901,...| [1, 2]|       true|    1000.55|   2021-01-15|2021-02-10 01:15:00|\n",
      "|  2|nbrewitt1@dailyma...|{+1 234 567 8923,...|    [3]|       true|      900.0|   2021-02-14|2021-02-18 03:33:00|\n",
      "|  3|openney2@vistapri...|{+1 714 512 9752,...| [2, 4]|       true|     850.55|   2021-01-21|2021-03-15 15:16:55|\n",
      "|  4|  amaddocks3@home.pl|        {null, null}|     []|      false|        NaN|         null|2021-04-10 17:45:30|\n",
      "|  5|krome4@shutterfly...|{+1 817 934 7142,...|     []|      false|        NaN|         null|2021-04-02 00:55:18|\n",
      "+---+--------------------+--------------------+-------+-----------+-----------+-------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# To drop multiple columns, u need to use column names as strings\n",
    "\n",
    "users_df.drop(\"first_name\",\"last_name\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fbd72a47",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "each col in the param list should be a string",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/7z/zf4v6y0d1q5bnv1v6p7tmvq80000gn/T/ipykernel_4618/722134575.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0musers_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"last_updated_ts\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"id\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprintSchema\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# this throw error as to drop multiple columns, we need to use strings only not the column objects\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, *cols)\u001b[0m\n\u001b[1;32m   2537\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcols\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2538\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2539\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"each col in the param list should be a string\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2540\u001b[0m             \u001b[0mjdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jseq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2541\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: each col in the param list should be a string"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "users_df.drop(col(\"last_updated_ts\"),col(\"id\")).printSchema()\n",
    "\n",
    "# this throw error as to drop multiple columns, we need to use strings only not the column objects\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6f37eaf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- email: string (nullable = true)\n",
      " |-- phone_numbers: struct (nullable = true)\n",
      " |    |-- mobile: string (nullable = true)\n",
      " |    |-- home: string (nullable = true)\n",
      " |-- courses: array (nullable = true)\n",
      " |    |-- element: long (containsNull = true)\n",
      " |-- is_customer: boolean (nullable = true)\n",
      " |-- amount_paid: double (nullable = true)\n",
      " |-- customer_from: date (nullable = true)\n",
      " |-- last_updated_ts: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# If we have column name which does not exist, the column will be ignored\n",
    "\n",
    "\n",
    "users_df.drop('user_id', 'first_name', 'last_name').printSchema()\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0406fb6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- courses: array (nullable = true)\n",
      " |    |-- element: long (containsNull = true)\n",
      " |-- is_customer: boolean (nullable = true)\n",
      " |-- amount_paid: double (nullable = true)\n",
      " |-- customer_from: date (nullable = true)\n",
      " |-- last_updated_ts: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pii_columns = ['first_name', 'last_name', 'email', 'phone_numbers', 'ssn_doesnot_exist']\n",
    "\n",
    "users_df_nopii = users_df.drop(*pii_columns) # we have to unpack the list\n",
    "\n",
    "users_df_nopii.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bfb827f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ca072988",
   "metadata": {},
   "source": [
    "## Dropping Duplicate Records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "37823eb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method distinct in module pyspark.sql.dataframe:\n",
      "\n",
      "distinct() method of pyspark.sql.dataframe.DataFrame instance\n",
      "    Returns a new :class:`DataFrame` containing the distinct rows in this :class:`DataFrame`.\n",
      "    \n",
      "    .. versionadded:: 1.3.0\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> df.distinct().count()\n",
      "    2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Drop duplicates based on certain columns.\n",
    "\n",
    "# We can use distinct, drop_duplicates or dropDuplicates for these scenarios.\n",
    "\n",
    "import datetime\n",
    "users = [\n",
    "    {\n",
    "        \"id\": 1,\n",
    "        \"first_name\": \"Corrie\",\n",
    "        \"last_name\": \"Van den Oord\",\n",
    "        \"email\": \"cvandenoord0@etsy.com\",\n",
    "        \"is_customer\": True,\n",
    "        \"amount_paid\": 1000.55,\n",
    "        \"customer_from\": datetime.date(2021, 1, 15),\n",
    "        \"last_updated_ts\": datetime.datetime(2021, 2, 10, 1, 15, 0)\n",
    "    },\n",
    "    {\n",
    "        \"id\": 2,\n",
    "        \"first_name\": \"Nikolaus\",\n",
    "        \"last_name\": \"Brewitt\",\n",
    "        \"email\": \"nbrewitt1@dailymail.co.uk\",\n",
    "        \"is_customer\": True,\n",
    "        \"amount_paid\": 900.0,\n",
    "        \"customer_from\": datetime.date(2021, 2, 14),\n",
    "        \"last_updated_ts\": datetime.datetime(2021, 2, 18, 3, 33, 0)\n",
    "    },\n",
    "    {\n",
    "        \"id\": 3,\n",
    "        \"first_name\": \"Orelie\",\n",
    "        \"last_name\": \"Penney\",\n",
    "        \"email\": \"openney2@vistaprint.com\",\n",
    "        \"is_customer\": True,\n",
    "        \"amount_paid\": 850.55,\n",
    "        \"customer_from\": datetime.date(2021, 1, 21),\n",
    "        \"last_updated_ts\": datetime.datetime(2021, 3, 15, 15, 16, 55)\n",
    "    },\n",
    "    {\n",
    "        \"id\": 3,\n",
    "        \"first_name\": \"Orelie\",\n",
    "        \"last_name\": \"Penney\",\n",
    "        \"email\": \"openney2@vistaprint.com\",\n",
    "        \"is_customer\": True,\n",
    "        \"amount_paid\": 850.55,\n",
    "        \"customer_from\": datetime.date(2021, 1, 21),\n",
    "        \"last_updated_ts\": datetime.datetime(2021, 3, 15, 15, 16, 55)\n",
    "    },\n",
    "    {\n",
    "        \"id\": 4,\n",
    "        \"first_name\": \"Ashby\",\n",
    "        \"last_name\": \"Maddocks\",\n",
    "        \"email\": \"amaddocks3@home.pl\",\n",
    "        \"is_customer\": False,\n",
    "        \"amount_paid\": None,\n",
    "        \"customer_from\": None,\n",
    "        \"last_updated_ts\": datetime.datetime(2021, 4, 10, 17, 45, 30)\n",
    "    },\n",
    "    {\n",
    "        \"id\": 4,\n",
    "        \"first_name\": \"Ashby\",\n",
    "        \"last_name\": \"Maddocks\",\n",
    "        \"email\": \"amaddocks3@home.pl\",\n",
    "        \"is_customer\": False,\n",
    "        \"amount_paid\": None,\n",
    "        \"customer_from\": None,\n",
    "        \"last_updated_ts\": datetime.datetime(2021, 4, 10, 17, 45, 30)\n",
    "    },\n",
    "    {\n",
    "        \"id\": 5,\n",
    "        \"first_name\": \"Kurt\",\n",
    "        \"last_name\": \"Rome\",\n",
    "        \"email\": \"krome4@shutterfly.com\",\n",
    "        \"is_customer\": False,\n",
    "        \"amount_paid\": None,\n",
    "        \"customer_from\": None,\n",
    "        \"last_updated_ts\": datetime.datetime(2021, 4, 2, 0, 55, 18)\n",
    "    },\n",
    "    {\n",
    "        \"id\": 2,\n",
    "        \"first_name\": \"Nikolaus\",\n",
    "        \"last_name\": \"Brewitt\",\n",
    "        \"email\": \"nbrewitt1@dailymail.co.uk\",\n",
    "        \"is_customer\": True,\n",
    "        \"amount_paid\": 1050.0,\n",
    "        \"customer_from\": datetime.date(2021, 2, 14),\n",
    "        \"last_updated_ts\": datetime.datetime(2021, 2, 25, 3, 33, 0)\n",
    "    }\n",
    "]\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "users_df = spark.createDataFrame(pd.DataFrame(users))\n",
    "\n",
    "help(users_df.distinct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "094fbc59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+------------+--------------------+-----------+-----------+-------------+-------------------+\n",
      "| id|first_name|   last_name|               email|is_customer|amount_paid|customer_from|    last_updated_ts|\n",
      "+---+----------+------------+--------------------+-----------+-----------+-------------+-------------------+\n",
      "|  1|    Corrie|Van den Oord|cvandenoord0@etsy...|       true|    1000.55|   2021-01-15|2021-02-10 01:15:00|\n",
      "|  2|  Nikolaus|     Brewitt|nbrewitt1@dailyma...|       true|      900.0|   2021-02-14|2021-02-18 03:33:00|\n",
      "|  3|    Orelie|      Penney|openney2@vistapri...|       true|     850.55|   2021-01-21|2021-03-15 15:16:55|\n",
      "|  4|     Ashby|    Maddocks|  amaddocks3@home.pl|      false|        NaN|         null|2021-04-10 17:45:30|\n",
      "|  5|      Kurt|        Rome|krome4@shutterfly...|      false|        NaN|         null|2021-04-02 00:55:18|\n",
      "|  2|  Nikolaus|     Brewitt|nbrewitt1@dailyma...|       true|     1050.0|   2021-02-14|2021-02-25 03:33:00|\n",
      "+---+----------+------------+--------------------+-----------+-----------+-------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "users_df.distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b1756e6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users_df.distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c102ff97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method dropDuplicates in module pyspark.sql.dataframe:\n",
      "\n",
      "dropDuplicates(subset=None) method of pyspark.sql.dataframe.DataFrame instance\n",
      "    :func:`drop_duplicates` is an alias for :func:`dropDuplicates`.\n",
      "    \n",
      "    .. versionadded:: 1.4\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(users_df.drop_duplicates)\n",
    "\n",
    "# this is an alias of dropDuplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "972948f1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method dropDuplicates in module pyspark.sql.dataframe:\n",
      "\n",
      "dropDuplicates(subset=None) method of pyspark.sql.dataframe.DataFrame instance\n",
      "    Return a new :class:`DataFrame` with duplicate rows removed,\n",
      "    optionally only considering certain columns.\n",
      "    \n",
      "    For a static batch :class:`DataFrame`, it just drops duplicate rows. For a streaming\n",
      "    :class:`DataFrame`, it will keep all data across triggers as intermediate state to drop\n",
      "    duplicates rows. You can use :func:`withWatermark` to limit how late the duplicate data can\n",
      "    be and system will accordingly limit the state. In addition, too late data older than\n",
      "    watermark will be dropped to avoid any possibility of duplicates.\n",
      "    \n",
      "    :func:`drop_duplicates` is an alias for :func:`dropDuplicates`.\n",
      "    \n",
      "    .. versionadded:: 1.4.0\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> from pyspark.sql import Row\n",
      "    >>> df = sc.parallelize([ \\\n",
      "    ...     Row(name='Alice', age=5, height=80), \\\n",
      "    ...     Row(name='Alice', age=5, height=80), \\\n",
      "    ...     Row(name='Alice', age=10, height=80)]).toDF()\n",
      "    >>> df.dropDuplicates().show()\n",
      "    +-----+---+------+\n",
      "    | name|age|height|\n",
      "    +-----+---+------+\n",
      "    |Alice|  5|    80|\n",
      "    |Alice| 10|    80|\n",
      "    +-----+---+------+\n",
      "    \n",
      "    >>> df.dropDuplicates(['name', 'height']).show()\n",
      "    +-----+---+------+\n",
      "    | name|age|height|\n",
      "    +-----+---+------+\n",
      "    |Alice|  5|    80|\n",
      "    +-----+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(users_df.dropDuplicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ec73f595",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+------------+--------------------+-----------+-----------+-------------+-------------------+\n",
      "| id|first_name|   last_name|               email|is_customer|amount_paid|customer_from|    last_updated_ts|\n",
      "+---+----------+------------+--------------------+-----------+-----------+-------------+-------------------+\n",
      "|  1|    Corrie|Van den Oord|cvandenoord0@etsy...|       true|    1000.55|   2021-01-15|2021-02-10 01:15:00|\n",
      "|  2|  Nikolaus|     Brewitt|nbrewitt1@dailyma...|       true|      900.0|   2021-02-14|2021-02-18 03:33:00|\n",
      "|  3|    Orelie|      Penney|openney2@vistapri...|       true|     850.55|   2021-01-21|2021-03-15 15:16:55|\n",
      "|  4|     Ashby|    Maddocks|  amaddocks3@home.pl|      false|        NaN|         null|2021-04-10 17:45:30|\n",
      "|  5|      Kurt|        Rome|krome4@shutterfly...|      false|        NaN|         null|2021-04-02 00:55:18|\n",
      "+---+----------+------------+--------------------+-----------+-----------+-------------+-------------------+\n",
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Parameter 'subset' must be a list of columns",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/7z/zf4v6y0d1q5bnv1v6p7tmvq80000gn/T/ipykernel_4618/2337120834.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# We can also drop duplicates based on certain columns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# This will fail as the function expects sequence type object such as list or array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0musers_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropDuplicates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mdropDuplicates\u001b[0;34m(self, subset)\u001b[0m\n\u001b[1;32m   1984\u001b[0m         if subset is not None and (\n\u001b[1;32m   1985\u001b[0m                 not isinstance(subset, Iterable) or isinstance(subset, str)):\n\u001b[0;32m-> 1986\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Parameter 'subset' must be a list of columns\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1987\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1988\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msubset\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Parameter 'subset' must be a list of columns"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# we should use a list not a string\n",
    "\n",
    "users_df.dropDuplicates(['id']).show()\n",
    "\n",
    "\n",
    "\n",
    "# We can also drop duplicates based on certain columns\n",
    "# This will fail as the function expects sequence type object such as list or array\n",
    "users_df.dropDuplicates('id').show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "4253ff8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+------------+--------------------+-----------+-----------+-------------+-------------------+\n",
      "| id|first_name|   last_name|               email|is_customer|amount_paid|customer_from|    last_updated_ts|\n",
      "+---+----------+------------+--------------------+-----------+-----------+-------------+-------------------+\n",
      "|  1|    Corrie|Van den Oord|cvandenoord0@etsy...|       true|    1000.55|   2021-01-15|2021-02-10 01:15:00|\n",
      "|  2|  Nikolaus|     Brewitt|nbrewitt1@dailyma...|       true|      900.0|   2021-02-14|2021-02-18 03:33:00|\n",
      "|  2|  Nikolaus|     Brewitt|nbrewitt1@dailyma...|       true|     1050.0|   2021-02-14|2021-02-25 03:33:00|\n",
      "|  3|    Orelie|      Penney|openney2@vistapri...|       true|     850.55|   2021-01-21|2021-03-15 15:16:55|\n",
      "|  4|     Ashby|    Maddocks|  amaddocks3@home.pl|      false|        NaN|         null|2021-04-10 17:45:30|\n",
      "|  5|      Kurt|        Rome|krome4@shutterfly...|      false|        NaN|         null|2021-04-02 00:55:18|\n",
      "+---+----------+------------+--------------------+-----------+-----------+-------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "users_df.dropDuplicates(['id', 'amount_paid']).show()\n",
    "\n",
    "# Removes duplicates if the combination of those 2 columns occurs multiple times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a8cf814",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d892267c",
   "metadata": {},
   "source": [
    "## Dropping based on null Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b7d84417",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping records based on null values.\n",
    "\n",
    "# Drop records when all column values are nulls.\n",
    "\n",
    "# Drop records any of the column value is null.\n",
    "\n",
    "# Drop records that have less than thresh non-null values.\n",
    "\n",
    "# Drop records when any of the column value or all column values are nulls for provided subset of columns.\n",
    "\n",
    "# We can use df.na.drop or df.dropna to take care of dealing with records having columns with null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "88613af4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+------------+--------------------+-----------+-----------+-------------+-------------------+\n",
      "| id|first_name|   last_name|               email|is_customer|amount_paid|customer_from|    last_updated_ts|\n",
      "+---+----------+------------+--------------------+-----------+-----------+-------------+-------------------+\n",
      "|1.0|    Corrie|Van den Oord|cvandenoord0@etsy...|       true|    1000.55|   2021-01-15|2021-02-10 01:15:00|\n",
      "|NaN|      null|        null|                null|       null|        NaN|         null|               null|\n",
      "|2.0|  Nikolaus|     Brewitt|nbrewitt1@dailyma...|       true|      900.0|   2021-02-14|2021-02-18 03:33:00|\n",
      "|3.0|    Orelie|      Penney|openney2@vistapri...|       true|     850.55|   2021-01-21|2021-03-15 15:16:55|\n",
      "|3.0|    Orelie|      Penney|openney2@vistapri...|       true|     850.55|   2021-01-21|2021-03-15 15:16:55|\n",
      "|4.0|     Ashby|    Maddocks|  amaddocks3@home.pl|      false|        NaN|         null|2021-04-10 17:45:30|\n",
      "|4.0|     Ashby|    Maddocks|  amaddocks3@home.pl|      false|        NaN|         null|2021-04-10 17:45:30|\n",
      "|5.0|      Kurt|        Rome|krome4@shutterfly...|      false|        NaN|         null|2021-04-02 00:55:18|\n",
      "|NaN|      null|        null|                null|       null|        NaN|         null|               null|\n",
      "|5.0|      null|        null|                null|       null|        NaN|         null|               null|\n",
      "|NaN|      null|        null|nbrewitt1@dailyma...|       null|        NaN|         null|               null|\n",
      "|NaN|      Kurt|        Rome|                null|      false|        NaN|         null|2021-04-02 00:55:18|\n",
      "|2.0|  Nikolaus|     Brewitt|nbrewitt1@dailyma...|       true|     1050.0|   2021-02-14|2021-02-25 03:33:00|\n",
      "+---+----------+------------+--------------------+-----------+-----------+-------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "users = [\n",
    "    {\n",
    "        \"id\": 1,\n",
    "        \"first_name\": \"Corrie\",\n",
    "        \"last_name\": \"Van den Oord\",\n",
    "        \"email\": \"cvandenoord0@etsy.com\",\n",
    "        \"is_customer\": True,\n",
    "        \"amount_paid\": 1000.55,\n",
    "        \"customer_from\": datetime.date(2021, 1, 15),\n",
    "        \"last_updated_ts\": datetime.datetime(2021, 2, 10, 1, 15, 0)\n",
    "    },\n",
    "    {\n",
    "        \"id\": None,\n",
    "        \"first_name\": None,\n",
    "        \"last_name\": None,\n",
    "        \"email\": None,\n",
    "        \"is_customer\": None,\n",
    "        \"amount_paid\": None,\n",
    "        \"customer_from\": None,\n",
    "        \"last_updated_ts\": None\n",
    "    },\n",
    "    {\n",
    "        \"id\": 2,\n",
    "        \"first_name\": \"Nikolaus\",\n",
    "        \"last_name\": \"Brewitt\",\n",
    "        \"email\": \"nbrewitt1@dailymail.co.uk\",\n",
    "        \"is_customer\": True,\n",
    "        \"amount_paid\": 900.0,\n",
    "        \"customer_from\": datetime.date(2021, 2, 14),\n",
    "        \"last_updated_ts\": datetime.datetime(2021, 2, 18, 3, 33, 0)\n",
    "    },\n",
    "    {\n",
    "        \"id\": 3,\n",
    "        \"first_name\": \"Orelie\",\n",
    "        \"last_name\": \"Penney\",\n",
    "        \"email\": \"openney2@vistaprint.com\",\n",
    "        \"is_customer\": True,\n",
    "        \"amount_paid\": 850.55,\n",
    "        \"customer_from\": datetime.date(2021, 1, 21),\n",
    "        \"last_updated_ts\": datetime.datetime(2021, 3, 15, 15, 16, 55)\n",
    "    },\n",
    "    {\n",
    "        \"id\": 3,\n",
    "        \"first_name\": \"Orelie\",\n",
    "        \"last_name\": \"Penney\",\n",
    "        \"email\": \"openney2@vistaprint.com\",\n",
    "        \"is_customer\": True,\n",
    "        \"amount_paid\": 850.55,\n",
    "        \"customer_from\": datetime.date(2021, 1, 21),\n",
    "        \"last_updated_ts\": datetime.datetime(2021, 3, 15, 15, 16, 55)\n",
    "    },\n",
    "    {\n",
    "        \"id\": 4,\n",
    "        \"first_name\": \"Ashby\",\n",
    "        \"last_name\": \"Maddocks\",\n",
    "        \"email\": \"amaddocks3@home.pl\",\n",
    "        \"is_customer\": False,\n",
    "        \"amount_paid\": None,\n",
    "        \"customer_from\": None,\n",
    "        \"last_updated_ts\": datetime.datetime(2021, 4, 10, 17, 45, 30)\n",
    "    },\n",
    "    {\n",
    "        \"id\": 4,\n",
    "        \"first_name\": \"Ashby\",\n",
    "        \"last_name\": \"Maddocks\",\n",
    "        \"email\": \"amaddocks3@home.pl\",\n",
    "        \"is_customer\": False,\n",
    "        \"amount_paid\": None,\n",
    "        \"customer_from\": None,\n",
    "        \"last_updated_ts\": datetime.datetime(2021, 4, 10, 17, 45, 30)\n",
    "    },\n",
    "    {\n",
    "        \"id\": 5,\n",
    "        \"first_name\": \"Kurt\",\n",
    "        \"last_name\": \"Rome\",\n",
    "        \"email\": \"krome4@shutterfly.com\",\n",
    "        \"is_customer\": False,\n",
    "        \"amount_paid\": None,\n",
    "        \"customer_from\": None,\n",
    "        \"last_updated_ts\": datetime.datetime(2021, 4, 2, 0, 55, 18)\n",
    "    },\n",
    "    {\n",
    "        \"id\": None,\n",
    "        \"first_name\": None,\n",
    "        \"last_name\": None,\n",
    "        \"email\": None,\n",
    "        \"is_customer\": None,\n",
    "        \"amount_paid\": None,\n",
    "        \"customer_from\": None,\n",
    "        \"last_updated_ts\": None\n",
    "    },\n",
    "    {\n",
    "        \"id\": 5,\n",
    "        \"first_name\": None,\n",
    "        \"last_name\": None,\n",
    "        \"email\": None,\n",
    "        \"is_customer\": None,\n",
    "        \"amount_paid\": None,\n",
    "        \"customer_from\": None,\n",
    "        \"last_updated_ts\": None\n",
    "    },\n",
    "    {\n",
    "        \"id\": None,\n",
    "        \"first_name\": None,\n",
    "        \"last_name\": None,\n",
    "        \"email\": \"nbrewitt1@dailymail.co.uk\",\n",
    "        \"is_customer\": None,\n",
    "        \"amount_paid\": None,\n",
    "        \"customer_from\": None,\n",
    "        \"last_updated_ts\": None\n",
    "    },\n",
    "    {\n",
    "        \"id\": None,\n",
    "        \"first_name\": \"Kurt\",\n",
    "        \"last_name\": \"Rome\",\n",
    "        \"email\": None,\n",
    "        \"is_customer\": False,\n",
    "        \"amount_paid\": None,\n",
    "        \"customer_from\": None,\n",
    "        \"last_updated_ts\": datetime.datetime(2021, 4, 2, 0, 55, 18)\n",
    "    },\n",
    "    {\n",
    "        \"id\": 2,\n",
    "        \"first_name\": \"Nikolaus\",\n",
    "        \"last_name\": \"Brewitt\",\n",
    "        \"email\": \"nbrewitt1@dailymail.co.uk\",\n",
    "        \"is_customer\": True,\n",
    "        \"amount_paid\": 1050.0,\n",
    "        \"customer_from\": datetime.date(2021, 2, 14),\n",
    "        \"last_updated_ts\": datetime.datetime(2021, 2, 25, 3, 33, 0)\n",
    "    }\n",
    "]\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "users_df = spark.createDataFrame(pd.DataFrame(users))\n",
    "     \n",
    "\n",
    "users_df.show()\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "a0ed6050",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method drop in module pyspark.sql.dataframe:\n",
      "\n",
      "drop(how='any', thresh=None, subset=None) method of pyspark.sql.dataframe.DataFrameNaFunctions instance\n",
      "    Returns a new :class:`DataFrame` omitting rows with null values.\n",
      "    :func:`DataFrame.dropna` and :func:`DataFrameNaFunctions.drop` are aliases of each other.\n",
      "    \n",
      "    .. versionadded:: 1.3.1\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    how : str, optional\n",
      "        'any' or 'all'.\n",
      "        If 'any', drop a row if it contains any nulls.\n",
      "        If 'all', drop a row only if all its values are null.\n",
      "    thresh: int, optional\n",
      "        default None\n",
      "        If specified, drop rows that have less than `thresh` non-null values.\n",
      "        This overwrites the `how` parameter.\n",
      "    subset : str, tuple or list, optional\n",
      "        optional list of column names to consider.\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> df4.na.drop().show()\n",
      "    +---+------+-----+\n",
      "    |age|height| name|\n",
      "    +---+------+-----+\n",
      "    | 10|    80|Alice|\n",
      "    +---+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(users_df.na.drop)\n",
    "\n",
    "# this is an alias of dropna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "00b4e41e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method dropna in module pyspark.sql.dataframe:\n",
      "\n",
      "dropna(how='any', thresh=None, subset=None) method of pyspark.sql.dataframe.DataFrame instance\n",
      "    Returns a new :class:`DataFrame` omitting rows with null values.\n",
      "    :func:`DataFrame.dropna` and :func:`DataFrameNaFunctions.drop` are aliases of each other.\n",
      "    \n",
      "    .. versionadded:: 1.3.1\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    how : str, optional\n",
      "        'any' or 'all'.\n",
      "        If 'any', drop a row if it contains any nulls.\n",
      "        If 'all', drop a row only if all its values are null.\n",
      "    thresh: int, optional\n",
      "        default None\n",
      "        If specified, drop rows that have less than `thresh` non-null values.\n",
      "        This overwrites the `how` parameter.\n",
      "    subset : str, tuple or list, optional\n",
      "        optional list of column names to consider.\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> df4.na.drop().show()\n",
      "    +---+------+-----+\n",
      "    |age|height| name|\n",
      "    +---+------+-----+\n",
      "    | 10|    80|Alice|\n",
      "    +---+------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/07/01 20:45:52 WARN HeartbeatReceiver: Removing executor driver with no recent heartbeats: 7183133 ms exceeds timeout 120000 ms\n",
      "23/07/01 20:45:52 WARN SparkContext: Killing executors is not supported by current scheduler.\n"
     ]
    }
   ],
   "source": [
    "help(users_df.dropna)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9247e0b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
